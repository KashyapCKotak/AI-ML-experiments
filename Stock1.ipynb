{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Stock1.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 2
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython2",
      "version": "2.7.13"
    },
    "kernelspec": {
      "display_name": "Python [conda root]",
      "language": "python",
      "name": "conda-root-py"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KashyapCKotak/AI-ML-experiments/blob/master/Stock1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "A8_lk4B9TXOV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#==============================================================================\n",
        "# %% calc inputs and outputs\n",
        "#==============================================================================\n",
        "###helper functions\n",
        "#return\n",
        "ret = lambda x: np.log(x/x.shift(1))\n",
        "#10-step and 100-step moving average\n",
        "ma10 =  lambda x: pd.rolling_mean( x, 10, min_periods=1 )\n",
        "ma100 =  lambda x: pd.rolling_mean( x, 100, min_periods=1 )\n",
        "#10-step and 100-step moving maximum\n",
        "max10 =  lambda x: pd.rolling_max( x, 10, min_periods=1 )\n",
        "max100 =  lambda x: pd.rolling_max( x, 100, min_periods=1 )\n",
        "#10-step and 100-step moving minimum\n",
        "min10 =  lambda x: pd.rolling_min( x, 10, min_periods=1 )\n",
        "min100 =  lambda x: pd.rolling_min( x, 100, min_periods=1 )\n",
        "#this list will contain names of all inputs for convenience\n",
        "factors = []\n",
        "#split the data by date - because when computing moving averages, etc - we cannot roll between days\n",
        "grp = df.groupby( 'DATE' )\n",
        "\n",
        "### bidasksize factors\n",
        "df['bidasksize'] = ( df['BIDSIZE'] + df['ASKSIZE'] )\n",
        "df['bidasksize_notional'] = ( df['BIDSIZE'] * df['BIDPRICE'] + df['ASKSIZE'] * df['ASKPRICE'] )\n",
        "df['bidasksize_imbalance'] = ( df['BIDSIZE'] - df['ASKSIZE'] ) \n",
        "df['bidasksize_imbalance_log'] = np.log( df['BIDSIZE'] / df['ASKSIZE'] ) \n",
        "df['bidasksize_imbalance_norm'] = ( df['BIDSIZE'] - df['ASKSIZE'] ) / ( df['BIDSIZE'] + df['ASKSIZE'] )\n",
        "df['bidasksize_ret'] = grp['bidasksize'].apply( ret ) \n",
        "df['asksize_ret'] = grp['ASKSIZE'].apply( ret ) \n",
        "df['bidsize_ret'] = grp['BIDSIZE'].apply( ret ) \n",
        "namesBidAsk = ['bidasksize', 'bidasksize_notional', 'bidasksize_imbalance', 'bidasksize_imbalance_log', 'bidasksize_imbalance_norm', 'bidasksize_ret', 'asksize_ret', 'bidsize_ret' ]\n",
        "factors += namesBidAsk\n",
        "#for the above factors - calculate MA10, MA100 and diff MA1-MA10, MA10-MA100\n",
        "for name in namesBidAsk:\n",
        "    df[ name + '_MA10' ] = grp[ name ].apply( ma10 )\n",
        "    df[ name + '_MA100' ] = grp[ name ].apply( ma100 )\n",
        "    df[ name + '_MA10_dev' ] = df[ name ] - df[ name + '_MA10' ]\n",
        "    df[ name + '_MA100_dev' ] = df[ name + '_MA10' ] - df[ name + '_MA100' ]\n",
        "\n",
        "    factors += [ name + '_MA10', name + '_MA100',  name + '_MA10_dev', name + '_MA100_dev' ]\n",
        "\n",
        "### spread factors\n",
        "df[ 'spread' ] = ( df['ASKPRICE'] - df['BIDPRICE']).round(9) \n",
        "df[ 'spread_MA10' ] = grp[ 'spread' ].apply( ma10 ) \n",
        "df[ 'spread_MA100' ] = grp[ 'spread' ].apply( ma100 )\n",
        "df[ 'spread_MA10_dev' ] = np.log( df[ 'spread' ] / df[ 'spread_MA10' ] )\n",
        "df[ 'spread_MA100_dev' ] = np.log( df[ 'spread_MA10' ] / df[ 'spread_MA100' ] )\n",
        "namesSpread = [ 'spread', 'spread_MA10', 'spread_MA100', 'spread_MA10_dev', 'spread_MA100_dev' ]\n",
        "factors += namesSpread\n",
        "\n",
        "### weighted mid factors\n",
        "df['mid'] = ( df['BIDPRICE'] + df['ASKPRICE'] ) / 2\n",
        "df['mid_MA10'] = grp[ 'mid' ].apply( ma10 )\n",
        "df['mid_MA100'] = grp[ 'mid' ].apply( ma100 )\n",
        "df['mid_weighted'] = ( df['BIDSIZE'] * df['ASKPRICE'] + df['ASKSIZE'] * df['BIDPRICE'] ) / ( df['BIDSIZE'] + df['ASKSIZE'] )\n",
        "df['mid_weighted_MA10'] = grp[ 'mid_weighted' ].apply( ma10 )\n",
        "df['mid_weighted_MA100'] = grp[ 'mid_weighted' ].apply( ma100 )\n",
        "\n",
        "df['mid_weighted_dev'] = np.log( df['mid_weighted'] / df['mid'] )\n",
        "df['mid_weighted_MA10_dev'] = np.log( df['mid_weighted_MA10'] / df['mid_MA10'] )\n",
        "df['mid_weighted_MA100_dev'] = np.log( df['mid_weighted_MA100'] / df['mid_MA100'] )\n",
        "\n",
        "namesWeighted = ['mid_weighted_dev', 'mid_weighted_MA10_dev', 'mid_weighted_MA100_dev']\n",
        "factors += namesWeighted\n",
        "\n",
        "### trades factors\n",
        "df['buysell'] = df['isBuy'] * 2 - 1\n",
        "df['buy'] = df['buysell'] > 0\n",
        "df['sell'] = df['buysell'] < 0\n",
        "df['size_signed'] = df['buysell']*df['SIZE']\n",
        "\n",
        "df['buysell_MA10'] = grp['buysell'].apply( ma10 )\n",
        "df['buysell_MA100'] = grp['buysell'].apply( ma100 )\n",
        "df['buy_MA10'] = grp['buy'].apply( ma10 )\n",
        "df['buy_MA100'] = grp['buy'].apply( ma100 )\n",
        "df['sell_MA10'] = grp['sell'].apply( ma10 )\n",
        "df['sell_MA100'] = grp['sell'].apply( ma100 )\n",
        "df['size_buy_MA10'] = (df['buy']*df['SIZE']).groupby(df['DATE']).apply( ma10 )\n",
        "df['size_buy_MA100'] = (df['buy']*df['SIZE']).groupby(df['DATE']).apply( ma100 )\n",
        "df['size_sell_MA10'] = (df['sell']*df['SIZE']).groupby(df['DATE']).apply( ma10 )\n",
        "df['size_sell_MA100'] = (df['sell']*df['SIZE']).groupby(df['DATE']).apply( ma100 )\n",
        "df['size_signed_MA10'] = grp['size_signed'].apply( ma10 )\n",
        "df['size_signed_MA100'] = grp['size_signed'].apply( ma100 )\n",
        "df['size_MA10'] = grp['SIZE'].apply( ma10 )\n",
        "df['size_MA100'] = grp['SIZE'].apply( ma100 )\n",
        "df['size_signed_MA10_norm'] = df['size_signed_MA10'] / df['size_MA10']\n",
        "df['size_signed_MA100_norm'] = df['size_signed_MA100'] / df['size_MA100']\n",
        "\n",
        "namesTrades = ['buysell_MA10', 'buysell_MA100', 'buy_MA10', 'buy_MA100', 'sell_MA10', 'sell_MA100',\n",
        "    'size_buy_MA10', 'size_buy_MA100', 'size_sell_MA10', 'size_sell_MA100', 'size_signed_MA10', 'size_signed_MA100', \n",
        "    'size_MA10', 'size_MA100', 'size_signed_MA10_norm', 'size_signed_MA100_norm' ]\n",
        "\n",
        "factors += namesTrades\n",
        "\n",
        "### price dynamics factors\n",
        "df['mid_ma10'] = grp['mid'].apply( ma10 )\n",
        "df['mid_ma100'] = grp['mid'].apply( ma100 )\n",
        "df['mid_max10'] = grp['mid'].apply( max10 )\n",
        "df['mid_max100'] = grp['mid'].apply( max100 )\n",
        "df['mid_min10'] = grp['mid'].apply( min10 )\n",
        "df['mid_min100'] = grp['mid'].apply( min100 )\n",
        "\n",
        "df['mid_max10_dev'] = np.log( df['mid_max10'] / df['mid'] )\n",
        "df['mid_max100_dev'] = np.log( df['mid_max100'] / df['mid'] )\n",
        "df['mid_min10_dev'] = np.log( df['mid_min10'] / df['mid'] )\n",
        "df['mid_min100_dev'] = np.log( df['mid_min100'] / df['mid'] )\n",
        "df['mid_maxmin10_dev'] = np.log( df['mid_max10'] / df['mid_min10'] )\n",
        "df['mid_maxmin100_dev'] = np.log( df['mid_max100'] / df['mid_min100'] )\n",
        "df['mid_ma10_dev'] = np.log( df['mid'] / df['mid_ma10'] )\n",
        "df['mid_ma100_dev'] = np.log( df['mid_ma10'] / df['mid_ma100'] )\n",
        "df['mid_ret'] = grp['mid'].apply( ret )\n",
        "df['mid_ret_MA10'] = grp['mid_ret'].apply( ma10 )\n",
        "df['mid_ret_MA100'] = grp['mid_ret'].apply( ma100 )\n",
        "\n",
        "namesMid = ['mid_max10_dev', 'mid_max100_dev', 'mid_min10_dev', 'mid_min100_dev', 'mid_maxmin10_dev', 'mid_maxmin100_dev',\n",
        "            'mid_ma10_dev', 'mid_ma100_dev', 'mid_ret', 'mid_ret_MA10', 'mid_ret_MA100']\n",
        "factors += namesMid\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yRtEJ8qoTXOW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### calculate outputs \n",
        "yname = 'updown'\n",
        "horizon = 10\n",
        "nameMidNext = 'mid_next%s_MA' % horizon\n",
        "#moving average mid of _next_ 10 ticks\n",
        "df[ nameMidNext ] = grp['mid'].apply( lambda x: pd.rolling_mean( x, horizon ) ).shift( -horizon )\n",
        "#difference between average of next 10 ticks and the current mid\n",
        "df[ nameMidNext + '_diff' ] = np.log( df[ 'mid_next%s_MA' % horizon ] / df['mid'] ) *10000 #in bps\n",
        "#if it above or below?\n",
        "df[ yname ] = np.sign( df[ nameMidNext + '_diff' ] )\n",
        "#do some cleaning, strictly need this - because of numerical noise in nameMidNext + '_diff' variable\n",
        "ind = df[ nameMidNext + '_diff' ].abs() < 0.1 #if difference is <0.1bps\n",
        "df[ yname ][ ind ] = 0 #set output to 0\n",
        "\n",
        "## add lagged output as one of factors\n",
        "ynamePrev = 'updown_prev_nz'\n",
        "df['updown_prev'] = grp[ 'updown' ].shift( horizon ) #last _observed_ y-value\n",
        "df[ ynamePrev ] = df['updown_prev'].copy()\n",
        "df[ ynamePrev ][ df[ 'updown_prev_nz' ]==0 ] = np.nan\n",
        "df[ ynamePrev ] = grp['updown_prev_nz'].fillna( method='ffill' )\n",
        "factors += [ 'updown_prev', 'updown_prev_nz' ]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fdgAAUlvTXOZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# transform skewed names \n",
        "#TODO: plot histograms below find out for which input factors \n",
        "#   the histograms are striongly right skewed and put their names here to un-skew\n",
        "#   (note that taking logarithm of 0 will give -inf, so for some variables you \n",
        "#   may get a lot of missing values - better not include such variables)\n",
        "#...........................\n",
        "namesSkewed = pd.Series([ ])\n",
        "namesUnSkewed = 'log_' + namesSkewed\n",
        "df[ namesUnSkewed ] = np.log(df[ namesSkewed ])\n",
        "factors = np.setdiff1d( factors, namesSkewed ).tolist()\n",
        "factors += namesUnSkewed.tolist()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hN8Bn0trTXOb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#TODO: suggest your own addittional factor(s) (at least 1) and compute them here\n",
        "#       explain their meaning in the doc\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "duxj4dLMTXOf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#==============================================================================\n",
        "# %% check distribs and timeseries of inputs\n",
        "# TODO: explain what you can conclude from these plots\n",
        "#==============================================================================\n",
        "\n",
        "grp = df.groupby('DATE')\n",
        "date = min(grp.groups.keys() )\n",
        "df1 = grp.get_group( date )\n",
        "plotInputsHistTimeseries( df1, factors, path )        \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fI0dUu5QTXOj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#==============================================================================\n",
        "# %% filter out null and zero y's\n",
        "#==============================================================================\n",
        "y = df[ yname ]\n",
        "print \"Null values %s, up %s, down %s, 0s %s\" % ( y.isnull().sum(), (y > 0).sum(), (y < 0).sum(), (y == 0).sum() )\n",
        "print \"removing nulls and 0's\"\n",
        "df = df[ y.notnull() & (y!=0)  ]\n",
        "\n",
        "# check number of NaN's and Inf's in the input variables\n",
        "print \"number of NaN values:\\n\", df[ factors ].isnull().sum().sort_values( inplace = False )\n",
        "print \"number of Inf values:\\n\", np.isinf( df[ factors ] ).sum().sort_values( inplace = False )\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7FmnUnF_TXOm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#==============================================================================\n",
        "# %% prepare fit/test dataframes\n",
        "#==============================================================================\n",
        "dates = df['DATE'].unique()\n",
        "grp = df.groupby('DATE')\n",
        "dffit = grp.get_group( dates[0] )\n",
        "dftest = grp.get_group( dates[1] )\n",
        "\n",
        "#sample equal number of objects for fitting\n",
        "grp = dffit.groupby( yname )\n",
        "nsample = min( grp.apply(len) )\n",
        "dffit = grp.apply( lambda x: x.sample( nsample ) )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NqewNjpmTXOo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#==============================================================================\n",
        "# %% selected factors\n",
        "#==============================================================================\n",
        "#TODO: do variable selection (see code below, not in this cell), \n",
        "#       select top best few factors and put them here\n",
        "#      your selected factors should be diverse in meaning\n",
        "#       list the selected variables in the document\n",
        "#    meanwhile, before you do that - use all the factors\n",
        "if False:\n",
        "    factorsSel = [\n",
        "\n",
        "    ]\n",
        "else:\n",
        "    factorsSel = factors"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XVjOyIK4TXOq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#==============================================================================\n",
        "# %% prepare fit and test data numpy arrays with selected factors \n",
        "#TODO: why do we need to use numpy arrays here? explain in the doc.\n",
        "#==============================================================================\n",
        "yfit = dffit[ yname ].values\n",
        "xfit = dffit[ factorsSel ].values\n",
        "#remove records where x or y is NaN or inf\n",
        "ind = np.isnan(yfit) | np.isinf(yfit) | np.isnan(xfit).any( axis=1 ) | np.isinf(xfit).any( axis=1 )\n",
        "xfit, yfit = xfit[ ~ind, :], yfit[~ind]\n",
        "\n",
        "ytest = dftest[ yname ].values\n",
        "ytestPrev = dftest[ ynamePrev ].values\n",
        "xtest = dftest[ factorsSel ].values\n",
        "#remove records where x or y is NaN or inf\n",
        "ind = np.isnan( ytest ) | np.isinf( ytest ) | np.isnan( ytestPrev ) | np.isinf( ytestPrev ) | np.isnan( xtest ).any( axis=1 ) | np.isinf( xtest ).any( axis=1 )\n",
        "xtest, ytest, ytestPrev = xtest[ ~ind, :], ytest[~ind], ytestPrev[~ind]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "85JZU4SQTXOu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#==============================================================================\n",
        "# %% create a classfier object\n",
        "#==============================================================================\n",
        "#this is a flag indicating which classifier we currently use\n",
        "clf_type = 'forest' #'forest', 'logreg'\n",
        "\n",
        "clfs = {}\n",
        "\n",
        "#TODO: create random forest classifier\n",
        "\n",
        "clfs[ 'forest' ] = clf\n",
        "\n",
        "#TODO: create logistic regression classifier\n",
        "\n",
        "clfs[ 'logreg' ] = clf \n",
        "\n",
        "if clf_type in clfs:\n",
        "    clf = clfs[ clf_type ]\n",
        "else:\n",
        "    clf = None\n",
        "    raise ValueError()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_cJZ8iY8TXOw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#==============================================================================\n",
        "# %% feature selection with random forest (may take time)\n",
        "#==============================================================================\n",
        "#TODO: Do feature selection with random forest.\n",
        "#   Analyze the selected features, their inruition. \n",
        "#   How feature selection is affected by the # of trees in the forest\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X9uHPHPQTXOz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#==============================================================================\n",
        "# %% plot features importances\n",
        "#==============================================================================\n",
        "# TODO: plot a bar-plot with feature importances "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dIVg6NifTXO1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#==============================================================================\n",
        "# %% features selection with lasso-logistic regression \n",
        "#==============================================================================\n",
        "from sklearn.preprocessing import scale\n",
        "logreg = sklearn.linear_model.LogisticRegression( penalty='l1', solver='liblinear' )\n",
        "#TODO: explain why we do scale here\n",
        "xfitscale = scale(xfit)\n",
        "#TODO: choose a reasonable C here and fit lasso regression\n",
        "#   explain how result depends on C and how you select C\n",
        "#   compare with features selected by forest\n",
        "C = 1\n",
        "logreg.set_params(C=C)\n",
        "logreg.fit( xfitscale, yfit )\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PCg_wNyXTXO5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#==============================================================================\n",
        "# %% fit the classifier and test on test data\n",
        "#==============================================================================\n",
        "for clf_type, clf in clfs.iteritems():\n",
        "    print '='*80, '\\n', clf_type\n",
        "    clf.fit( xfit, yfit )\n",
        "        \n",
        "    yfitpred = clf.predict( xfit )\n",
        "    print '-'*80, '\\n', \"Train sample perfomance\"\n",
        "    #TODO: here print the Accuracy, Precision and Recall of classification for each class separately\n",
        "\n",
        "    ytestpred = clf.predict( xtest )\n",
        "    \n",
        "    print '-'*80, '\\n', \"Test sample perfomance\"\n",
        "    #TODO: here print the Accuracy, Precision and Recall of classification for each class separately\n",
        "\n",
        "#TODO: explain the fitting/testing results - is there overfitting? \n",
        "#   if so, what do you do to reduce overfitting?"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DoN0y3blTXO8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#==============================================================================\n",
        "# %% print logreg params\n",
        "#==============================================================================\n",
        "clf = clfs['logreg']\n",
        "#TODO: print coefficients of logistic regression\n",
        "#       analyze them - argue that they make sense (or not)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PwNB8aezTXPB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#==============================================================================\n",
        "# %% do cross validation of the classifier\n",
        "#==============================================================================\n",
        "for clf_type, clf in clfs.iteritems():\n",
        "    print '='*80, '\\n', clf_type\n",
        "    #TODO: do cross-validation of each classifier (use sklearn function for this) and print the results\n",
        "    #       discuss them - are they good or bad?"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M2Wsr731TXPD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#==============================================================================\n",
        "# %% test random and baseline on a test sample\n",
        "#==============================================================================\n",
        "\n",
        "# test random preditions on the test data\n",
        "#TODO: generate random predictions for test data and check their accuracy\n",
        "# explain why you gor this accuracy\n",
        "\n",
        "\n",
        "\n",
        "#test baseline model on the test data\n",
        "#TODO: use flat predictions for test data (you computed them when computing inputs/outputs)\n",
        "# and check their accuracy\n",
        "# explain why you gor this accuracy\n",
        "\n",
        "\n",
        "#test INVERTED baseline model on the test data\n",
        "#TODO: use inverted flat predictions for test data (you computed them when computing inputs/outputs)\n",
        "# and check their accuracy\n",
        "# explain why you gor this accuracy\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vA9E-pUgTXPH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#==============================================================================\n",
        "# %% for pairwise features - decision surfaces: \n",
        "#==============================================================================\n",
        "from itertools import combinations\n",
        "path2d = path + \"plots/2d/\"\n",
        "if not os.path.exists( path2d ): os.makedirs( path2d )\n",
        "#TODO: select 3 factors with different meaning and put them here\n",
        "#    plot the below plots and  explain the meaning\n",
        "factorsToDraw = [\n",
        "\n",
        "        \n",
        " ]\n",
        "\n",
        "for clf_type, clf in clfs.iteritems():\n",
        "    print '='*80, '\\n', clf_type\n",
        "\n",
        "    for name1, name2 in combinations(factorsToDraw, 2): \n",
        "        print name1, name2\n",
        "        plotSurface( dffit, name1, name2, yname, clf, path2d, '%s_%s_%s.png' % ( clf_type, name1, name2 ))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UdbXCB0JTXPJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#==============================================================================\n",
        "# %% overfitting example\n",
        "#==============================================================================\n",
        "clf = deepcopy( clfs[ 'forest' ])\n",
        "#TODO: select 2 factors and generate the below example of overfitting, \n",
        "# discuss - why you see the difference in plots and what it means\n",
        "factor1 = ''\n",
        "factor2 = ''\n",
        "for min_samples_leaf in [200, 50, 5]: \n",
        "    clf.min_samples_leaf = min_samples_leaf\n",
        "    clf.min_weight_fraction_leaf = 0.0\n",
        "    plotSurface( dffit, factor1 , factor2, yname, clf, path2d, 'overfit_%s.png' % min_samples_leaf )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pup4aSpGTXPL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#==============================================================================\n",
        "# %% calc train/test sample error vs complexity \n",
        "#==============================================================================\n",
        "clf = deepcopy( clfs[ 'forest' ])\n",
        "clf = skl.tree.DecisionTreeClassifier()\n",
        "accs_train, accs_test = [], []\n",
        "complexities = range(10000, 1000, -100) + range(1000, 100, -100) + range(100, 10, -10) + range(10, 1, -1)\n",
        "\n",
        "for complexity in complexities: \n",
        "    print complexity\n",
        "    clf.min_samples_leaf = complexity\n",
        "    clf.min_weight_fraction_leaf = 0.0\n",
        "    #TODO: fit the forest classifier and test predictions on test data\n",
        "\n",
        "    #TODO: memorize the accuracy on training and test data into the arrays\n",
        "\n",
        "accs_train = pd.Series( accs_train, index=complexities )\n",
        "accs_test = pd.Series( accs_test, index=complexities )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0tGDZ1NZTXPO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#==============================================================================\n",
        "# %% plot train/test sample error vs complexity \n",
        "#==============================================================================\n",
        "#TODO: plot how the train/test _error rate_ depend on complexity of the model\n",
        "# discuss why it has the shape that it has.\n",
        "#   what is the optimal complexity based on this plot?"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lpdeDNV4TXPQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#==============================================================================\n",
        "# %% ROC curves: \n",
        "# http://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_auc_score.html#sklearn.metrics.roc_auc_score\n",
        "# http://scikit-learn.org/stable/auto_examples/model_selection/plot_roc.html#example-model-selection-plot-roc-py\n",
        "# http://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_curve.html#sklearn.metrics.roc_curve\n",
        "#==============================================================================\n",
        "#TODO: plot the ROC curves for both classifiers (one overlayed on top of other), \n",
        "# discuss what you see, which model is better?\n",
        "pathcurves = path + \"plots/curves/\"\n",
        "if not os.path.exists( pathcurves ): os.makedirs( pathcurves )\n",
        "\n",
        "#plot ROC curves for our models\n",
        "for clf_type, clf in clfs.iteritems():  \n",
        "    #...\n",
        "\n",
        "#plot curve for random guessing\n",
        "#TODO also plot the ROC for random predictions (you should generate random probabilities to do that)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HMZr8m8wTXPU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#==============================================================================\n",
        "# %% precision recall curves: \n",
        "# http://scikit-learn.org/stable/modules/generated/sklearn.metrics.precision_recall_curve.html#sklearn.metrics.precision_recall_curve\n",
        "# http://scikit-learn.org/stable/auto_examples/model_selection/plot_precision_recall.html#example-model-selection-plot-precision-recall-py\n",
        "#==============================================================================\n",
        "#TODO: plot the ROC curves for both classifiers (one overlayed on top of other), \n",
        "#discuss what you see, which model is better?\n",
        "pathcurves = path + \"plots/curves/\"\n",
        "if not os.path.exists( pathcurves ): os.makedirs( pathcurves )\n",
        "\n",
        "#plot curves for our models\n",
        "for clf_type, clf in clfs.iteritems():\n",
        "    #...\n",
        "    \n",
        "#plot curve for random guessing\n",
        "#TODO also plot the ROC for random predictions (you should generate random probabilities to do that)\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}