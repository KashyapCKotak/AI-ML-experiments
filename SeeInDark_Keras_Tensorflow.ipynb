{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SeeInDark_Keras_Tensorflow.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KashyapCKotak/AI-ML-experiments/blob/master/SeeInDark_Keras_Tensorflow.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zuMNVBqt0u-H",
        "colab_type": "code",
        "outputId": "12730672-115e-400b-d9a9-3ee4d85651e7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import requests\n",
        "import os\n",
        "\n",
        "def download_file_from_google_drive(id, destination):\n",
        "    URL = \"https://docs.google.com/uc?export=download\"\n",
        "\n",
        "    session = requests.Session()\n",
        "\n",
        "    response = session.get(URL, params = { 'id' : id }, stream = True)\n",
        "    token = get_confirm_token(response)\n",
        "\n",
        "    if token:\n",
        "        params = { 'id' : id, 'confirm' : token }\n",
        "        response = session.get(URL, params = params, stream = True)\n",
        "\n",
        "    save_response_content(response, destination)    \n",
        "\n",
        "def get_confirm_token(response):\n",
        "    for key, value in response.cookies.items():\n",
        "        if key.startswith('download_warning'):\n",
        "            return value\n",
        "\n",
        "    return None\n",
        "\n",
        "def save_response_content(response, destination):\n",
        "    CHUNK_SIZE = 32768\n",
        "\n",
        "    with open(destination, \"wb\") as f:\n",
        "        for chunk in response.iter_content(CHUNK_SIZE):\n",
        "            if chunk: # filter out keep-alive new chunks\n",
        "                f.write(chunk)\n",
        "\n",
        "\n",
        "\n",
        "print('Dowloading Sony subset... (25GB)')\n",
        "download_file_from_google_drive('10kpAcvldtcb9G2ze5hTcF1odzu4V_Zvh', './Sony.zip')\n",
        "\n",
        "#print('Dowloading Fuji subset... (52GB)')\n",
        "#download_file_from_google_drive('12hvKCjwuilKTZPe9EZ7ZTb-azOmUA3HT', 'dataset/Fuji.zip')\n",
        "\n",
        "os.system('unzip ./Sony.zip -d dataset')\n",
        "#os.system('unzip dataset/Fuji.zip -d dataset')\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Dowloading Sony subset... (25GB)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FsP1lrRN-HPz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import shutil\n",
        "shutil.rmtree('result_Sony', ignore_errors=True, onerror=None)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P-_85I6g3eA5",
        "colab_type": "code",
        "outputId": "a45e6ed1-ec00-4408-eb2f-fd76141d66a4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "os.system('unzip ./Sony.zip -d dataset')\n",
        "#os.system('unzip dataset/Fuji.zip -d dataset')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z9yF04nOlUCP",
        "colab_type": "text"
      },
      "source": [
        "**Main Train Code:**\n",
        "================================="
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m_zLVCtYlfSH",
        "colab_type": "code",
        "outputId": "dcb2a19a-8261-482f-f80f-990afe02b6f1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "source": [
        "!pip install rawpy"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: rawpy in /usr/local/lib/python3.6/dist-packages (0.13.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from rawpy) (1.16.4)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IogQOu0slZNT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from __future__ import division\n",
        "import os, time, scipy.io\n",
        "import tensorflow as tf\n",
        "import tensorflow.contrib.slim as slim\n",
        "import numpy as np\n",
        "import rawpy\n",
        "import glob\n",
        "import keras"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-gTj24DXlazJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import shutil\n",
        "shutil.rmtree('result_Sony', ignore_errors=True, onerror=None)\n",
        "\n",
        "input_dir = '/content/drive/My Drive/Data/SeeInDarkDataset/dataset/Sony/short/'\n",
        "gt_dir = '/content/drive/My Drive/Data/SeeInDarkDataset/dataset/Sony/long/'\n",
        "os.mkdir('result_Sony')\n",
        "checkpoint_dir = './result_Sony/'\n",
        "result_dir = './result_Sony/'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k1bgs9-jrfgp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_fns = glob.glob(gt_dir + '0*.ARW')\n",
        "###\n",
        "# 0* is for Training images. 1 for Test and 2 for Validation\n",
        "###\n",
        "train_ids = [int(os.path.basename(train_fn)[0:5]) for train_fn in train_fns]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lbi6fY6ZiBz_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ps = 512  # just a random patch size for training\n",
        "save_freq = 500"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V56pWdxpiGxk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "DEBUG = 0\n",
        "if DEBUG == 1:\n",
        "    save_freq = 2\n",
        "    train_ids = train_ids[0:5]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vexCrDNtiIrN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def lrelu(x): # not used. Used Kears LeakyReLU with alpha 0.2 instead\n",
        "    return tf.maximum(x * 0.2, x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6ZkJ3o0ZiKQk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def upsample_and_concat(x1,x2,output_channels,in_channels):\n",
        "  #x1=vars[0]\n",
        "  #x2=vars[1]\n",
        "  #output_channels=vars[3]\n",
        "  #in_channels=vars[4]\n",
        "  \n",
        "  pool_size = 2\n",
        "  deconv_filter = tf.Variable(tf.truncated_normal([pool_size, pool_size, output_channels, in_channels], stddev=0.02))\n",
        "  deconv = tf.nn.conv2d_transpose(x1, deconv_filter, tf.shape(x2), strides=[1, pool_size, pool_size, 1])\n",
        "  #deconv=keras.layers.Conv2DTranspose(filters=tf.shape(x2),kernel_size=deconv_filter,strides=[1, pool_size, pool_size, 1])(x1)\n",
        "\n",
        "  deconv_output = tf.concat([deconv, x2], 3)\n",
        "  deconv_output.set_shape([None, None, None, output_channels * 2])\n",
        "\n",
        "  return deconv_output"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QUz7pW3dm8Qh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def Depth_to_space_tf(input):\n",
        "  return tf.depth_to_space(input, 2)\n",
        "\n",
        "def build_model():\n",
        "  inputs=keras.layers.Input(shape=(None, None, 4))\n",
        "  #definition: slim.conv2d(inputs,number of outputs, kernel shape)\n",
        "  #definition: slim.max_pool2d(inputsconv2d_transpose ,kernel_size, padding)\n",
        "  #conv1 = slim.conv2d(input, 32, [3, 3], rate=1, activation_fn=lrelu, scope='g_conv1_1')\n",
        "  conv1=keras.layers.Conv2D(filters=32,kernel_size=(3,3),strides=(1,1),name='conv1_1')(inputs)\n",
        "  conv1=keras.layers.LeakyReLU(alpha=0.2,name='conv1_1_relu')(conv1)\n",
        "  #conv1 = slim.conv2d(conv1, 32, [3, 3], rate=1, activation_fn=lrelu, scope='g_conv1_2')\n",
        "  conv1=keras.layers.Conv2D(filters=32,kernel_size=(3,3),strides=(1,1),name='conv1_2')(conv1)\n",
        "  conv1=keras.layers.LeakyReLU(alpha=0.2,name='conv1_2_relu')(conv1)\n",
        "  #pool1 = slim.max_pool2d(conv1, [2, 2], padding='SAME')\n",
        "  pool1=keras.layers.MaxPooling2D(pool_size=(2,2),padding=\"same\",name='pool1')(conv1)\n",
        "\n",
        "  #conv2 = slim.conv2d(pool1, 64, [3, 3], rate=1, activation_fn=lrelu, scope='g_conv2_1')\n",
        "  conv2=keras.layers.Conv2D(filters=64,kernel_size=(3,3),strides=(1,1),name='conv2_1')(pool1)\n",
        "  conv2=keras.layers.LeakyReLU(alpha=0.2,name='conv2_1_relu')(conv2)\n",
        "  #conv2 = slim.conv2d(conv2, 64, [3, 3], rate=1, activation_fn=lrelu, scope='g_conv2_2')\n",
        "  conv2=keras.layers.Conv2D(filters=64,kernel_size=(3,3),strides=(1,1),name='conv2_2')(conv2)\n",
        "  conv2=keras.layers.LeakyReLU(alpha=0.2,name='conv2_2_relu')(conv2)\n",
        "  #pool2 = slim.max_pool2d(conv2, [2, 2], padding='SAME')\n",
        "  pool2=keras.layers.MaxPooling2D(pool_size=(2,2),padding=\"same\",name='pool2')(conv2)\n",
        "\n",
        "  #conv3 = slim.conv2d(pool2, 128, [3, 3], rate=1, activation_fn=lrelu, scope='g_conv3_1')\n",
        "  conv3=keras.layers.Conv2D(filters=128,kernel_size=(3,3),strides=(1,1),name='conv3_1')(pool2)\n",
        "  conv3=keras.layers.LeakyReLU(alpha=0.2,name='conv3_1_relu')(conv3)\n",
        "  #conv3 = slim.conv2d(conv3, 128, [3, 3], rate=1, activation_fn=lrelu, scope='g_conv3_2')\n",
        "  conv3=keras.layers.Conv2D(filters=128,kernel_size=(3,3),strides=(1,1),name='conv3_2')(conv3)\n",
        "  conv3=keras.layers.LeakyReLU(alpha=0.2,name='conv3_2_relu')(conv3)\n",
        "  #pool3 = slim.max_pool2d(conv3, [2, 2], padding='SAME')\n",
        "  pool3=keras.layers.MaxPooling2D(pool_size=(2,2),padding=\"same\",name='pool3')(conv3)\n",
        "\n",
        "  #conv4 = slim.conv2d(pool3, 256, [3, 3], rate=1, activation_fn=lrelu, scope='g_conv4_1')\n",
        "  conv4=keras.layers.Conv2D(filters=256,kernel_size=(3,3),strides=(1,1),name='conv4_1')(pool3)\n",
        "  conv4=keras.layers.LeakyReLU(alpha=0.2,name='conv4_1_relu')(conv4)\n",
        "  #conv4 = slim.conv2d(conv4, 256, [3, 3], rate=1, activation_fn=lrelu, scope='g_conv4_2')\n",
        "  conv4=keras.layers.Conv2D(filters=256,kernel_size=(3,3),strides=(1,1),name='conv4_2')(conv4)\n",
        "  conv4=keras.layers.LeakyReLU(alpha=0.2,name='conv4_2_relu')(conv4)\n",
        "  #pool4 = slim.max_pool2d(conv4, [2, 2], padding='SAME')\n",
        "  pool4=keras.layers.MaxPooling2D(pool_size=(2,2),padding=\"same\",name='pool4')(conv4)\n",
        "\n",
        "  #conv5 = slim.conv2d(pool4, 512, [3, 3], rate=1, activation_fn=lrelu, scope='g_conv5_1')\n",
        "  conv5=keras.layers.Conv2D(filters=512,kernel_size=(3,3),strides=(1,1),name='conv5_1')(pool4)\n",
        "  conv5=keras.layers.LeakyReLU(alpha=0.2,name='conv5_1_relu')(conv5)\n",
        "  #conv5 = slim.conv2d(conv5, 512, [3, 3], rate=1, activation_fn=lrelu, scope='g_conv5_2')\n",
        "  conv5=keras.layers.Conv2D(filters=512,kernel_size=(3,3),strides=(1,1),name='conv5_2')(conv5)\n",
        "  conv5=keras.layers.LeakyReLU(alpha=0.2,name='conv5_2_relu')(conv5)\n",
        "\n",
        "  #up6 = upsample_and_concat(conv5, conv4, 256, 512)\n",
        "  up6=keras.layers.core.Lambda(upsample_and_concat,arguments={'x2':conv4,'output_channels':256,'in_channels':512},name='upsample_concat_1')(conv5)\n",
        "  #conv6 = slim.conv2d(up6, 256, [3, 3], rate=1, activation_fn=lrelu, scope='g_conv6_1')\n",
        "  conv6=keras.layers.Conv2D(filters=256,kernel_size=(3,3),strides=(1,1),name='conv6_1')(up6)\n",
        "  conv6=keras.layers.LeakyReLU(alpha=0.2,name='conv6_1_relu')(conv6)\n",
        "  #conv6 = slim.conv2d(conv6, 256, [3, 3], rate=1, activation_fn=lrelu, scope='g_conv6_2')\n",
        "  conv6=keras.layers.Conv2D(filters=256,kernel_size=(3,3),strides=(1,1),name='conv6_2')(conv6)\n",
        "  conv6=keras.layers.LeakyReLU(alpha=0.2,name='conv6_2_relu')(conv6)\n",
        "\n",
        "  #up7 = upsample_and_concat(conv6, conv3, 128, 256)\n",
        "  up7=keras.layers.core.Lambda(upsample_and_concat,arguments={'x2':conv3,'output_channels':128,'in_channels':256},name='upsample_concat_2')(conv6)\n",
        "  #conv7 = slim.conv2d(up7, 128, [3, 3], rate=1, activation_fn=lrelu, scope='g_conv7_1')\n",
        "  conv7=keras.layers.Conv2D(filters=128,kernel_size=(3,3),strides=(1,1),name='conv7_1')(up7)\n",
        "  conv7=keras.layers.LeakyReLU(alpha=0.2,name='conv7_1_relu')(conv7)\n",
        "  #conv7 = slim.conv2d(conv7, 128, [3, 3], rate=1, activation_fn=lrelu, scope='g_conv7_2')\n",
        "  conv7=keras.layers.Conv2D(filters=128,kernel_size=(3,3),strides=(1,1),name='conv7_2')(conv7)\n",
        "  conv7=keras.layers.LeakyReLU(alpha=0.2,name='conv7_2_relu')(conv7)\n",
        "\n",
        "  #up8 = upsample_and_concat(conv7, conv2, 64, 128)\n",
        "  up8=keras.layers.core.Lambda(upsample_and_concat,arguments={'x2':conv2,'output_channels':64,'in_channels':128},name='upsample_concat_3')(conv7)\n",
        "  #conv8 = slim.conv2d(up8, 64, [3, 3], rate=1, activation_fn=lrelu, scope='g_conv8_1')\n",
        "  conv8=keras.layers.Conv2D(filters=64,kernel_size=(3,3),strides=(1,1),name='conv8_1')(up8)\n",
        "  conv8=keras.layers.LeakyReLU(alpha=0.2,name='conv8_1_relu')(conv8)\n",
        "  #conv8 = slim.conv2d(conv8, 64, [3, 3], rate=1, activation_fn=lrelu, scope='g_conv8_2')\n",
        "  conv8=keras.layers.Conv2D(filters=64,kernel_size=(3,3),strides=(1,1),name='conv8_2')(conv8)\n",
        "  conv8=keras.layers.LeakyReLU(alpha=0.2,name='conv8_2_relu')(conv8)\n",
        "\n",
        "  #up9 = upsample_and_concat(conv8, conv1, 32, 64)\n",
        "  up9=keras.layers.core.Lambda(upsample_and_concat,arguments={'x2':conv1,'output_channels':32,'in_channels':64},name='upsample_concat_4')(conv8)\n",
        "  #conv9 = slim.conv2d(up9, 32, [3, 3], rate=1, activation_fn=lrelu, scope='g_conv9_1')\n",
        "  conv9=keras.layers.Conv2D(filters=32,kernel_size=(3,3),strides=(1,1),name='conv9_1')(up9)\n",
        "  conv9=keras.layers.LeakyReLU(alpha=0.2,name='conv9_1_relu')(conv9)\n",
        "  #conv9 = slim.conv2d(conv9, 32, [3, 3], rate=1, activation_fn=lrelu, scope='g_conv9_2')\n",
        "  conv9=keras.layers.Conv2D(filters=32,kernel_size=(3,3),strides=(1,1),name='conv9_2')(conv9)\n",
        "  conv9=keras.layers.LeakyReLU(alpha=0.2,name='conv9_2_relu')(conv9)\n",
        "\n",
        "  #conv10 = slim.conv2d(conv9, 12, [1, 1], rate=1, activation_fn=None, scope='g_conv10')\n",
        "  conv10=keras.layers.Conv2D(filters=12,kernel_size=(1,1),strides=(1,1),name='conv10')(conv9)\n",
        "  #no activation function for this last layer\n",
        "\n",
        "  predictions = keras.layers.core.Lambda(Depth_to_space_tf,name='depth_to_space')(conv10)\n",
        "  model = keras.models.Model(inputs=inputs, outputs=predictions)\n",
        "  \n",
        "  return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g8DyCh5GyNEU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def pack_raw(raw):\n",
        "    # pack Bayer image to 4 channels\n",
        "    im = raw.raw_image_visible.astype(np.float32)\n",
        "    #print(\"im shape:\",im.shape)\n",
        "    #print(\"im:\")\n",
        "    #print(im)\n",
        "    im = np.maximum(im - 512, 0) / (16383 - 512)  # subtract the black level and normalise with 16383 \n",
        "    ##\n",
        "    # Seems 16383 is the max value due to a 14 bit depth. can be replaced with \n",
        "    ##\n",
        "    #print(\"after normalisation\")\n",
        "    #print(im)\n",
        "\n",
        "    im = np.expand_dims(im, axis=2)\n",
        "    img_shape = im.shape\n",
        "    H = img_shape[0]\n",
        "    W = img_shape[1]\n",
        "    #print(\"after expand dims:%s ,H:%d ,W:%d \" % (img_shape,H,W))\n",
        "\n",
        "    ###\n",
        "    # single bayer pixel format:\n",
        "    # 0:R 1:G\n",
        "    # 2:G 3:B\n",
        "    ###\n",
        "    out = np.concatenate((im[0:H:2, 0:W:2, :], # get the 0th Red as per above format\n",
        "                          im[0:H:2, 1:W:2, :], # get the 1st Green\n",
        "                          im[1:H:2, 1:W:2, :], # get the 2nd Green\n",
        "                          im[1:H:2, 0:W:2, :]), axis=2) # get the 3rd Green\n",
        "    #print(\"out.shape:\",out.shape)\n",
        "    #print(out)\n",
        "    return out\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l6698Y7izIn9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#sess = tf.Session()\n",
        "in_image = tf.placeholder(tf.float32, [None, None, None, 4])\n",
        "gt_image = tf.placeholder(tf.float32, [None, None, None, 3])\n",
        "#out_image = network(in_image)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XXBqHKjN0NB1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#G_loss = tf.reduce_mean(tf.abs(out_image - gt_image))\n",
        "t_vars = tf.trainable_variables()\n",
        "#lr = tf.placeholder(tf.float32)\n",
        "#G_opt = tf.train.AdamOptimizer(learning_rate=lr).minimize(G_loss)\n",
        "#saver = tf.train.Saver()\n",
        "def reduced_mean(y_true,y_pred):\n",
        "  return tf.reduce_mean(tf.abs(y_pred - y_true))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "75AcCZyH0XXL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#ckpt = tf.train.get_checkpoint_state(checkpoint_dir)\n",
        "#if ckpt:\n",
        "#    print('loaded ' + ckpt.model_checkpoint_path)\n",
        "#    saver.restore(sess, ckpt.model_checkpoint_path)\n",
        "###\n",
        "# above commented as not needed can be uncommented\n",
        "###\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hjDLxAdO0zds",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "gt_images = [None] * 6000\n",
        "input_images = {}\n",
        "input_images['300'] = [None] * len(train_ids)\n",
        "input_images['250'] = [None] * len(train_ids)\n",
        "input_images['100'] = [None] * len(train_ids)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0_0TfaMn3xFN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#? g_loss = np.zeros((5000, 1))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "poYFXTqz5G6F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "allfolders = glob.glob('./result/*0')\n",
        "lastepoch = 0\n",
        "for folder in allfolders:\n",
        "    lastepoch = np.maximum(lastepoch, int(folder[-4:]))\n",
        "    \n",
        "###\n",
        "# The above code is used to infer the last completed epoch\n",
        "# ( may be in case a previously saved model is restored)\n",
        "###"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZaoFTFE05usd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "learning_rate = 0.0001"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ArFQgX5BmH9V",
        "colab_type": "code",
        "outputId": "140607a5-4233-42f0-db2c-89e971b99482",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "train_ids[0:10]"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[90, 91, 92, 72, 73, 75, 76, 78, 81, 83]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "icTY4imvYh2k",
        "colab_type": "code",
        "outputId": "edd039ff-ccfe-4a11-9db3-93b746a2306a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "len(train_ids)"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "161"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MkPgT9DWxYhW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_id = 1\n",
        "in_files = glob.glob(input_dir + '%05d_00*.ARW' % train_id) # get all the images path with pattern 0*train_id\n",
        "#print(in_files)\n",
        "in_path = in_files[np.random.randint(0, len(in_files))] # get any one image randomly\n",
        "#print(in_path)\n",
        "in_fn = os.path.basename(in_path) # get only the full image name\n",
        "#print(in_fn)\n",
        "\n",
        "gt_files = glob.glob(gt_dir + '%05d_00*.ARW' % train_id) # get the ground truth image path(s) (only 1 may exist. hence select [0]th below)\n",
        "#print(gt_files)\n",
        "gt_path = gt_files[0] # get the first one (as only 1 gt image should exist)\n",
        "#print(gt_path)\n",
        "gt_fn = os.path.basename(gt_path) # get only the full image name for gt image\n",
        "#print(gt_fn)\n",
        "in_exposure = float(in_fn[9:-5]) # get the exposure for input image\n",
        "#print(in_exposure)\n",
        "gt_exposure = float(gt_fn[9:-5]) # get the exposure for gt image\n",
        "#print(gt_exposure)\n",
        "ratio = min(gt_exposure / in_exposure, 300)\n",
        "print(\"ratio:\",ratio)\n",
        "########################\n",
        "gt_raw = rawpy.imread('/content/drive/My Drive/Data/SeeInDarkDataset/dataset/Sony/long/00001_00_10s.ARW')\n",
        "print(gt_raw.raw_pattern)\n",
        "print(gt_raw)\n",
        "im = gt_raw.postprocess(use_camera_wb=True,\n",
        "                        half_size=False,\n",
        "                        no_auto_bright=True, output_bps=16)\n",
        "print(im.shape)\n",
        "print(im[0:2,:,:])\n",
        "gt_images = np.expand_dims(np.float32(im / 65535.0),\n",
        "        axis=0)\n",
        "print(\"gt_image shape\",gt_images.shape)\n",
        "print(gt_images[:,0:2,:,:])\n",
        "############################\n",
        "raw = rawpy.imread('/content/drive/My Drive/Data/SeeInDarkDataset/dataset/Sony/short/00001_00_0.04s.ARW')\n",
        "packed_input_image=pack_raw(raw)\n",
        "input_img = np.expand_dims(packed_input_image, axis=0) * ratio\n",
        "print(\"input_image shape\",input_img.shape)\n",
        "print(input_img[:,0:2,:,:])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iL2RirGZzS2S",
        "colab_type": "code",
        "outputId": "479c0565-a73a-4c38-97e7-bd829d4f80fd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "model=build_model()\n",
        "print(model.summary())\n",
        "keras.utils.vis_utils.plot_model(model, to_file='model_plot.png', show_shapes=True, show_layer_names=True)\n",
        "model.compile(optimizer='adam',loss=reduced_mean)"
      ],
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_8 (InputLayer)         (None, None, None, 4)     0         \n",
            "_________________________________________________________________\n",
            "conv1_1 (Conv2D)             (None, None, None, 32)    1184      \n",
            "_________________________________________________________________\n",
            "conv1_1_relu (LeakyReLU)     (None, None, None, 32)    0         \n",
            "_________________________________________________________________\n",
            "conv1_2 (Conv2D)             (None, None, None, 32)    9248      \n",
            "_________________________________________________________________\n",
            "conv1_2_relu (LeakyReLU)     (None, None, None, 32)    0         \n",
            "_________________________________________________________________\n",
            "pool1 (MaxPooling2D)         (None, None, None, 32)    0         \n",
            "_________________________________________________________________\n",
            "conv2_1 (Conv2D)             (None, None, None, 64)    18496     \n",
            "_________________________________________________________________\n",
            "conv2_1_relu (LeakyReLU)     (None, None, None, 64)    0         \n",
            "_________________________________________________________________\n",
            "conv2_2 (Conv2D)             (None, None, None, 64)    36928     \n",
            "_________________________________________________________________\n",
            "conv2_2_relu (LeakyReLU)     (None, None, None, 64)    0         \n",
            "_________________________________________________________________\n",
            "pool2 (MaxPooling2D)         (None, None, None, 64)    0         \n",
            "_________________________________________________________________\n",
            "conv3_1 (Conv2D)             (None, None, None, 128)   73856     \n",
            "_________________________________________________________________\n",
            "conv3_1_relu (LeakyReLU)     (None, None, None, 128)   0         \n",
            "_________________________________________________________________\n",
            "conv3_2 (Conv2D)             (None, None, None, 128)   147584    \n",
            "_________________________________________________________________\n",
            "conv3_2_relu (LeakyReLU)     (None, None, None, 128)   0         \n",
            "_________________________________________________________________\n",
            "pool3 (MaxPooling2D)         (None, None, None, 128)   0         \n",
            "_________________________________________________________________\n",
            "conv4_1 (Conv2D)             (None, None, None, 256)   295168    \n",
            "_________________________________________________________________\n",
            "conv4_1_relu (LeakyReLU)     (None, None, None, 256)   0         \n",
            "_________________________________________________________________\n",
            "conv4_2 (Conv2D)             (None, None, None, 256)   590080    \n",
            "_________________________________________________________________\n",
            "conv4_2_relu (LeakyReLU)     (None, None, None, 256)   0         \n",
            "_________________________________________________________________\n",
            "pool4 (MaxPooling2D)         (None, None, None, 256)   0         \n",
            "_________________________________________________________________\n",
            "conv5_1 (Conv2D)             (None, None, None, 512)   1180160   \n",
            "_________________________________________________________________\n",
            "conv5_1_relu (LeakyReLU)     (None, None, None, 512)   0         \n",
            "_________________________________________________________________\n",
            "conv5_2 (Conv2D)             (None, None, None, 512)   2359808   \n",
            "_________________________________________________________________\n",
            "conv5_2_relu (LeakyReLU)     (None, None, None, 512)   0         \n",
            "_________________________________________________________________\n",
            "upsample_concat_1 (Lambda)   (None, None, None, 512)   0         \n",
            "_________________________________________________________________\n",
            "conv6_1 (Conv2D)             (None, None, None, 256)   1179904   \n",
            "_________________________________________________________________\n",
            "conv6_1_relu (LeakyReLU)     (None, None, None, 256)   0         \n",
            "_________________________________________________________________\n",
            "conv6_2 (Conv2D)             (None, None, None, 256)   590080    \n",
            "_________________________________________________________________\n",
            "conv6_2_relu (LeakyReLU)     (None, None, None, 256)   0         \n",
            "_________________________________________________________________\n",
            "upsample_concat_2 (Lambda)   (None, None, None, 256)   0         \n",
            "_________________________________________________________________\n",
            "conv7_1 (Conv2D)             (None, None, None, 128)   295040    \n",
            "_________________________________________________________________\n",
            "conv7_1_relu (LeakyReLU)     (None, None, None, 128)   0         \n",
            "_________________________________________________________________\n",
            "conv7_2 (Conv2D)             (None, None, None, 128)   147584    \n",
            "_________________________________________________________________\n",
            "conv7_2_relu (LeakyReLU)     (None, None, None, 128)   0         \n",
            "_________________________________________________________________\n",
            "upsample_concat_3 (Lambda)   (None, None, None, 128)   0         \n",
            "_________________________________________________________________\n",
            "conv8_1 (Conv2D)             (None, None, None, 64)    73792     \n",
            "_________________________________________________________________\n",
            "conv8_1_relu (LeakyReLU)     (None, None, None, 64)    0         \n",
            "_________________________________________________________________\n",
            "conv8_2 (Conv2D)             (None, None, None, 64)    36928     \n",
            "_________________________________________________________________\n",
            "conv8_2_relu (LeakyReLU)     (None, None, None, 64)    0         \n",
            "_________________________________________________________________\n",
            "upsample_concat_4 (Lambda)   (None, None, None, 64)    0         \n",
            "_________________________________________________________________\n",
            "conv9_1 (Conv2D)             (None, None, None, 32)    18464     \n",
            "_________________________________________________________________\n",
            "conv9_1_relu (LeakyReLU)     (None, None, None, 32)    0         \n",
            "_________________________________________________________________\n",
            "conv9_2 (Conv2D)             (None, None, None, 32)    9248      \n",
            "_________________________________________________________________\n",
            "conv9_2_relu (LeakyReLU)     (None, None, None, 32)    0         \n",
            "_________________________________________________________________\n",
            "conv10 (Conv2D)              (None, None, None, 12)    396       \n",
            "_________________________________________________________________\n",
            "depth_to_space (Lambda)      (None, None, None, 3)     0         \n",
            "=================================================================\n",
            "Total params: 7,063,948\n",
            "Trainable params: 7,063,948\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W0726 11:03:46.557630 139637959735168 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K6rt0FGT6jY-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for epoch in range(lastepoch, 4001):\n",
        "    #if os.path.isdir('result/%04d' % epoch):\n",
        "    #    continue\n",
        "    ###\n",
        "    # used for skipping the current epoch if the checkpoint directory exists\n",
        "    ###\n",
        "    cnt = 0\n",
        "    if epoch > 2000:\n",
        "        learning_rate = 1e-5\n",
        "\n",
        "    for ind in np.random.permutation(len(train_ids)):\n",
        "\n",
        "        # get the path from image id\n",
        "\n",
        "        train_id = train_ids[ind]\n",
        "        in_files = glob.glob(input_dir + '%05d_00*.ARW' % train_id) # get all the images path with pattern 0*train_id\n",
        "        in_path = in_files[np.random.randint(0, len(in_files))] # get any one image randomly\n",
        "        in_fn = os.path.basename(in_path) # get only the full image name\n",
        "\n",
        "        gt_files = glob.glob(gt_dir + '%05d_00*.ARW' % train_id) # get the ground truth image path(s) (only 1 may exist. hence select [0]th below)\n",
        "        gt_path = gt_files[0] # get the first one (as only 1 gt image should exist)\n",
        "        gt_fn = os.path.basename(gt_path) # get only the full image name for gt image\n",
        "        in_exposure = float(in_fn[9:-5]) # get the exposure for input image\n",
        "        gt_exposure = float(gt_fn[9:-5]) # get the exposure for gt image\n",
        "        ratio = min(gt_exposure / in_exposure, 300) # get the amplification ratio\n",
        "\n",
        "        st = time.time()\n",
        "        cnt += 1\n",
        "\n",
        "        if input_images[str(ratio)[0:3]][ind] is None: # if image is not loaded (first epoch), load it\n",
        "            raw = rawpy.imread(in_path)\n",
        "            input_images[str(ratio)[0:3]][ind] = \\\n",
        "                np.expand_dims(pack_raw(raw), axis=0) * ratio # pack the bayer image in 4 channels of RGBG\n",
        "\n",
        "            gt_raw = rawpy.imread(gt_path)\n",
        "            im = gt_raw.postprocess(use_camera_wb=True,\n",
        "                                    half_size=False,\n",
        "                                    no_auto_bright=True, output_bps=16)\n",
        "            gt_images[ind] = np.expand_dims(np.float32(im / 65535.0),axis=0) # divide by 65535 to normalise (scale between 0 and 1)\n",
        "\n",
        "        # crop\n",
        "\n",
        "        H = input_images[str(ratio)[0:3]][ind].shape[1] # get the image height (number of rows)\n",
        "        W = input_images[str(ratio)[0:3]][ind].shape[2] # get the image width (number of columns)\n",
        "\n",
        "        xx = np.random.randint(0, W - ps) # get a random number in W-ps (W-512)\n",
        "        yy = np.random.randint(0, H - ps) # get a random number in H-ps (H-512)\n",
        "        input_patch = input_images[str(ratio)[0:3]][ind][:, yy:yy + ps,\n",
        "                xx:xx + ps, :]\n",
        "        gt_patch = gt_images[ind][:, yy * 2:yy * 2 + ps * 2, xx * 2:xx\n",
        "                                  * 2 + ps * 2, :]\n",
        "\n",
        "        if np.random.randint(2) == 1:  # random flip for rows\n",
        "            input_patch = np.flip(input_patch, axis=1)\n",
        "            gt_patch = np.flip(gt_patch, axis=1)\n",
        "        if np.random.randint(2) == 1:  # random flip for columns\n",
        "            input_patch = np.flip(input_patch, axis=2)\n",
        "            gt_patch = np.flip(gt_patch, axis=2)\n",
        "        if np.random.randint(2) == 1:  # random transpose\n",
        "            input_patch = np.transpose(input_patch, (0, 2, 1, 3))\n",
        "            gt_patch = np.transpose(gt_patch, (0, 2, 1, 3))\n",
        "\n",
        "        input_patch = np.minimum(input_patch, 1.0)\n",
        "\n",
        "        (_, G_current, output) = sess.run([G_opt, G_loss, out_image],\n",
        "                feed_dict={in_image: input_patch, gt_image: gt_patch,\n",
        "                lr: learning_rate})\n",
        "        output = np.minimum(np.maximum(output, 0), 1)\n",
        "        g_loss[ind] = G_current\n",
        "\n",
        "        print '%d %d Loss=%.3f Time=%.3f' % (epoch, cnt,\n",
        "                np.mean(g_loss[np.where(g_loss)]), time.time() - st)\n",
        "\n",
        "        if epoch % save_freq == 0:\n",
        "            if not os.path.isdir(result_dir + '%04d' % epoch):\n",
        "                os.makedirs(result_dir + '%04d' % epoch)\n",
        "\n",
        "            temp = np.concatenate((gt_patch[0, :, :, :], output[0, :, :\n",
        "                                  , :]), axis=1)\n",
        "            scipy.misc.toimage(temp * 255, high=255, low=0, cmin=0,\n",
        "                               cmax=255).save(result_dir\n",
        "                    + '%04d/%05d_00_train_%d.jpg' % (epoch, train_id,\n",
        "                    ratio))\n",
        "\n",
        "    saver.save(sess, checkpoint_dir + 'model.ckpt')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BmI_d7BV0aXj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GxEdVlGM0ax7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hCfPJXmz0bGZ",
        "colab_type": "code",
        "outputId": "db4dd6b0-8328-4599-eda3-354ba8d93ff4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "from keras.layers import Input, Dense\n",
        "from keras.models import Model\n",
        "\n",
        "# This returns a tensor\n",
        "inputs = Input(shape=(784,))\n",
        "tf.print(inputs)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Operation 'PrintV2' type=PrintV2>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rvevq8NP0cal",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}