{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SeeInDark_Keras_Tensorflow_Sequence.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python2",
      "display_name": "Python 2"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KashyapCKotak/AI-ML-experiments/blob/master/SeeInDark_Keras_Tensorflow_Sequence.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zuMNVBqt0u-H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# import requests\n",
        "# import os\n",
        "\n",
        "# def download_file_from_google_drive(id, destination):\n",
        "#     URL = \"https://docs.google.com/uc?export=download\"\n",
        "\n",
        "#     session = requests.Session()\n",
        "\n",
        "#     response = session.get(URL, params = { 'id' : id }, stream = True)\n",
        "#     token = get_confirm_token(response)\n",
        "\n",
        "#     if token:\n",
        "#         params = { 'id' : id, 'confirm' : token }\n",
        "#         response = session.get(URL, params = params, stream = True)\n",
        "\n",
        "#     save_response_content(response, destination)    \n",
        "\n",
        "# def get_confirm_token(response):\n",
        "#     for key, value in response.cookies.items():\n",
        "#         if key.startswith('download_warning'):\n",
        "#             return value\n",
        "\n",
        "#     return None\n",
        "\n",
        "# def save_response_content(response, destination):\n",
        "#     CHUNK_SIZE = 32768\n",
        "\n",
        "#     with open(destination, \"wb\") as f:\n",
        "#         for chunk in response.iter_content(CHUNK_SIZE):\n",
        "#             if chunk: # filter out keep-alive new chunks\n",
        "#                 f.write(chunk)\n",
        "\n",
        "\n",
        "\n",
        "# print('Dowloading Sony subset... (25GB)')\n",
        "# download_file_from_google_drive('10kpAcvldtcb9G2ze5hTcF1odzu4V_Zvh', './Sony.zip')\n",
        "\n",
        "# #print('Dowloading Fuji subset... (52GB)')\n",
        "# #download_file_from_google_drive('12hvKCjwuilKTZPe9EZ7ZTb-azOmUA3HT', 'dataset/Fuji.zip')\n",
        "\n",
        "# os.system('unzip ./Sony.zip -d dataset')\n",
        "# #os.system('unzip dataset/Fuji.zip -d dataset')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FsP1lrRN-HPz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# import shutil\n",
        "# shutil.rmtree('result_Sony', ignore_errors=True, onerror=None)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P-_85I6g3eA5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# os.system('unzip ./Sony.zip -d dataset')\n",
        "#os.system('unzip dataset/Fuji.zip -d dataset')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xDQVxKF8nooG",
        "colab_type": "code",
        "outputId": "7166590c-9e75-4bfc-9f99-2c6ac3abbc86",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z9yF04nOlUCP",
        "colab_type": "text"
      },
      "source": [
        "**Main Train Code:**\n",
        "================================="
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m_zLVCtYlfSH",
        "colab_type": "code",
        "outputId": "8c96e1c4-fbb9-470d-94e6-76076df60536",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "!pip install rawpy"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: rawpy in /usr/local/lib/python3.6/dist-packages (0.13.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from rawpy) (1.16.4)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IogQOu0slZNT",
        "colab_type": "code",
        "outputId": "43627fb9-8f39-4594-8150-41f3f3ec223a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "from __future__ import division\n",
        "import os, time, scipy.io\n",
        "import tensorflow as tf\n",
        "import tensorflow.contrib.slim as slim\n",
        "import numpy as np\n",
        "import rawpy\n",
        "import glob\n",
        "import keras\n",
        "import shutil\n",
        "import datetime as dt\n",
        "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "import threading"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-gTj24DXlazJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# import shutil\n",
        "# shutil.rmtree('result_Sony', ignore_errors=True, onerror=None)\n",
        "\n",
        "# input_dir = '/content/drive/My Drive/Data/SeeInDarkDataset/dataset/Sony/short/'\n",
        "# gt_dir = '/content/drive/My Drive/Data/SeeInDarkDataset/dataset/Sony/long/'\n",
        "# os.mkdir('result_Sony')\n",
        "# checkpoint_dir = './result_Sony/'\n",
        "# result_dir = './result_Sony/'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k1bgs9-jrfgp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# train_fns = glob.glob(gt_dir + '0*.ARW')\n",
        "###\n",
        "# 0* is for Training images. 1 for Test and 2 for Validation\n",
        "###\n",
        "# train_ids = [int(os.path.basename(train_fn)[0:5]) for train_fn in train_fns]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lbi6fY6ZiBz_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# ps = 512  # just a random patch size for training"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V56pWdxpiGxk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# save_freq = 500\n",
        "# DEBUG = 0\n",
        "# if DEBUG == 1:\n",
        "#     save_freq = 2\n",
        "#     train_ids = train_ids[0:5]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vexCrDNtiIrN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def lrelu(x): # not used. Used Kears LeakyReLU with alpha 0.2 instead\n",
        "    return tf.maximum(x * 0.2, x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6ZkJ3o0ZiKQk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\" This cell contains the custom Lambda layers to be used in building the  keras mdoel \"\"\"\n",
        "def upsample_and_concat(x1,x2,output_channels,in_channels,layer):\n",
        "  print(\"layer:\",layer)\n",
        "  pool_size = 2\n",
        "  deconv_filter = tf.Variable(tf.truncated_normal([pool_size, pool_size, output_channels, in_channels], stddev=0.02))\n",
        "  print(\"x1 shape:\",x1.shape)\n",
        "  print(\"x2 shape:\",x2.shape)\n",
        "  deconvtf=tf.nn.conv2d_transpose(x1, deconv_filter, tf.shape(x2), strides=[1, pool_size, pool_size, 1])\n",
        "  print(\"deconvtf shape:\",deconvtf.shape)\n",
        "  #deconv=keras.layers.Conv2DTranspose(filters=output_channels,kernel_size=(pool_size, pool_size),strides=(pool_size, pool_size))(x1)\n",
        "  #print(\"deconvk shape:\",deconv.shape)\n",
        "\n",
        "  deconv_output = tf.concat([deconvtf, x2], 3)\n",
        "  print(\"deconv_output shape:\",deconv_output.shape)\n",
        "  deconv_output.set_shape([None, None, None, output_channels * 2])\n",
        "\n",
        "  return deconv_output\n",
        "\n",
        "def Depth_to_space_tf(input):\n",
        "  return tf.depth_to_space(input, 2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2cRpNNVlYXKz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# testing code\n",
        "# t=tf.ones([1,450,350,4])\n",
        "# deconv_filter = tf.Variable(tf.truncated_normal([2, 2, 256, 512], stddev=0.02))\n",
        "# tf.nn.conv2d_transpose(t, deconv_filter, [1,450,350,], strides=[1, pool_size, pool_size, 1])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QUz7pW3dm8Qh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def build_model():\n",
        "  inputs=keras.layers.Input(shape=(None, None, 4))\n",
        "  #definition: slim.conv2d(inputs,number of outputs, kernel shape)\n",
        "  #definition: slim.max_pool2d(inputsconv2d_transpose ,kernel_size, padding)\n",
        "  #conv1 = slim.conv2d(input, 32, [3, 3], rate=1, activation_fn=lrelu, scope='g_conv1_1')\n",
        "  conv1=keras.layers.Conv2D(filters=32,kernel_size=(3,3),strides=(1,1),padding='same',name='conv1_1')(inputs)\n",
        "  conv1=keras.layers.LeakyReLU(alpha=0.2,name='conv1_1_relu')(conv1)\n",
        "  print(\"conv1 shape:\",conv1.shape)\n",
        "  #conv1 = slim.conv2d(conv1, 32, [3, 3], rate=1, activation_fn=lrelu, scope='g_conv1_2')\n",
        "  conv1=keras.layers.Conv2D(filters=32,kernel_size=(3,3),strides=(1,1),padding='same',name='conv1_2')(conv1)\n",
        "  conv1=keras.layers.LeakyReLU(alpha=0.2,name='conv1_2_relu')(conv1)\n",
        "  print(\"conv1 shape:\",conv1.shape)\n",
        "  #pool1 = slim.max_pool2d(conv1, [2, 2], padding='SAME')\n",
        "  pool1=keras.layers.MaxPooling2D(pool_size=(2,2),padding=\"same\",name='pool1')(conv1)\n",
        "  print(\"conv1 shape after pool:\",pool1.shape)\n",
        "\n",
        "  #conv2 = slim.conv2d(pool1, 64, [3, 3], rate=1, activation_fn=lrelu, scope='g_conv2_1')\n",
        "  conv2=keras.layers.Conv2D(filters=64,kernel_size=(3,3),strides=(1,1),padding='same',name='conv2_1')(pool1)\n",
        "  conv2=keras.layers.LeakyReLU(alpha=0.2,name='conv2_1_relu')(conv2)\n",
        "  #conv2 = slim.conv2d(conv2, 64, [3, 3], rate=1, activation_fn=lrelu, scope='g_conv2_2')\n",
        "  conv2=keras.layers.Conv2D(filters=64,kernel_size=(3,3),strides=(1,1),padding='same',name='conv2_2')(conv2)\n",
        "  conv2=keras.layers.LeakyReLU(alpha=0.2,name='conv2_2_relu')(conv2)\n",
        "  #pool2 = slim.max_pool2d(conv2, [2, 2], padding='SAME')\n",
        "  pool2=keras.layers.MaxPooling2D(pool_size=(2,2),padding=\"same\",name='pool2')(conv2)\n",
        "\n",
        "  #conv3 = slim.conv2d(pool2, 128, [3, 3], rate=1, activation_fn=lrelu, scope='g_conv3_1')\n",
        "  conv3=keras.layers.Conv2D(filters=128,kernel_size=(3,3),strides=(1,1),padding='same',name='conv3_1')(pool2)\n",
        "  conv3=keras.layers.LeakyReLU(alpha=0.2,name='conv3_1_relu')(conv3)\n",
        "  #conv3 = slim.conv2d(conv3, 128, [3, 3], rate=1, activation_fn=lrelu, scope='g_conv3_2')\n",
        "  conv3=keras.layers.Conv2D(filters=128,kernel_size=(3,3),strides=(1,1),padding='same',name='conv3_2')(conv3)\n",
        "  conv3=keras.layers.LeakyReLU(alpha=0.2,name='conv3_2_relu')(conv3)\n",
        "  #pool3 = slim.max_pool2d(conv3, [2, 2], padding='SAME')\n",
        "  pool3=keras.layers.MaxPooling2D(pool_size=(2,2),padding=\"same\",name='pool3')(conv3)\n",
        "\n",
        "  #conv4 = slim.conv2d(pool3, 256, [3, 3], rate=1, activation_fn=lrelu, scope='g_conv4_1')\n",
        "  conv4=keras.layers.Conv2D(filters=256,kernel_size=(3,3),strides=(1,1),padding='same',name='conv4_1')(pool3)\n",
        "  conv4=keras.layers.LeakyReLU(alpha=0.2,name='conv4_1_relu')(conv4)\n",
        "  #conv4 = slim.conv2d(conv4, 256, [3, 3], rate=1, activation_fn=lrelu, scope='g_conv4_2')\n",
        "  conv4=keras.layers.Conv2D(filters=256,kernel_size=(3,3),strides=(1,1),padding='same',name='conv4_2')(conv4)\n",
        "  conv4=keras.layers.LeakyReLU(alpha=0.2,name='conv4_2_relu')(conv4)\n",
        "  #pool4 = slim.max_pool2d(conv4, [2, 2], padding='SAME')\n",
        "  pool4=keras.layers.MaxPooling2D(pool_size=(2,2),padding=\"same\",name='pool4')(conv4)\n",
        "\n",
        "  #conv5 = slim.conv2d(pool4, 512, [3, 3], rate=1, activation_fn=lrelu, scope='g_conv5_1')\n",
        "  conv5=keras.layers.Conv2D(filters=512,kernel_size=(3,3),strides=(1,1),padding='same',name='conv5_1')(pool4)\n",
        "  conv5=keras.layers.LeakyReLU(alpha=0.2,name='conv5_1_relu')(conv5)\n",
        "  #conv5 = slim.conv2d(conv5, 512, [3, 3], rate=1, activation_fn=lrelu, scope='g_conv5_2')\n",
        "  conv5=keras.layers.Conv2D(filters=512,kernel_size=(3,3),strides=(1,1),padding='same',name='conv5_2')(conv5)\n",
        "  conv5=keras.layers.LeakyReLU(alpha=0.2,name='conv5_2_relu')(conv5)\n",
        "\n",
        "  #up6 = upsample_and_concat(conv5, conv4, 256, 512)\n",
        "  up6=keras.layers.core.Lambda(upsample_and_concat,arguments={'x2':conv4,'output_channels':256,'in_channels':512,'layer':'upsample_concat_1'},name='upsample_concat_1')(conv5)\n",
        "  #conv6 = slim.conv2d(up6, 256, [3, 3], rate=1, activation_fn=lrelu, scope='g_conv6_1')\n",
        "  conv6=keras.layers.Conv2D(filters=256,kernel_size=(3,3),strides=(1,1),padding='same',name='conv6_1')(up6)\n",
        "  conv6=keras.layers.LeakyReLU(alpha=0.2,name='conv6_1_relu')(conv6)\n",
        "  #conv6 = slim.conv2d(conv6, 256, [3, 3], rate=1, activation_fn=lrelu, scope='g_conv6_2')\n",
        "  conv6=keras.layers.Conv2D(filters=256,kernel_size=(3,3),strides=(1,1),padding='same',name='conv6_2')(conv6)\n",
        "  conv6=keras.layers.LeakyReLU(alpha=0.2,name='conv6_2_relu')(conv6)\n",
        "\n",
        "  #up7 = upsample_and_concat(conv6, conv3, 128, 256)\n",
        "  up7=keras.layers.core.Lambda(upsample_and_concat,arguments={'x2':conv3,'output_channels':128,'in_channels':256,'layer':'upsample_concat_2'},name='upsample_concat_2')(conv6)\n",
        "  #conv7 = slim.conv2d(up7, 128, [3, 3], rate=1, activation_fn=lrelu, scope='g_conv7_1')\n",
        "  conv7=keras.layers.Conv2D(filters=128,kernel_size=(3,3),strides=(1,1),padding='same',name='conv7_1')(up7)\n",
        "  conv7=keras.layers.LeakyReLU(alpha=0.2,name='conv7_1_relu')(conv7)\n",
        "  #conv7 = slim.conv2d(conv7, 128, [3, 3], rate=1, activation_fn=lrelu, scope='g_conv7_2')\n",
        "  conv7=keras.layers.Conv2D(filters=128,kernel_size=(3,3),strides=(1,1),padding='same',name='conv7_2')(conv7)\n",
        "  conv7=keras.layers.LeakyReLU(alpha=0.2,name='conv7_2_relu')(conv7)\n",
        "\n",
        "  #up8 = upsample_and_concat(conv7, conv2, 64, 128)\n",
        "  up8=keras.layers.core.Lambda(upsample_and_concat,arguments={'x2':conv2,'output_channels':64,'in_channels':128,'layer':'upsample_concat_3'},name='upsample_concat_3')(conv7)\n",
        "  #conv8 = slim.conv2d(up8, 64, [3, 3], rate=1, activation_fn=lrelu, scope='g_conv8_1')\n",
        "  conv8=keras.layers.Conv2D(filters=64,kernel_size=(3,3),strides=(1,1),padding='same',name='conv8_1')(up8)\n",
        "  conv8=keras.layers.LeakyReLU(alpha=0.2,name='conv8_1_relu')(conv8)\n",
        "  #conv8 = slim.conv2d(conv8, 64, [3, 3], rate=1, activation_fn=lrelu, scope='g_conv8_2')\n",
        "  conv8=keras.layers.Conv2D(filters=64,kernel_size=(3,3),strides=(1,1),padding='same',name='conv8_2')(conv8)\n",
        "  conv8=keras.layers.LeakyReLU(alpha=0.2,name='conv8_2_relu')(conv8)\n",
        "\n",
        "  #up9 = upsample_and_concat(conv8, conv1, 32, 64)\n",
        "  up9=keras.layers.core.Lambda(upsample_and_concat,arguments={'x2':conv1,'output_channels':32,'in_channels':64,'layer':'upsample_concat_4'},name='upsample_concat_4')(conv8)\n",
        "  #conv9 = slim.conv2d(up9, 32, [3, 3], rate=1, activation_fn=lrelu, scope='g_conv9_1')\n",
        "  conv9=keras.layers.Conv2D(filters=32,kernel_size=(3,3),strides=(1,1),padding='same',name='conv9_1')(up9)\n",
        "  conv9=keras.layers.LeakyReLU(alpha=0.2,name='conv9_1_relu')(conv9)\n",
        "  #conv9 = slim.conv2d(conv9, 32, [3, 3], rate=1, activation_fn=lrelu, scope='g_conv9_2')\n",
        "  conv9=keras.layers.Conv2D(filters=32,kernel_size=(3,3),strides=(1,1),padding='same',name='conv9_2')(conv9)\n",
        "  conv9=keras.layers.LeakyReLU(alpha=0.2,name='conv9_2_relu')(conv9)\n",
        "  print(\"conv9 shape:\",conv9.shape)\n",
        "\n",
        "  #conv10 = slim.conv2d(conv9, 12, [1, 1], rate=1, activation_fn=None, scope='g_conv10')\n",
        "  conv10=keras.layers.Conv2D(filters=12,kernel_size=(1,1),strides=(1,1),name='conv10')(conv9)\n",
        "  print(\"conv10 shape:\",conv10.shape)\n",
        "  #no activation function for this last layer\n",
        "\n",
        "  predictions = keras.layers.core.Lambda(Depth_to_space_tf,name='depth_to_space')(conv10)\n",
        "  model = keras.models.Model(inputs=inputs, outputs=predictions)\n",
        "  \n",
        "  return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g8DyCh5GyNEU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def pack_raw(raw):\n",
        "    # pack Bayer image to 4 channels\n",
        "    im = raw.raw_image_visible.astype(np.float32)\n",
        "    #print(\"im shape:\",im.shape)\n",
        "    #print(\"im:\")\n",
        "    #print(im)\n",
        "    im = np.maximum(im - 512, 0) / (16383 - 512)  # subtract the black level and normalise with 16383 \n",
        "    ##\n",
        "    # Seems 16383 is the max value due to a 14 bit depth. can be replaced with \n",
        "    ##\n",
        "    #print(\"after normalisation\")\n",
        "    #print(im)\n",
        "\n",
        "    im = np.expand_dims(im, axis=2)\n",
        "    img_shape = im.shape\n",
        "    H = img_shape[0]\n",
        "    W = img_shape[1]\n",
        "    #print(\"after expand dims:%s ,H:%d ,W:%d \" % (img_shape,H,W))\n",
        "\n",
        "    ###\n",
        "    # single bayer pixel format:\n",
        "    # 0:R 1:G\n",
        "    # 2:G 3:B\n",
        "    ###\n",
        "    out = np.concatenate((im[0:H:2, 0:W:2, :], # get the 0th Red as per above format\n",
        "                          im[0:H:2, 1:W:2, :], # get the 1st Green\n",
        "                          im[1:H:2, 1:W:2, :], # get the 2nd Green\n",
        "                          im[1:H:2, 0:W:2, :]), axis=2) # get the 3rd Green\n",
        "    #print(\"out.shape:\",out.shape)\n",
        "    #print(out)\n",
        "    return out\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l6698Y7izIn9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#sess = tf.Session()\n",
        "# in_image = tf.placeholder(tf.float32, [None, None, None, 4])\n",
        "# gt_image = tf.placeholder(tf.float32, [None, None, None, 3])\n",
        "#out_image = network(in_image)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XXBqHKjN0NB1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#G_loss = tf.reduce_mean(tf.abs(out_image - gt_image))\n",
        "# t_vars = tf.trainable_variables()\n",
        "#lr = tf.placeholder(tf.float32)\n",
        "#G_opt = tf.train.AdamOptimizer(learning_rate=lr).minimize(G_loss)\n",
        "#saver = tf.train.Saver()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "75AcCZyH0XXL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#ckpt = tf.train.get_checkpoint_state(checkpoint_dir)\n",
        "#if ckpt:\n",
        "#    print('loaded ' + ckpt.model_checkpoint_path)\n",
        "#    saver.restore(sess, ckpt.model_checkpoint_path)\n",
        "###\n",
        "# above commented as not needed can be uncommented\n",
        "###\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hjDLxAdO0zds",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# gt_images = [None] * 6000\n",
        "# input_images = {}\n",
        "# input_images['300'] = [None] * len(train_ids)\n",
        "# input_images['250'] = [None] * len(train_ids)\n",
        "# input_images['100'] = [None] * len(train_ids)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0_0TfaMn3xFN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#? g_loss = np.zeros((5000, 1))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "poYFXTqz5G6F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# allfolders = glob.glob('./result/*0')\n",
        "# lastepoch = 0\n",
        "# for folder in allfolders:\n",
        "#     lastepoch = np.maximum(lastepoch, int(folder[-4:]))\n",
        "    \n",
        "###\n",
        "# The above code is used to infer the last completed epoch\n",
        "# ( may be in case a previously saved model is restored)\n",
        "###"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZaoFTFE05usd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# learning_rate = 0.0001"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ArFQgX5BmH9V",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# train_ids[0:10]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "icTY4imvYh2k",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# len(train_ids)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MkPgT9DWxYhW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# train_id = 1\n",
        "# in_files = glob.glob(input_dir + '%05d_00*.ARW' % train_id) # get all the images path with pattern 0*train_id\n",
        "# #print(in_files)\n",
        "# in_path = in_files[np.random.randint(0, len(in_files))] # get any one image randomly\n",
        "# #print(in_path)\n",
        "# in_fn = os.path.basename(in_path) # get only the full image name\n",
        "# #print(in_fn)\n",
        "\n",
        "# gt_files = glob.glob(gt_dir + '%05d_00*.ARW' % train_id) # get the ground truth image path(s) (only 1 may exist. hence select [0]th below)\n",
        "# #print(gt_files)\n",
        "# gt_path = gt_files[0] # get the first one (as only 1 gt image should exist)\n",
        "# #print(gt_path)\n",
        "# gt_fn = os.path.basename(gt_path) # get only the full image name for gt image\n",
        "# #print(gt_fn)\n",
        "# in_exposure = float(in_fn[9:-5]) # get the exposure for input image\n",
        "# #print(in_exposure)\n",
        "# gt_exposure = float(gt_fn[9:-5]) # get the exposure for gt image\n",
        "# #print(gt_exposure)\n",
        "# ratio = min(gt_exposure / in_exposure, 300)\n",
        "# print(\"ratio:\",ratio)\n",
        "# ########################\n",
        "# gt_raw = rawpy.imread('/content/drive/My Drive/Data/SeeInDarkDataset/dataset/Sony/long/00001_00_10s.ARW')\n",
        "# print(gt_raw.raw_pattern)\n",
        "# print(gt_raw)\n",
        "# im = gt_raw.postprocess(use_camera_wb=True,\n",
        "#                         half_size=False,\n",
        "#                         no_auto_bright=True, output_bps=16)\n",
        "# print(im.shape)\n",
        "# print(im[0:2,:,:])\n",
        "# gt_images = np.expand_dims(np.float32(im / 65535.0),\n",
        "#         axis=0)\n",
        "# print(\"gt_image shape\",gt_images.shape)\n",
        "# print(gt_images[:,0:2,:,:])\n",
        "# ############################\n",
        "# raw = rawpy.imread('/content/drive/My Drive/Data/SeeInDarkDataset/dataset/Sony/short/00001_00_0.04s.ARW')\n",
        "# packed_input_image=pack_raw(raw)\n",
        "# input_img = np.expand_dims(packed_input_image, axis=0) * ratio\n",
        "# print(\"input_image shape\",input_img.shape)\n",
        "# print(input_img[:,0:2,:,:])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XZKmZNa_0H7N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Params():\n",
        "  def __init__(self,epochs):\n",
        "    shutil.rmtree('/content/drive/My Drive/Data/SeeInDarkDataset/dataset/Sony/result_Sony', ignore_errors=True, onerror=None)\n",
        "    self.input_dir = '/content/drive/My Drive/Data/SeeInDarkDataset/dataset/Sony/short/'\n",
        "    self.gt_dir = '/content/drive/My Drive/Data/SeeInDarkDataset/dataset/Sony/long/'\n",
        "    os.mkdir('/content/drive/My Drive/Data/SeeInDarkDataset/dataset/Sony/result_Sony')\n",
        "    self.checkpoint_dir = '/content/drive/My Drive/Data/SeeInDarkDataset/dataset/Sony/result_Sony/'\n",
        "    self.result_dir = '/content/drive/My Drive/Data/SeeInDarkDataset/dataset/Sony/result_Sony/'\n",
        "    self.save_dir = '/content/drive/My Drive/Data/SeeInDarkDataset/dataset/Sony/save_Sony/'\n",
        "    if os.path.exists(self.save_dir):\n",
        "      pass\n",
        "    else:\n",
        "      os.mkdir('/content/drive/My Drive/Data/SeeInDarkDataset/dataset/Sony/save_Sony')\n",
        "    \n",
        "    \n",
        "    self.epochs=epochs\n",
        "    self.epoch_counter=0\n",
        "    self.ps=512 # batch/patch size for images for training\n",
        "    self.learning_rate=0.0001 # not used in keras"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HoZq9FqA1Elm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Data():\n",
        "  def __init__(self,params):\n",
        "    self.train_fns = glob.glob(params.gt_dir + '0*.ARW')\n",
        "    \"\"\" 0* is for Training images. 1 for Test and 2 for Validation \"\"\"\n",
        "    self.train_ids = [int(os.path.basename(train_fn)[0:5]) for train_fn in self.train_fns]\n",
        "    print(\"Total Train Ids:\",len(self.train_ids))\n",
        "    print(\"Sample Train Ids:\",self.train_ids[0:10])\n",
        "    self.total_train_ids=len(self.train_ids)\n",
        "    \n",
        "    self.valid_fns = glob.glob(params.gt_dir + '2*.ARW')\n",
        "    \"\"\" 0* is for Training images. 1 for Test and 2 for Validation \"\"\"\n",
        "    self.valid_ids = [int(os.path.basename(valid_fn)[0:5]) for valid_fn in self.valid_fns]\n",
        "    print(\"Total Validation Ids:\",len(self.valid_ids))\n",
        "    print(\"Sample Validation Ids:\",self.valid_ids[0:10])\n",
        "    self.total_valid_ids=len(self.valid_ids)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bVcN0RNdVzAW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class DataLoader(keras.utils.Sequence):\n",
        "  \"\"\"Class to continously load images\"\"\"\n",
        "  \n",
        "  def __init__(self,params,data):\n",
        "    self.epochs=params.epochs\n",
        "    self.input_dir=params.input_dir\n",
        "    self.gt_dir=params.gt_dir\n",
        "    self.train_ids=data.train_ids\n",
        "    self.shuffled_ids=np.random.permutation(len(self.train_ids))\n",
        "    for index, val in np.ndenumerate(self.shuffled_ids):\n",
        "      print ('index:{}, image:{}'.format(index[0], val))\n",
        "    self.epoch_counter=0\n",
        "    # self.ind=-1\n",
        "    self.ps=params.ps # batch/patch size for images for training\n",
        "    #self.lock = threading.Lock() # used for making thread safe iterators\n",
        "    #self.on_epoch_end()\n",
        "    \n",
        "#     self.gt_images = [None] * 6000\n",
        "#     self.input_images = {}\n",
        "#     self.input_images['300'] = [None] * len(train_ids)\n",
        "#     self.input_images['250'] = [None] * len(train_ids)\n",
        "#     self.input_images['100'] = [None] * len(train_ids)\n",
        "    \n",
        "  def on_epoch_end(self):\n",
        "    self.shuffled_ids=np.random.permutation(len(self.train_ids))\n",
        "    print(\"in on epoch end\")\n",
        "\n",
        "\n",
        "  def __len__(self):\n",
        "    'Denotes the number of batches per epoch'\n",
        "    return len(self.train_ids)\n",
        "\n",
        "  def __getitem__(self, ind):\n",
        "    'Generates data containing batch_size samples' # X : (n_samples, *dim, n_channels)\n",
        "    train_id = self.train_ids[self.shuffled_ids[ind]]\n",
        "    #print(\"retireved the train id: {}, ind: {}\".format(train_id,ind))\n",
        "    in_files = glob.glob(self.input_dir + '%05d_00*.ARW' % train_id) # get all the images path with pattern 0*train_id\n",
        "    try:\n",
        "      in_path = in_files[np.random.randint(0, len(in_files))] # get any one image randomly\n",
        "    except:\n",
        "      print(\"exception- retireved the train id:{}, ind:{}, in_files:{}\".format(train_id,ind,in_files))\n",
        "\n",
        "    in_fn = os.path.basename(in_path) # get only the full image name\n",
        "\n",
        "    gt_files = glob.glob(self.gt_dir + '%05d_00*.ARW' % train_id) # get the ground truth image path(s) (only 1 may exist. hence select [0]th below)\n",
        "    gt_path = gt_files[0] # get the first one (as only 1 gt image should exist)\n",
        "    gt_fn = os.path.basename(gt_path) # get only the full image name for gt image\n",
        "    in_exposure = float(in_fn[9:-5]) # get the exposure for input image\n",
        "    gt_exposure = float(gt_fn[9:-5]) # get the exposure for gt image\n",
        "    ratio = min(gt_exposure / in_exposure, 300) # get the amplification ratio\n",
        "\n",
        "    st = time.time()\n",
        "    #cnt += 1\n",
        "\n",
        "    #if input_images[str(ratio)[0:3]][ind] is None: # if image is not loaded (first epoch), load it\n",
        "    raw = rawpy.imread(in_path)\n",
        "    input_images = np.expand_dims(pack_raw(raw), axis=0) * ratio # pack the bayer image in 4 channels of RGBG\n",
        "\n",
        "    gt_raw = rawpy.imread(gt_path)\n",
        "    im = gt_raw.postprocess(use_camera_wb=True,\n",
        "                  half_size=False,\n",
        "                  no_auto_bright=True, output_bps=16)\n",
        "    gt_images = np.expand_dims(np.float32(im / 65535.0),axis=0) # divide by 65535 to normalise (scale between 0 and 1)\n",
        "\n",
        "    # crop\n",
        "\n",
        "    H = input_images.shape[1] # get the image height (number of rows)\n",
        "    W = input_images.shape[2] # get the image width (number of columns)\n",
        "\n",
        "    xx = np.random.randint(0, W - self.ps) # get a random number in W-ps (W-512)\n",
        "    yy = np.random.randint(0, H - self.ps) # get a random number in H-ps (H-512)\n",
        "    input_patch = input_images[:, yy:yy + self.ps, xx:xx + self.ps, :]\n",
        "    gt_patch = gt_images[:, yy * 2:yy * 2 + self.ps * 2, xx * 2:xx * 2 + self.ps * 2, :]\n",
        "\n",
        "    if np.random.randint(2) == 1:  # random flip for rows\n",
        "      input_patch = np.flip(input_patch, axis=1)\n",
        "      gt_patch = np.flip(gt_patch, axis=1)\n",
        "    if np.random.randint(2) == 1:  # random flip for columns\n",
        "      input_patch = np.flip(input_patch, axis=2)\n",
        "      gt_patch = np.flip(gt_patch, axis=2)\n",
        "    if np.random.randint(2) == 1:  # random transpose\n",
        "      input_patch = np.transpose(input_patch, (0, 2, 1, 3))\n",
        "      gt_patch = np.transpose(gt_patch, (0, 2, 1, 3))\\\n",
        "\n",
        "    input_patch = np.minimum(input_patch, 1.0)\n",
        "\n",
        "    return (input_patch,gt_patch)\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YECuKfSCS7JW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_generator(model,data_gen,epochs,steps_per_epoch,save_fname):\n",
        "  print(\"Model Training started!!! Jay Yogeshwar!!!\")\n",
        "  callbacks = [\n",
        "    ModelCheckpoint(filepath=save_fname, monitor='loss', verbose=1, save_best_only=True)\n",
        "  ]\n",
        "  model_history=model.fit_generator(\n",
        "      generator=data_gen,\n",
        "      steps_per_epoch=steps_per_epoch,\n",
        "      epochs=epochs,\n",
        "      callbacks=callbacks,\n",
        "      max_queue_size=20,\n",
        "      workers=7,\n",
        "      use_multiprocessing=True\n",
        "  )\n",
        "  print(\"Model Training completed!!! Jay Yogeshwar!!!\")\n",
        "  return model_history"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vvdY4QQSpiKq",
        "colab_type": "code",
        "outputId": "63d3fcc3-5e9c-4477-9161-f96c459c32cb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "from keras import backend as K\n",
        "\n",
        "def recall(y_true, y_pred):\n",
        "  true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "  possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
        "  recall = true_positives / (possible_positives + K.epsilon())\n",
        "  return recall\n",
        "\n",
        "def precision(y_true, y_pred):\n",
        "  true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "  predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
        "  precision = true_positives / (predicted_positives + K.epsilon())\n",
        "  return precision\n",
        "\n",
        "def reduced_mean(y_true,y_pred):\n",
        "  return tf.reduce_mean(tf.abs(y_pred - y_true))\n",
        "\n",
        "epochs=4001\n",
        "params = Params(epochs)\n",
        "data = Data(params)\n",
        "dataloader = DataLoader(params,data)\n",
        "model=build_model()\n",
        "with open('model_summary.txt','w') as fh:\n",
        "    # Pass the file handle in as a lambda function to make it callable\n",
        "    model.summary(print_fn=lambda x: fh.write(x + '\\n'))\n",
        "keras.utils.vis_utils.plot_model(model, to_file='model_plot.png', show_shapes=True, show_layer_names=True)\n",
        "model.compile(optimizer='adam',loss=reduced_mean,metrics=['accuracy'])\n",
        "steps_per_epoch=len(data.train_ids)\n",
        "save_fname = os.path.join(params.save_dir, '%s-e%s.h5' % (dt.datetime.now().strftime('%d%m%Y-%H%M%S'), str(epochs)))\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total Train Ids: 161\n",
            "Sample Train Ids: [90, 91, 92, 72, 73, 75, 76, 78, 81, 83]\n",
            "Total Validation Ids: 20\n",
            "Sample Validation Ids: [20079, 20080, 20089, 20107, 20115, 20120, 20147, 20153, 20177, 20005]\n",
            "index:0, image:158\n",
            "index:1, image:102\n",
            "index:2, image:104\n",
            "index:3, image:37\n",
            "index:4, image:57\n",
            "index:5, image:44\n",
            "index:6, image:66\n",
            "index:7, image:126\n",
            "index:8, image:19\n",
            "index:9, image:110\n",
            "index:10, image:115\n",
            "index:11, image:43\n",
            "index:12, image:133\n",
            "index:13, image:16\n",
            "index:14, image:17\n",
            "index:15, image:125\n",
            "index:16, image:124\n",
            "index:17, image:46\n",
            "index:18, image:116\n",
            "index:19, image:3\n",
            "index:20, image:64\n",
            "index:21, image:136\n",
            "index:22, image:51\n",
            "index:23, image:78\n",
            "index:24, image:75\n",
            "index:25, image:5\n",
            "index:26, image:147\n",
            "index:27, image:93\n",
            "index:28, image:1\n",
            "index:29, image:142\n",
            "index:30, image:12\n",
            "index:31, image:4\n",
            "index:32, image:71\n",
            "index:33, image:59\n",
            "index:34, image:82\n",
            "index:35, image:90\n",
            "index:36, image:30\n",
            "index:37, image:6\n",
            "index:38, image:31\n",
            "index:39, image:154\n",
            "index:40, image:84\n",
            "index:41, image:123\n",
            "index:42, image:14\n",
            "index:43, image:38\n",
            "index:44, image:11\n",
            "index:45, image:128\n",
            "index:46, image:29\n",
            "index:47, image:25\n",
            "index:48, image:73\n",
            "index:49, image:89\n",
            "index:50, image:2\n",
            "index:51, image:40\n",
            "index:52, image:107\n",
            "index:53, image:28\n",
            "index:54, image:72\n",
            "index:55, image:83\n",
            "index:56, image:120\n",
            "index:57, image:36\n",
            "index:58, image:118\n",
            "index:59, image:105\n",
            "index:60, image:153\n",
            "index:61, image:76\n",
            "index:62, image:95\n",
            "index:63, image:21\n",
            "index:64, image:97\n",
            "index:65, image:92\n",
            "index:66, image:9\n",
            "index:67, image:62\n",
            "index:68, image:24\n",
            "index:69, image:60\n",
            "index:70, image:15\n",
            "index:71, image:159\n",
            "index:72, image:140\n",
            "index:73, image:149\n",
            "index:74, image:79\n",
            "index:75, image:52\n",
            "index:76, image:150\n",
            "index:77, image:96\n",
            "index:78, image:8\n",
            "index:79, image:70\n",
            "index:80, image:55\n",
            "index:81, image:143\n",
            "index:82, image:63\n",
            "index:83, image:109\n",
            "index:84, image:41\n",
            "index:85, image:157\n",
            "index:86, image:117\n",
            "index:87, image:50\n",
            "index:88, image:54\n",
            "index:89, image:127\n",
            "index:90, image:32\n",
            "index:91, image:61\n",
            "index:92, image:94\n",
            "index:93, image:47\n",
            "index:94, image:67\n",
            "index:95, image:58\n",
            "index:96, image:10\n",
            "index:97, image:27\n",
            "index:98, image:22\n",
            "index:99, image:148\n",
            "index:100, image:139\n",
            "index:101, image:91\n",
            "index:102, image:144\n",
            "index:103, image:122\n",
            "index:104, image:156\n",
            "index:105, image:99\n",
            "index:106, image:13\n",
            "index:107, image:138\n",
            "index:108, image:33\n",
            "index:109, image:141\n",
            "index:110, image:34\n",
            "index:111, image:81\n",
            "index:112, image:77\n",
            "index:113, image:65\n",
            "index:114, image:112\n",
            "index:115, image:48\n",
            "index:116, image:135\n",
            "index:117, image:56\n",
            "index:118, image:39\n",
            "index:119, image:87\n",
            "index:120, image:155\n",
            "index:121, image:108\n",
            "index:122, image:69\n",
            "index:123, image:146\n",
            "index:124, image:145\n",
            "index:125, image:114\n",
            "index:126, image:121\n",
            "index:127, image:160\n",
            "index:128, image:134\n",
            "index:129, image:137\n",
            "index:130, image:106\n",
            "index:131, image:100\n",
            "index:132, image:26\n",
            "index:133, image:111\n",
            "index:134, image:132\n",
            "index:135, image:130\n",
            "index:136, image:68\n",
            "index:137, image:98\n",
            "index:138, image:80\n",
            "index:139, image:152\n",
            "index:140, image:23\n",
            "index:141, image:0\n",
            "index:142, image:53\n",
            "index:143, image:45\n",
            "index:144, image:151\n",
            "index:145, image:86\n",
            "index:146, image:131\n",
            "index:147, image:129\n",
            "index:148, image:85\n",
            "index:149, image:103\n",
            "index:150, image:74\n",
            "index:151, image:113\n",
            "index:152, image:7\n",
            "index:153, image:35\n",
            "index:154, image:101\n",
            "index:155, image:18\n",
            "index:156, image:119\n",
            "index:157, image:88\n",
            "index:158, image:49\n",
            "index:159, image:20\n",
            "index:160, image:42\n",
            "conv1 shape: (?, ?, ?, 32)\n",
            "conv1 shape: (?, ?, ?, 32)\n",
            "conv1 shape after pool: (?, ?, ?, 32)\n",
            "layer: upsample_concat_1\n",
            "x1 shape: (?, ?, ?, 512)\n",
            "x2 shape: (?, ?, ?, 256)\n",
            "deconvtf shape: (?, ?, ?, 256)\n",
            "deconv_output shape: (?, ?, ?, 512)\n",
            "layer: upsample_concat_1\n",
            "x1 shape: (?, ?, ?, 512)\n",
            "x2 shape: (?, ?, ?, 256)\n",
            "deconvtf shape: (?, ?, ?, 256)\n",
            "deconv_output shape: (?, ?, ?, 512)\n",
            "layer: upsample_concat_2\n",
            "x1 shape: (?, ?, ?, 256)\n",
            "x2 shape: (?, ?, ?, 128)\n",
            "deconvtf shape: (?, ?, ?, 128)\n",
            "deconv_output shape: (?, ?, ?, 256)\n",
            "layer: upsample_concat_2\n",
            "x1 shape: (?, ?, ?, 256)\n",
            "x2 shape: (?, ?, ?, 128)\n",
            "deconvtf shape: (?, ?, ?, 128)\n",
            "deconv_output shape: (?, ?, ?, 256)\n",
            "layer: upsample_concat_3\n",
            "x1 shape: (?, ?, ?, 128)\n",
            "x2 shape: (?, ?, ?, 64)\n",
            "deconvtf shape: (?, ?, ?, 64)\n",
            "deconv_output shape: (?, ?, ?, 128)\n",
            "layer: upsample_concat_3\n",
            "x1 shape: (?, ?, ?, 128)\n",
            "x2 shape: (?, ?, ?, 64)\n",
            "deconvtf shape: (?, ?, ?, 64)\n",
            "deconv_output shape: (?, ?, ?, 128)\n",
            "layer: upsample_concat_4\n",
            "x1 shape: (?, ?, ?, 64)\n",
            "x2 shape: (?, ?, ?, 32)\n",
            "deconvtf shape: (?, ?, ?, 32)\n",
            "deconv_output shape: (?, ?, ?, 64)\n",
            "layer: upsample_concat_4\n",
            "x1 shape: (?, ?, ?, 64)\n",
            "x2 shape: (?, ?, ?, 32)\n",
            "deconvtf shape: (?, ?, ?, 32)\n",
            "deconv_output shape: (?, ?, ?, 64)\n",
            "conv9 shape: (?, ?, ?, 32)\n",
            "conv10 shape: (?, ?, ?, 12)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mJ8IR20yraX8",
        "colab_type": "code",
        "outputId": "39462e2c-dfd5-4b80-d517-cd86dea18cee",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 469
        }
      },
      "source": [
        "\"\"\" ================ TRAIN THE MODEL ================ \"\"\"\n",
        "steps_per_epoch=1\n",
        "model_history=train_generator(model,dataloader,params.epochs,steps_per_epoch,save_fname)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model Training started!!! Jay Yogeshwar!!!\n",
            "Epoch 1/4001\n",
            "1/1 [==============================] - 13s 13s/step - loss: 0.1257 - acc: 0.2295\n",
            "\n",
            "Epoch 00001: loss improved from inf to 0.12566, saving model to /content/drive/My Drive/Data/SeeInDarkDataset/dataset/Sony/save_Sony/27072019-152734-e4001.h5\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-55-fde8b84b2b2d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;34m\"\"\" ================ TRAIN THE MODEL ================ \"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mmodel_history\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdataloader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msave_fname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-53-fe346acbb374>\u001b[0m in \u001b[0;36mtrain_generator\u001b[0;34m(model, data_gen, epochs, steps_per_epoch, save_fname)\u001b[0m\n\u001b[1;32m     11\u001b[0m       \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m       \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m       \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m   )\n\u001b[1;32m     15\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Model Training completed!!! Jay Yogeshwar!!!\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name + '` call to the ' +\n\u001b[1;32m     90\u001b[0m                               'Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1416\u001b[0m             \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1417\u001b[0m             \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1418\u001b[0;31m             initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1419\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1420\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m    249\u001b[0m                     \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 251\u001b[0;31m             \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_epoch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    252\u001b[0m             \u001b[0mepoch\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mcallback_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_training\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/callbacks.py\u001b[0m in \u001b[0;36mon_epoch_end\u001b[0;34m(self, epoch, logs)\u001b[0m\n\u001b[1;32m     77\u001b[0m         \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlogs\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mcallback\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m             \u001b[0mcallback\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_epoch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mon_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/callbacks.py\u001b[0m in \u001b[0;36mon_epoch_end\u001b[0;34m(self, epoch, logs)\u001b[0m\n\u001b[1;32m    444\u001b[0m                             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moverwrite\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    445\u001b[0m                         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 446\u001b[0;31m                             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moverwrite\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    447\u001b[0m                     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    448\u001b[0m                         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbose\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/network.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, filepath, overwrite, include_optimizer)\u001b[0m\n\u001b[1;32m   1088\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mNotImplementedError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1089\u001b[0m         \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msave_model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1090\u001b[0;31m         \u001b[0msave_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moverwrite\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minclude_optimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1091\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1092\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msave_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moverwrite\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/saving.py\u001b[0m in \u001b[0;36msave_model\u001b[0;34m(model, filepath, overwrite, include_optimizer)\u001b[0m\n\u001b[1;32m    380\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    381\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 382\u001b[0;31m         \u001b[0m_serialize_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minclude_optimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    383\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    384\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mopened_new_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/saving.py\u001b[0m in \u001b[0;36m_serialize_model\u001b[0;34m(model, f, include_optimizer)\u001b[0m\n\u001b[1;32m     81\u001b[0m     \u001b[0mmodel_config\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m     \u001b[0mmodel_config\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'class_name'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m     \u001b[0mmodel_config\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'config'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m     \u001b[0mmodel_config\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdumps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_config\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdefault\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mget_json_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m     \u001b[0mmodel_config\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_config\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/network.py\u001b[0m in \u001b[0;36mget_config\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    929\u001b[0m             \u001b[0mmodel_outputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_node_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    930\u001b[0m         \u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'output_layers'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_outputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 931\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    932\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    933\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/copy.py\u001b[0m in \u001b[0;36mdeepcopy\u001b[0;34m(x, memo, _nil)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mcopier\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_deepcopy_dispatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcopier\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcopier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/copy.py\u001b[0m in \u001b[0;36m_deepcopy_dict\u001b[0;34m(x, memo, deepcopy)\u001b[0m\n\u001b[1;32m    238\u001b[0m     \u001b[0mmemo\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    239\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 240\u001b[0;31m         \u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    241\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    242\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_deepcopy_dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/copy.py\u001b[0m in \u001b[0;36mdeepcopy\u001b[0;34m(x, memo, _nil)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mcopier\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_deepcopy_dispatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcopier\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcopier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/copy.py\u001b[0m in \u001b[0;36m_deepcopy_list\u001b[0;34m(x, memo, deepcopy)\u001b[0m\n\u001b[1;32m    213\u001b[0m     \u001b[0mappend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0ma\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 215\u001b[0;31m         \u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    216\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    217\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_deepcopy_list\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/copy.py\u001b[0m in \u001b[0;36mdeepcopy\u001b[0;34m(x, memo, _nil)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mcopier\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_deepcopy_dispatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcopier\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcopier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/copy.py\u001b[0m in \u001b[0;36m_deepcopy_dict\u001b[0;34m(x, memo, deepcopy)\u001b[0m\n\u001b[1;32m    238\u001b[0m     \u001b[0mmemo\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    239\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 240\u001b[0;31m         \u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    241\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    242\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_deepcopy_dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/copy.py\u001b[0m in \u001b[0;36mdeepcopy\u001b[0;34m(x, memo, _nil)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mcopier\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_deepcopy_dispatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcopier\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcopier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/copy.py\u001b[0m in \u001b[0;36m_deepcopy_dict\u001b[0;34m(x, memo, deepcopy)\u001b[0m\n\u001b[1;32m    238\u001b[0m     \u001b[0mmemo\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    239\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 240\u001b[0;31m         \u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    241\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    242\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_deepcopy_dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/copy.py\u001b[0m in \u001b[0;36mdeepcopy\u001b[0;34m(x, memo, _nil)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mcopier\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_deepcopy_dispatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcopier\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcopier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/copy.py\u001b[0m in \u001b[0;36m_deepcopy_dict\u001b[0;34m(x, memo, deepcopy)\u001b[0m\n\u001b[1;32m    238\u001b[0m     \u001b[0mmemo\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    239\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 240\u001b[0;31m         \u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    241\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    242\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_deepcopy_dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/copy.py\u001b[0m in \u001b[0;36mdeepcopy\u001b[0;34m(x, memo, _nil)\u001b[0m\n\u001b[1;32m    178\u001b[0m                     \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 180\u001b[0;31m                     \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_reconstruct\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mrv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    181\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m     \u001b[0;31m# If is its own copy, don't memoize.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/copy.py\u001b[0m in \u001b[0;36m_reconstruct\u001b[0;34m(x, memo, func, args, state, listiter, dictiter, deepcopy)\u001b[0m\n\u001b[1;32m    278\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mstate\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    279\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdeep\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 280\u001b[0;31m             \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    281\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'__setstate__'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    282\u001b[0m             \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__setstate__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/copy.py\u001b[0m in \u001b[0;36mdeepcopy\u001b[0;34m(x, memo, _nil)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mcopier\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_deepcopy_dispatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcopier\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcopier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/copy.py\u001b[0m in \u001b[0;36m_deepcopy_dict\u001b[0;34m(x, memo, deepcopy)\u001b[0m\n\u001b[1;32m    238\u001b[0m     \u001b[0mmemo\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    239\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 240\u001b[0;31m         \u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    241\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    242\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_deepcopy_dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/copy.py\u001b[0m in \u001b[0;36mdeepcopy\u001b[0;34m(x, memo, _nil)\u001b[0m\n\u001b[1;32m    178\u001b[0m                     \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 180\u001b[0;31m                     \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_reconstruct\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mrv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    181\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m     \u001b[0;31m# If is its own copy, don't memoize.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/copy.py\u001b[0m in \u001b[0;36m_reconstruct\u001b[0;34m(x, memo, func, args, state, listiter, dictiter, deepcopy)\u001b[0m\n\u001b[1;32m    278\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mstate\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    279\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdeep\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 280\u001b[0;31m             \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    281\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'__setstate__'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    282\u001b[0m             \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__setstate__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/copy.py\u001b[0m in \u001b[0;36mdeepcopy\u001b[0;34m(x, memo, _nil)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mcopier\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_deepcopy_dispatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcopier\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcopier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/copy.py\u001b[0m in \u001b[0;36m_deepcopy_dict\u001b[0;34m(x, memo, deepcopy)\u001b[0m\n\u001b[1;32m    238\u001b[0m     \u001b[0mmemo\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    239\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 240\u001b[0;31m         \u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    241\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    242\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_deepcopy_dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/copy.py\u001b[0m in \u001b[0;36mdeepcopy\u001b[0;34m(x, memo, _nil)\u001b[0m\n\u001b[1;32m    178\u001b[0m                     \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 180\u001b[0;31m                     \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_reconstruct\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mrv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    181\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m     \u001b[0;31m# If is its own copy, don't memoize.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/copy.py\u001b[0m in \u001b[0;36m_reconstruct\u001b[0;34m(x, memo, func, args, state, listiter, dictiter, deepcopy)\u001b[0m\n\u001b[1;32m    278\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mstate\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    279\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdeep\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 280\u001b[0;31m             \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    281\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'__setstate__'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    282\u001b[0m             \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__setstate__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/copy.py\u001b[0m in \u001b[0;36mdeepcopy\u001b[0;34m(x, memo, _nil)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mcopier\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_deepcopy_dispatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcopier\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcopier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/copy.py\u001b[0m in \u001b[0;36m_deepcopy_dict\u001b[0;34m(x, memo, deepcopy)\u001b[0m\n\u001b[1;32m    238\u001b[0m     \u001b[0mmemo\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    239\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 240\u001b[0;31m         \u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    241\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    242\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_deepcopy_dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/copy.py\u001b[0m in \u001b[0;36mdeepcopy\u001b[0;34m(x, memo, _nil)\u001b[0m\n\u001b[1;32m    167\u001b[0m                     \u001b[0mreductor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"__reduce_ex__\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mreductor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 169\u001b[0;31m                         \u001b[0mrv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreductor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    170\u001b[0m                     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m                         \u001b[0mreductor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"__reduce__\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: can't pickle _thread.RLock objects"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K6rt0FGT6jY-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# for epoch in range(lastepoch, 4001):\n",
        "#     #if os.path.isdir('result/%04d' % epoch):\n",
        "#     #    continue\n",
        "#     ###\n",
        "#     # used for skipping the current epoch if the checkpoint directory exists\n",
        "#     ###\n",
        "#     cnt = 0\n",
        "#     if epoch > 2000:\n",
        "#         learning_rate = 1e-5\n",
        "\n",
        "#     for ind in np.random.permutation(len(train_ids)):\n",
        "\n",
        "#         # get the path from image id\n",
        "\n",
        "#         train_id = train_ids[ind]\n",
        "#         in_files = glob.glob(input_dir + '%05d_00*.ARW' % train_id) # get all the images path with pattern 0*train_id\n",
        "#         in_path = in_files[np.random.randint(0, len(in_files))] # get any one image randomly\n",
        "#         in_fn = os.path.basename(in_path) # get only the full image name\n",
        "\n",
        "#         gt_files = glob.glob(gt_dir + '%05d_00*.ARW' % train_id) # get the ground truth image path(s) (only 1 may exist. hence select [0]th below)\n",
        "#         gt_path = gt_files[0] # get the first one (as only 1 gt image should exist)\n",
        "#         gt_fn = os.path.basename(gt_path) # get only the full image name for gt image\n",
        "#         in_exposure = float(in_fn[9:-5]) # get the exposure for input image\n",
        "#         gt_exposure = float(gt_fn[9:-5]) # get the exposure for gt image\n",
        "#         ratio = min(gt_exposure / in_exposure, 300) # get the amplification ratio\n",
        "\n",
        "#         st = time.time()\n",
        "#         cnt += 1\n",
        "\n",
        "#         if input_images[str(ratio)[0:3]][ind] is None: # if image is not loaded (first epoch), load it\n",
        "#             raw = rawpy.imread(in_path)\n",
        "#             input_images[str(ratio)[0:3]][ind] = \\\n",
        "#                 np.expand_dims(pack_raw(raw), axis=0) * ratio # pack the bayer image in 4 channels of RGBG\n",
        "\n",
        "#             gt_raw = rawpy.imread(gt_path)\n",
        "#             im = gt_raw.postprocess(use_camera_wb=True,\n",
        "#                                     half_size=False,\n",
        "#                                     no_auto_bright=True, output_bps=16)\n",
        "#             gt_images[ind] = np.expand_dims(np.float32(im / 65535.0),axis=0) # divide by 65535 to normalise (scale between 0 and 1)\n",
        "\n",
        "#         # crop\n",
        "\n",
        "#         H = input_images[str(ratio)[0:3]][ind].shape[1] # get the image height (number of rows)\n",
        "#         W = input_images[str(ratio)[0:3]][ind].shape[2] # get the image width (number of columns)\n",
        "\n",
        "#         xx = np.random.randint(0, W - ps) # get a random number in W-ps (W-512)\n",
        "#         yy = np.random.randint(0, H - ps) # get a random number in H-ps (H-512)\n",
        "#         input_patch = input_images[str(ratio)[0:3]][ind][:, yy:yy + ps,\n",
        "#                 xx:xx + ps, :]\n",
        "#         gt_patch = gt_images[ind][:, yy * 2:yy * 2 + ps * 2, xx * 2:xx\n",
        "#                                   * 2 + ps * 2, :]\n",
        "\n",
        "#         if np.random.randint(2) == 1:  # random flip for rows\n",
        "#             input_patch = np.flip(input_patch, axis=1)\n",
        "#             gt_patch = np.flip(gt_patch, axis=1)\n",
        "#         if np.random.randint(2) == 1:  # random flip for columns\n",
        "#             input_patch = np.flip(input_patch, axis=2)\n",
        "#             gt_patch = np.flip(gt_patch, axis=2)\n",
        "#         if np.random.randint(2) == 1:  # random transpose\n",
        "#             input_patch = np.transpose(input_patch, (0, 2, 1, 3))\n",
        "#             gt_patch = np.transpose(gt_patch, (0, 2, 1, 3))\\\n",
        "\n",
        "#         input_patch = np.minimum(input_patch, 1.0)\n",
        "\n",
        "#         (_, G_current, output) = sess.run([G_opt, G_loss, out_image],\n",
        "#                 feed_dict={in_image: input_patch, gt_image: gt_patch,\n",
        "#                 lr: learning_rate})\n",
        "#         output = np.minimum(np.maximum(output, 0), 1)\n",
        "#         g_loss[ind] = G_current\n",
        "\n",
        "#         print '%d %d Loss=%.3f Time=%.3f' % (epoch, cnt,\n",
        "#                 np.mean(g_loss[np.where(g_loss)]), time.time() - st)\n",
        "\n",
        "#         if epoch % save_freq == 0:\n",
        "#             if not os.path.isdir(result_dir + '%04d' % epoch):\n",
        "#                 os.makedirs(result_dir + '%04d' % epoch)\n",
        "\n",
        "#             temp = np.concatenate((gt_patch[0, :, :, :], output[0, :, :\n",
        "#                                   , :]), axis=1)\n",
        "#             scipy.misc.toimage(temp * 255, high=255, low=0, cmin=0,\n",
        "#                                cmax=255).save(result_dir\n",
        "#                     + '%04d/%05d_00_train_%d.jpg' % (epoch, train_id,\n",
        "#                     ratio))\n",
        "\n",
        "#     saver.save(sess, checkpoint_dir + 'model.ckpt')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ir4Xcjn_mfhx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}