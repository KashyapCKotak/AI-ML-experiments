{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SeeInDark_Keras_Tensorflow_Sequence.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KashyapCKotak/AI-ML-experiments/blob/master/SeeInDark_Keras_Tensorflow_Sequence.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zuMNVBqt0u-H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# import requests\n",
        "# import os\n",
        "\n",
        "# def download_file_from_google_drive(id, destination):\n",
        "#     URL = \"https://docs.google.com/uc?export=download\"\n",
        "\n",
        "#     session = requests.Session()\n",
        "\n",
        "#     response = session.get(URL, params = { 'id' : id }, stream = True)\n",
        "#     token = get_confirm_token(response)\n",
        "\n",
        "#     if token:\n",
        "#         params = { 'id' : id, 'confirm' : token }\n",
        "#         response = session.get(URL, params = params, stream = True)\n",
        "\n",
        "#     save_response_content(response, destination)    \n",
        "\n",
        "# def get_confirm_token(response):\n",
        "#     for key, value in response.cookies.items():\n",
        "#         if key.startswith('download_warning'):\n",
        "#             return value\n",
        "\n",
        "#     return None\n",
        "\n",
        "# def save_response_content(response, destination):\n",
        "#     CHUNK_SIZE = 32768\n",
        "\n",
        "#     with open(destination, \"wb\") as f:\n",
        "#         for chunk in response.iter_content(CHUNK_SIZE):\n",
        "#             if chunk: # filter out keep-alive new chunks\n",
        "#                 f.write(chunk)\n",
        "\n",
        "\n",
        "\n",
        "# print('Dowloading Sony subset... (25GB)')\n",
        "# download_file_from_google_drive('10kpAcvldtcb9G2ze5hTcF1odzu4V_Zvh', './Sony.zip')\n",
        "\n",
        "# #print('Dowloading Fuji subset... (52GB)')\n",
        "# #download_file_from_google_drive('12hvKCjwuilKTZPe9EZ7ZTb-azOmUA3HT', 'dataset/Fuji.zip')\n",
        "\n",
        "# os.system('unzip ./Sony.zip -d dataset')\n",
        "# #os.system('unzip dataset/Fuji.zip -d dataset')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FsP1lrRN-HPz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# import shutil\n",
        "# shutil.rmtree('result_Sony', ignore_errors=True, onerror=None)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P-_85I6g3eA5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# os.system('unzip ./Sony.zip -d dataset')\n",
        "#os.system('unzip dataset/Fuji.zip -d dataset')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xDQVxKF8nooG",
        "colab_type": "code",
        "outputId": "ce2f7e1f-729a-478f-8e2c-36dcfac5b264",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8yyhvphaOsft",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "c8bbfa92-42e5-4aac-c403-6aa57ee3ee0b"
      },
      "source": [
        "import json\n",
        "import glob\n",
        "epoch_file='/content/drive/My Drive/Data/SeeInDarkDataset/dataset/Sony/save_Sony/epochNum.txt'\n",
        "save_dir='/content/drive/My Drive/Data/SeeInDarkDataset/dataset/Sony/save_Sony/'\n",
        "last_epoch=0\n",
        "last_line=\"\"\n",
        "with open(epoch_file, mode='r', buffering=1) as epoch_file:\n",
        "  last_line=list(epoch_file)[-1]\n",
        "last_epoch=json.loads(last_line)['epoch']\n",
        "print(\"Epoch to start:\",last_epoch)\n",
        "list_of_files=glob.glob(save_dir+\"*.hdf5\")\n",
        "latest_file_path = max(list_of_files, key=os.path.getctime)\n",
        "print (\"Latest file path:\",latest_file_path)"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch to start: 1\n",
            "Latest file path: /content/drive/My Drive/Data/SeeInDarkDataset/dataset/Sony/save_Sony/04082019-173906-weights-e-10-l-14.17.hdf5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z9yF04nOlUCP",
        "colab_type": "text"
      },
      "source": [
        "**Main Train Code:**\n",
        "================================="
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m_zLVCtYlfSH",
        "colab_type": "code",
        "outputId": "5b5a1866-c2f2-46e2-b1f0-45cde7dafb80",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 138
        }
      },
      "source": [
        "!pip install rawpy"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting rawpy\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/da/04/997062d83766f5fbfa4dc0d20180c5085b55a74e69559e9cb6d4f1e9550e/rawpy-0.13.1-cp36-cp36m-manylinux1_x86_64.whl (682kB)\n",
            "\u001b[K     |████████████████████████████████| 686kB 5.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from rawpy) (1.16.4)\n",
            "Installing collected packages: rawpy\n",
            "Successfully installed rawpy-0.13.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IogQOu0slZNT",
        "colab_type": "code",
        "outputId": "fa0e2346-b0eb-47de-f1df-a35d1b1b25c4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from __future__ import division\n",
        "import os, time, scipy.io\n",
        "import tensorflow as tf\n",
        "import tensorflow.contrib.slim as slim\n",
        "import numpy as np\n",
        "import rawpy\n",
        "import glob\n",
        "import keras\n",
        "import shutil\n",
        "import datetime as dt\n",
        "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "import threading\n",
        "import json"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-gTj24DXlazJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# import shutil\n",
        "# shutil.rmtree('result_Sony', ignore_errors=True, onerror=None)\n",
        "\n",
        "# input_dir = '/content/drive/My Drive/Data/SeeInDarkDataset/dataset/Sony/short/'\n",
        "# gt_dir = '/content/drive/My Drive/Data/SeeInDarkDataset/dataset/Sony/long/'\n",
        "# os.mkdir('result_Sony')\n",
        "# checkpoint_dir = './result_Sony/'\n",
        "# result_dir = './result_Sony/'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k1bgs9-jrfgp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# train_fns = glob.glob(gt_dir + '0*.ARW')\n",
        "###\n",
        "# 0* is for Training images. 1 for Test and 2 for Validation\n",
        "###\n",
        "# train_ids = [int(os.path.basename(train_fn)[0:5]) for train_fn in train_fns]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lbi6fY6ZiBz_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# ps = 512  # just a random patch size for training"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V56pWdxpiGxk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# save_freq = 500\n",
        "# DEBUG = 0\n",
        "# if DEBUG == 1:\n",
        "#     save_freq = 2\n",
        "#     train_ids = train_ids[0:5]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vexCrDNtiIrN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def lrelu(x): # not used. Used Kears LeakyReLU with alpha 0.2 instead\n",
        "    return tf.maximum(x * 0.2, x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6ZkJ3o0ZiKQk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\" This cell contains the custom Lambda layers to be used in building the  keras mdoel \"\"\"\n",
        "def upsample_and_concat(x1,x2,output_channels,in_channels,layer):\n",
        "  print(\"layer:\",layer)\n",
        "  pool_size = 2\n",
        "  deconv_filter = tf.Variable(tf.truncated_normal([pool_size, pool_size, output_channels, in_channels], stddev=0.02))\n",
        "  print(\"x1 shape:\",x1.shape)\n",
        "  print(\"x2 shape:\",x2.shape)\n",
        "  deconvtf=tf.nn.conv2d_transpose(x1, deconv_filter, tf.shape(x2), strides=[1, pool_size, pool_size, 1])\n",
        "  print(\"deconvtf shape:\",deconvtf.shape)\n",
        "  #deconv=keras.layers.Conv2DTranspose(filters=output_channels,kernel_size=(pool_size, pool_size),strides=(pool_size, pool_size))(x1)\n",
        "  #print(\"deconvk shape:\",deconv.shape)\n",
        "\n",
        "  deconv_output = tf.concat([deconvtf, x2], 3)\n",
        "  print(\"deconv_output shape:\",deconv_output.shape)\n",
        "  deconv_output.set_shape([None, None, None, output_channels * 2])\n",
        "\n",
        "  return deconv_output\n",
        "\n",
        "def Depth_to_space_tf(input):\n",
        "  return tf.depth_to_space(input, 2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2cRpNNVlYXKz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# testing code\n",
        "# t=tf.ones([1,450,350,4])\n",
        "# deconv_filter = tf.Variable(tf.truncated_normal([2, 2, 256, 512], stddev=0.02))\n",
        "# tf.nn.conv2d_transpose(t, deconv_filter, [1,450,350,], strides=[1, pool_size, pool_size, 1])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QUz7pW3dm8Qh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def build_model():\n",
        "  inputs=keras.layers.Input(shape=(None, None, 4))\n",
        "  #definition: slim.conv2d(inputs,number of outputs, kernel shape)\n",
        "  #definition: slim.max_pool2d(inputsconv2d_transpose ,kernel_size, padding)\n",
        "  #conv1 = slim.conv2d(input, 32, [3, 3], rate=1, activation_fn=lrelu, scope='g_conv1_1')\n",
        "  conv1=keras.layers.Conv2D(filters=32,kernel_size=(3,3),strides=(1,1),padding='same',name='conv1_1')(inputs)\n",
        "  conv1=keras.layers.LeakyReLU(alpha=0.2,name='conv1_1_relu')(conv1)\n",
        "  print(\"conv1 shape:\",conv1.shape)\n",
        "  #conv1 = slim.conv2d(conv1, 32, [3, 3], rate=1, activation_fn=lrelu, scope='g_conv1_2')\n",
        "  conv1=keras.layers.Conv2D(filters=32,kernel_size=(3,3),strides=(1,1),padding='same',name='conv1_2')(conv1)\n",
        "  conv1=keras.layers.LeakyReLU(alpha=0.2,name='conv1_2_relu')(conv1)\n",
        "  print(\"conv1 shape:\",conv1.shape)\n",
        "  #pool1 = slim.max_pool2d(conv1, [2, 2], padding='SAME')\n",
        "  pool1=keras.layers.MaxPooling2D(pool_size=(2,2),padding=\"same\",name='pool1')(conv1)\n",
        "  print(\"conv1 shape after pool:\",pool1.shape)\n",
        "\n",
        "  #conv2 = slim.conv2d(pool1, 64, [3, 3], rate=1, activation_fn=lrelu, scope='g_conv2_1')\n",
        "  conv2=keras.layers.Conv2D(filters=64,kernel_size=(3,3),strides=(1,1),padding='same',name='conv2_1')(pool1)\n",
        "  conv2=keras.layers.LeakyReLU(alpha=0.2,name='conv2_1_relu')(conv2)\n",
        "  #conv2 = slim.conv2d(conv2, 64, [3, 3], rate=1, activation_fn=lrelu, scope='g_conv2_2')\n",
        "  conv2=keras.layers.Conv2D(filters=64,kernel_size=(3,3),strides=(1,1),padding='same',name='conv2_2')(conv2)\n",
        "  conv2=keras.layers.LeakyReLU(alpha=0.2,name='conv2_2_relu')(conv2)\n",
        "  #pool2 = slim.max_pool2d(conv2, [2, 2], padding='SAME')\n",
        "  pool2=keras.layers.MaxPooling2D(pool_size=(2,2),padding=\"same\",name='pool2')(conv2)\n",
        "\n",
        "  #conv3 = slim.conv2d(pool2, 128, [3, 3], rate=1, activation_fn=lrelu, scope='g_conv3_1')\n",
        "  conv3=keras.layers.Conv2D(filters=128,kernel_size=(3,3),strides=(1,1),padding='same',name='conv3_1')(pool2)\n",
        "  conv3=keras.layers.LeakyReLU(alpha=0.2,name='conv3_1_relu')(conv3)\n",
        "  #conv3 = slim.conv2d(conv3, 128, [3, 3], rate=1, activation_fn=lrelu, scope='g_conv3_2')\n",
        "  conv3=keras.layers.Conv2D(filters=128,kernel_size=(3,3),strides=(1,1),padding='same',name='conv3_2')(conv3)\n",
        "  conv3=keras.layers.LeakyReLU(alpha=0.2,name='conv3_2_relu')(conv3)\n",
        "  #pool3 = slim.max_pool2d(conv3, [2, 2], padding='SAME')\n",
        "  pool3=keras.layers.MaxPooling2D(pool_size=(2,2),padding=\"same\",name='pool3')(conv3)\n",
        "\n",
        "  #conv4 = slim.conv2d(pool3, 256, [3, 3], rate=1, activation_fn=lrelu, scope='g_conv4_1')\n",
        "  conv4=keras.layers.Conv2D(filters=256,kernel_size=(3,3),strides=(1,1),padding='same',name='conv4_1')(pool3)\n",
        "  conv4=keras.layers.LeakyReLU(alpha=0.2,name='conv4_1_relu')(conv4)\n",
        "  #conv4 = slim.conv2d(conv4, 256, [3, 3], rate=1, activation_fn=lrelu, scope='g_conv4_2')\n",
        "  conv4=keras.layers.Conv2D(filters=256,kernel_size=(3,3),strides=(1,1),padding='same',name='conv4_2')(conv4)\n",
        "  conv4=keras.layers.LeakyReLU(alpha=0.2,name='conv4_2_relu')(conv4)\n",
        "  #pool4 = slim.max_pool2d(conv4, [2, 2], padding='SAME')\n",
        "  pool4=keras.layers.MaxPooling2D(pool_size=(2,2),padding=\"same\",name='pool4')(conv4)\n",
        "\n",
        "  #conv5 = slim.conv2d(pool4, 512, [3, 3], rate=1, activation_fn=lrelu, scope='g_conv5_1')\n",
        "  conv5=keras.layers.Conv2D(filters=512,kernel_size=(3,3),strides=(1,1),padding='same',name='conv5_1')(pool4)\n",
        "  conv5=keras.layers.LeakyReLU(alpha=0.2,name='conv5_1_relu')(conv5)\n",
        "  #conv5 = slim.conv2d(conv5, 512, [3, 3], rate=1, activation_fn=lrelu, scope='g_conv5_2')\n",
        "  conv5=keras.layers.Conv2D(filters=512,kernel_size=(3,3),strides=(1,1),padding='same',name='conv5_2')(conv5)\n",
        "  conv5=keras.layers.LeakyReLU(alpha=0.2,name='conv5_2_relu')(conv5)\n",
        "\n",
        "  #up6 = upsample_and_concat(conv5, conv4, 256, 512)\n",
        "  up6=keras.layers.core.Lambda(upsample_and_concat,arguments={'x2':conv4,'output_channels':256,'in_channels':512,'layer':'upsample_concat_1'},name='upsample_concat_1')(conv5)\n",
        "  #conv6 = slim.conv2d(up6, 256, [3, 3], rate=1, activation_fn=lrelu, scope='g_conv6_1')\n",
        "  conv6=keras.layers.Conv2D(filters=256,kernel_size=(3,3),strides=(1,1),padding='same',name='conv6_1')(up6)\n",
        "  conv6=keras.layers.LeakyReLU(alpha=0.2,name='conv6_1_relu')(conv6)\n",
        "  #conv6 = slim.conv2d(conv6, 256, [3, 3], rate=1, activation_fn=lrelu, scope='g_conv6_2')\n",
        "  conv6=keras.layers.Conv2D(filters=256,kernel_size=(3,3),strides=(1,1),padding='same',name='conv6_2')(conv6)\n",
        "  conv6=keras.layers.LeakyReLU(alpha=0.2,name='conv6_2_relu')(conv6)\n",
        "\n",
        "  #up7 = upsample_and_concat(conv6, conv3, 128, 256)\n",
        "  up7=keras.layers.core.Lambda(upsample_and_concat,arguments={'x2':conv3,'output_channels':128,'in_channels':256,'layer':'upsample_concat_2'},name='upsample_concat_2')(conv6)\n",
        "  #conv7 = slim.conv2d(up7, 128, [3, 3], rate=1, activation_fn=lrelu, scope='g_conv7_1')\n",
        "  conv7=keras.layers.Conv2D(filters=128,kernel_size=(3,3),strides=(1,1),padding='same',name='conv7_1')(up7)\n",
        "  conv7=keras.layers.LeakyReLU(alpha=0.2,name='conv7_1_relu')(conv7)\n",
        "  #conv7 = slim.conv2d(conv7, 128, [3, 3], rate=1, activation_fn=lrelu, scope='g_conv7_2')\n",
        "  conv7=keras.layers.Conv2D(filters=128,kernel_size=(3,3),strides=(1,1),padding='same',name='conv7_2')(conv7)\n",
        "  conv7=keras.layers.LeakyReLU(alpha=0.2,name='conv7_2_relu')(conv7)\n",
        "\n",
        "  #up8 = upsample_and_concat(conv7, conv2, 64, 128)\n",
        "  up8=keras.layers.core.Lambda(upsample_and_concat,arguments={'x2':conv2,'output_channels':64,'in_channels':128,'layer':'upsample_concat_3'},name='upsample_concat_3')(conv7)\n",
        "  #conv8 = slim.conv2d(up8, 64, [3, 3], rate=1, activation_fn=lrelu, scope='g_conv8_1')\n",
        "  conv8=keras.layers.Conv2D(filters=64,kernel_size=(3,3),strides=(1,1),padding='same',name='conv8_1')(up8)\n",
        "  conv8=keras.layers.LeakyReLU(alpha=0.2,name='conv8_1_relu')(conv8)\n",
        "  #conv8 = slim.conv2d(conv8, 64, [3, 3], rate=1, activation_fn=lrelu, scope='g_conv8_2')\n",
        "  conv8=keras.layers.Conv2D(filters=64,kernel_size=(3,3),strides=(1,1),padding='same',name='conv8_2')(conv8)\n",
        "  conv8=keras.layers.LeakyReLU(alpha=0.2,name='conv8_2_relu')(conv8)\n",
        "\n",
        "  #up9 = upsample_and_concat(conv8, conv1, 32, 64)\n",
        "  up9=keras.layers.core.Lambda(upsample_and_concat,arguments={'x2':conv1,'output_channels':32,'in_channels':64,'layer':'upsample_concat_4'},name='upsample_concat_4')(conv8)\n",
        "  #conv9 = slim.conv2d(up9, 32, [3, 3], rate=1, activation_fn=lrelu, scope='g_conv9_1')\n",
        "  conv9=keras.layers.Conv2D(filters=32,kernel_size=(3,3),strides=(1,1),padding='same',name='conv9_1')(up9)\n",
        "  conv9=keras.layers.LeakyReLU(alpha=0.2,name='conv9_1_relu')(conv9)\n",
        "  #conv9 = slim.conv2d(conv9, 32, [3, 3], rate=1, activation_fn=lrelu, scope='g_conv9_2')\n",
        "  conv9=keras.layers.Conv2D(filters=32,kernel_size=(3,3),strides=(1,1),padding='same',name='conv9_2')(conv9)\n",
        "  conv9=keras.layers.LeakyReLU(alpha=0.2,name='conv9_2_relu')(conv9)\n",
        "  print(\"conv9 shape:\",conv9.shape)\n",
        "\n",
        "  #conv10 = slim.conv2d(conv9, 12, [1, 1], rate=1, activation_fn=None, scope='g_conv10')\n",
        "  conv10=keras.layers.Conv2D(filters=12,kernel_size=(1,1),strides=(1,1),name='conv10')(conv9)\n",
        "  print(\"conv10 shape:\",conv10.shape)\n",
        "  #no activation function for this last layer\n",
        "\n",
        "  predictions = keras.layers.core.Lambda(Depth_to_space_tf,name='depth_to_space')(conv10)\n",
        "  model = keras.models.Model(inputs=inputs, outputs=predictions)\n",
        "  \n",
        "  return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g8DyCh5GyNEU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def pack_raw(raw):\n",
        "    # pack Bayer image to 4 channels\n",
        "    im = raw.raw_image_visible.astype(np.float32)\n",
        "    #print(\"im shape:\",im.shape)\n",
        "    #print(\"im:\")\n",
        "    #print(im)\n",
        "    im = np.maximum(im - 512, 0) / (16383 - 512)  # subtract the black level and normalise with 16383 \n",
        "    ##\n",
        "    # Seems 16383 is the max value due to a 14 bit depth. can be replaced with \n",
        "    ##\n",
        "    #print(\"after normalisation\")\n",
        "    #print(im)\n",
        "\n",
        "    im = np.expand_dims(im, axis=2)\n",
        "    img_shape = im.shape\n",
        "    H = img_shape[0]\n",
        "    W = img_shape[1]\n",
        "    #print(\"after expand dims:%s ,H:%d ,W:%d \" % (img_shape,H,W))\n",
        "\n",
        "    ###\n",
        "    # single bayer pixel format:\n",
        "    # 0:R 1:G\n",
        "    # 2:G 3:B\n",
        "    ###\n",
        "    out = np.concatenate((im[0:H:2, 0:W:2, :], # get the 0th Red as per above format\n",
        "                          im[0:H:2, 1:W:2, :], # get the 1st Green\n",
        "                          im[1:H:2, 1:W:2, :], # get the 2nd Green\n",
        "                          im[1:H:2, 0:W:2, :]), axis=2) # get the 3rd Green\n",
        "    #print(\"out.shape:\",out.shape)\n",
        "    #print(out)\n",
        "    return out\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l6698Y7izIn9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#sess = tf.Session()\n",
        "# in_image = tf.placeholder(tf.float32, [None, None, None, 4])\n",
        "# gt_image = tf.placeholder(tf.float32, [None, None, None, 3])\n",
        "#out_image = network(in_image)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XXBqHKjN0NB1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#G_loss = tf.reduce_mean(tf.abs(out_image - gt_image))\n",
        "# t_vars = tf.trainable_variables()\n",
        "#lr = tf.placeholder(tf.float32)\n",
        "#G_opt = tf.train.AdamOptimizer(learning_rate=lr).minimize(G_loss)\n",
        "#saver = tf.train.Saver()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "75AcCZyH0XXL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#ckpt = tf.train.get_checkpoint_state(checkpoint_dir)\n",
        "#if ckpt:\n",
        "#    print('loaded ' + ckpt.model_checkpoint_path)\n",
        "#    saver.restore(sess, ckpt.model_checkpoint_path)\n",
        "###\n",
        "# above commented as not needed can be uncommented\n",
        "###\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hjDLxAdO0zds",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# gt_images = [None] * 6000\n",
        "# input_images = {}\n",
        "# input_images['300'] = [None] * len(train_ids)\n",
        "# input_images['250'] = [None] * len(train_ids)\n",
        "# input_images['100'] = [None] * len(train_ids)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0_0TfaMn3xFN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#? g_loss = np.zeros((5000, 1))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "poYFXTqz5G6F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# allfolders = glob.glob('./result/*0')\n",
        "# lastepoch = 0\n",
        "# for folder in allfolders:\n",
        "#     lastepoch = np.maximum(lastepoch, int(folder[-4:]))\n",
        "    \n",
        "###\n",
        "# The above code is used to infer the last completed epoch\n",
        "# ( may be in case a previously saved model is restored)\n",
        "###"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZaoFTFE05usd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# learning_rate = 0.0001"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ArFQgX5BmH9V",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# train_ids[0:10]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "icTY4imvYh2k",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# len(train_ids)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MkPgT9DWxYhW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# train_id = 1\n",
        "# in_files = glob.glob(input_dir + '%05d_00*.ARW' % train_id) # get all the images path with pattern 0*train_id\n",
        "# #print(in_files)\n",
        "# in_path = in_files[np.random.randint(0, len(in_files))] # get any one image randomly\n",
        "# #print(in_path)\n",
        "# in_fn = os.path.basename(in_path) # get only the full image name\n",
        "# #print(in_fn)\n",
        "\n",
        "# gt_files = glob.glob(gt_dir + '%05d_00*.ARW' % train_id) # get the ground truth image path(s) (only 1 may exist. hence select [0]th below)\n",
        "# #print(gt_files)\n",
        "# gt_path = gt_files[0] # get the first one (as only 1 gt image should exist)\n",
        "# #print(gt_path)\n",
        "# gt_fn = os.path.basename(gt_path) # get only the full image name for gt image\n",
        "# #print(gt_fn)\n",
        "# in_exposure = float(in_fn[9:-5]) # get the exposure for input image\n",
        "# #print(in_exposure)\n",
        "# gt_exposure = float(gt_fn[9:-5]) # get the exposure for gt image\n",
        "# #print(gt_exposure)\n",
        "# ratio = min(gt_exposure / in_exposure, 300)\n",
        "# print(\"ratio:\",ratio)\n",
        "# ########################\n",
        "# gt_raw = rawpy.imread('/content/drive/My Drive/Data/SeeInDarkDataset/dataset/Sony/long/00001_00_10s.ARW')\n",
        "# print(gt_raw.raw_pattern)\n",
        "# print(gt_raw)\n",
        "# im = gt_raw.postprocess(use_camera_wb=True,\n",
        "#                         half_size=False,\n",
        "#                         no_auto_bright=True, output_bps=16)\n",
        "# print(im.shape)\n",
        "# print(im[0:2,:,:])\n",
        "# gt_images = np.expand_dims(np.float32(im / 65535.0),\n",
        "#         axis=0)\n",
        "# print(\"gt_image shape\",gt_images.shape)\n",
        "# print(gt_images[:,0:2,:,:])\n",
        "# ############################\n",
        "# raw = rawpy.imread('/content/drive/My Drive/Data/SeeInDarkDataset/dataset/Sony/short/00001_00_0.04s.ARW')\n",
        "# packed_input_image=pack_raw(raw)\n",
        "# input_img = np.expand_dims(packed_input_image, axis=0) * ratio\n",
        "# print(\"input_image shape\",input_img.shape)\n",
        "# print(input_img[:,0:2,:,:])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XZKmZNa_0H7N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Params():\n",
        "  def __init__(self,epochs):\n",
        "    shutil.rmtree('/content/drive/My Drive/Data/SeeInDarkDataset/dataset/Sony/result_Sony', ignore_errors=True, onerror=None)\n",
        "    self.input_dir = '/content/drive/My Drive/Data/SeeInDarkDataset/dataset/Sony/short/'\n",
        "    self.gt_dir = '/content/drive/My Drive/Data/SeeInDarkDataset/dataset/Sony/long/'\n",
        "    os.mkdir('/content/drive/My Drive/Data/SeeInDarkDataset/dataset/Sony/result_Sony')\n",
        "    self.checkpoint_dir = '/content/drive/My Drive/Data/SeeInDarkDataset/dataset/Sony/result_Sony/'\n",
        "    self.result_dir = '/content/drive/My Drive/Data/SeeInDarkDataset/dataset/Sony/result_Sony/'\n",
        "    self.save_dir = '/content/drive/My Drive/Data/SeeInDarkDataset/dataset/Sony/save_Sony/'\n",
        "    self.epoch_file = '/content/drive/My Drive/Data/SeeInDarkDataset/dataset/Sony/save_Sony/epochNum.txt'\n",
        "    if os.path.exists(self.save_dir):\n",
        "      pass\n",
        "    else:\n",
        "      os.mkdir('/content/drive/My Drive/Data/SeeInDarkDataset/dataset/Sony/save_Sony')\n",
        "    \n",
        "    \n",
        "    self.epochs=epochs\n",
        "    self.epoch_counter=0\n",
        "    self.ps=512 # batch/patch size for images for training\n",
        "    self.learning_rate=0.0001 # not used in keras"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HoZq9FqA1Elm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Data():\n",
        "  def __init__(self,params):\n",
        "    self.train_fns = glob.glob(params.gt_dir + '0*.ARW')\n",
        "    \"\"\" 0* is for Training images. 1 for Test and 2 for Validation \"\"\"\n",
        "    self.train_ids = [int(os.path.basename(train_fn)[0:5]) for train_fn in self.train_fns]\n",
        "    print(\"Total Train Ids:\",len(self.train_ids))\n",
        "    print(\"Sample Train Ids:\",self.train_ids[0:10])\n",
        "    self.total_train_ids=len(self.train_ids)\n",
        "    \n",
        "    self.valid_fns = glob.glob(params.gt_dir + '2*.ARW')\n",
        "    \"\"\" 0* is for Training images. 1 for Test and 2 for Validation \"\"\"\n",
        "    self.valid_ids = [int(os.path.basename(valid_fn)[0:5]) for valid_fn in self.valid_fns]\n",
        "    print(\"Total Validation Ids:\",len(self.valid_ids))\n",
        "    print(\"Sample Validation Ids:\",self.valid_ids[0:10])\n",
        "    self.total_valid_ids=len(self.valid_ids)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bVcN0RNdVzAW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class DataLoader(keras.utils.Sequence):\n",
        "  \"\"\"Class to continously load images\"\"\"\n",
        "  \n",
        "  def __init__(self,params,data):\n",
        "    self.epochs=params.epochs\n",
        "    self.input_dir=params.input_dir\n",
        "    self.gt_dir=params.gt_dir\n",
        "    self.train_ids=data.train_ids\n",
        "    self.gt_image_map=[None] * 6000\n",
        "    self.shuffled_ids=np.random.permutation(len(self.train_ids))\n",
        "#     for index, val in np.ndenumerate(self.shuffled_ids):\n",
        "#       print ('index:{}, image:{}'.format(index[0], val))\n",
        "    self.epoch_counter=0\n",
        "    # self.ind=-1\n",
        "    self.ps=params.ps # batch/patch size for images for training\n",
        "    #self.lock = threading.Lock() # used for making thread safe iterators\n",
        "    #self.on_epoch_end()\n",
        "    \n",
        "#     self.gt_images = [None] * 6000\n",
        "#     self.input_images = {}\n",
        "#     self.input_images['300'] = [None] * len(train_ids)\n",
        "#     self.input_images['250'] = [None] * len(train_ids)\n",
        "#     self.input_images['100'] = [None] * len(train_ids)\n",
        "    \n",
        "  def on_epoch_end(self):\n",
        "    self.shuffled_ids=np.random.permutation(len(self.train_ids))\n",
        "#     print(\"in on epoch end\")\n",
        "\n",
        "  def loadImages(self):\n",
        "    i=0\n",
        "    for train_id in self.train_ids:\n",
        "      if(i>=200):\n",
        "        self.gt_image_map[train_id]=None\n",
        "      else:\n",
        "        print('loading gt_image id:',train_id)\n",
        "        gt_files = glob.glob(self.gt_dir + '%05d_00*.ARW' % train_id) # get the ground truth image path(s) (only 1 may exist. hence select [0]th below)\n",
        "        gt_path = gt_files[0] # get the first one (as only 1 gt image should exist)\n",
        "        gt_raw = rawpy.imread(gt_path)\n",
        "        im = gt_raw.postprocess(use_camera_wb=True,\n",
        "                      half_size=False,\n",
        "                      no_auto_bright=True, output_bps=16)\n",
        "        gt_images = np.expand_dims(np.float32(im / 65535.0),axis=0) # divide by 65535 to normalise (scale between 0 and 1)\n",
        "        self.gt_image_map[train_id]=gt_images\n",
        "        i+=1      \n",
        "    print('gt_images loading done')\n",
        "\n",
        "  def __len__(self):\n",
        "    'Denotes the number of batches per epoch'\n",
        "    return len(self.train_ids)\n",
        "\n",
        "  def __getitem__(self, ind):\n",
        "    'Generates data containing batch_size samples' # X : (n_samples, *dim, n_channels)\n",
        "    train_id = self.train_ids[self.shuffled_ids[ind]]\n",
        "    #print(\"retireved the train id: {}, ind: {}\".format(train_id,ind))\n",
        "    in_files = glob.glob(self.input_dir + '%05d_00*.ARW' % train_id) # get all the images path with pattern 0*train_id\n",
        "    try:\n",
        "      in_path = in_files[np.random.randint(0, len(in_files))] # get any one image randomly\n",
        "    except:\n",
        "      print(\"exception- retireved the train id:{}, ind:{}, in_files:{}\".format(train_id,ind,in_files))\n",
        "\n",
        "    in_fn = os.path.basename(in_path) # get only the full image name\n",
        "\n",
        "    gt_files = glob.glob(self.gt_dir + '%05d_00*.ARW' % train_id) # get the ground truth image path(s) (only 1 may exist. hence select [0]th below)\n",
        "    gt_path = gt_files[0] # get the first one (as only 1 gt image should exist)\n",
        "    gt_fn = os.path.basename(gt_path) # get only the full image name for gt image\n",
        "    in_exposure = float(in_fn[9:-5]) # get the exposure for input image\n",
        "    gt_exposure = float(gt_fn[9:-5]) # get the exposure for gt image\n",
        "    ratio = min(gt_exposure / in_exposure, 300) # get the amplification ratio\n",
        "\n",
        "    start = time.process_time()\n",
        "    #cnt += 1\n",
        "\n",
        "    #if input_images[str(ratio)[0:3]][ind] is None: # if image is not loaded (first epoch), load it\n",
        "    raw = rawpy.imread(in_path)\n",
        "    input_images = np.expand_dims(pack_raw(raw), axis=0) * ratio # pack the bayer image in 4 channels of RGBG\n",
        "    if self.gt_image_map[train_id] is None:\n",
        "      print(\"reading gt image\")\n",
        "      # print(\"dict len now:\",len(self.gt_image_map))\n",
        "      gt_raw = rawpy.imread(gt_path)\n",
        "      im = gt_raw.postprocess(use_camera_wb=True,\n",
        "                    half_size=False,\n",
        "                    no_auto_bright=True, output_bps=16)\n",
        "      gt_images = np.expand_dims(np.float32(im / 65535.0),axis=0) # divide by 65535 to normalise (scale between 0 and 1)\n",
        "      self.gt_image_map[train_id]=gt_images\n",
        "    else:\n",
        "      gt_images = self.gt_image_map[train_id]\n",
        "    # crop\n",
        "    print(\"time taken for reading image %d:%s with in_path: %s & gt_path:%s\" % (train_id,(time.process_time() - start),in_path,gt_path))\n",
        "\n",
        "    H = input_images.shape[1] # get the image height (number of rows)\n",
        "    W = input_images.shape[2] # get the image width (number of columns)\n",
        "\n",
        "    xx = np.random.randint(0, W - self.ps) # get a random number in W-ps (W-512)\n",
        "    yy = np.random.randint(0, H - self.ps) # get a random number in H-ps (H-512)\n",
        "    input_patch = input_images[:, yy:yy + self.ps, xx:xx + self.ps, :]\n",
        "    gt_patch = gt_images[:, yy * 2:yy * 2 + self.ps * 2, xx * 2:xx * 2 + self.ps * 2, :]\n",
        "\n",
        "    if np.random.randint(2) == 1:  # random flip for rows\n",
        "      input_patch = np.flip(input_patch, axis=1)\n",
        "      gt_patch = np.flip(gt_patch, axis=1)\n",
        "    if np.random.randint(2) == 1:  # random flip for columns\n",
        "      input_patch = np.flip(input_patch, axis=2)\n",
        "      gt_patch = np.flip(gt_patch, axis=2)\n",
        "    if np.random.randint(2) == 1:  # random transpose\n",
        "      input_patch = np.transpose(input_patch, (0, 2, 1, 3))\n",
        "      gt_patch = np.transpose(gt_patch, (0, 2, 1, 3))\\\n",
        "\n",
        "    input_patch = np.minimum(input_patch, 1.0)\n",
        "    # print('generated an image with ind:',ind)\n",
        "\n",
        "    return (input_patch,gt_patch)\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YECuKfSCS7JW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# import datetime\n",
        "# class StoreEpochCallBack(tf.keras.callbacks.Callback):\n",
        "#   def __init__(self, params):\n",
        "#     self.f=open(params.epoch_num_file,\"w\")\n",
        "#   def on_train_batch_begin(self, batch, logs=None):\n",
        "#     print('Training: batch {} begins at {}'.format(batch, datetime.datetime.now().time()))\n",
        "\n",
        "#   def on_train_batch_end(self, batch, logs=None):\n",
        "#     print('Training: batch {} ends at {}'.format(batch, datetime.datetime.now().time()))\n",
        "\n",
        "#   def on_test_batch_begin(self, batch, logs=None):\n",
        "#     print('Evaluating: batch {} begins at {}'.format(batch, datetime.datetime.now().time()))\n",
        "\n",
        "#   def on_test_batch_end(self, batch, logs=None):\n",
        "#     print('Evaluating: batch {} ends at {}'.format(batch, datetime.datetime.now().time()))\n",
        "\n",
        "def train_generator(model,data_gen,epochs,steps_per_epoch,save_fname,epoch_num_file,save_dir):\n",
        "  print(\"Model Training started!!! Jay Yogeshwar!!!\")\n",
        "  last_epoch=0\n",
        "  last_line=\"\"\n",
        "  with open(epoch_num_file, mode='r', buffering=1) as begin_epoch_file:\n",
        "    last_line=list(begin_epoch_file)[-1]\n",
        "  last_epoch=json.loads(last_line)['epoch']\n",
        "  print(\"Epoch to start:\",last_epoch)\n",
        "  if(last_epoch>10):\n",
        "    list_of_files=glob.glob(save_dir+\"*.hdf5\")\n",
        "    latest_file_path = max(list_of_files, key=os.path.getctime)\n",
        "    print (\"Latest saved model path:\",latest_file_path)\n",
        "    model.load_weights(latest_file_path)\n",
        "  epoch_file=open(epoch_num_file, mode='a', buffering=1)\n",
        "  epoch_callback = keras.callbacks.LambdaCallback(\n",
        "    on_epoch_begin=lambda epoch, logs: epoch_file.write(\n",
        "        json.dumps({'epoch': (epoch+1), 'loss': 0.0123, 'datetime': (dt.datetime.now().strftime('%d%m%Y-%H%M%S'))}) + '\\n'),\n",
        "    on_train_end=lambda logs: epoch_file.close()\n",
        "  )\n",
        "  callbacks = [\n",
        "    epoch_callback,\n",
        "    ModelCheckpoint(period=10,filepath=save_fname, mode='min', monitor='loss', save_weights_only=True, verbose=1, save_best_only=True)\n",
        "  ]\n",
        "  model_history=model.fit_generator(\n",
        "      initial_epoch=last_epoch,\n",
        "      generator=data_gen,\n",
        "      steps_per_epoch=steps_per_epoch,\n",
        "      epochs=epochs,\n",
        "      callbacks=callbacks,\n",
        "      max_queue_size=5,\n",
        "      workers=2,\n",
        "      use_multiprocessing=True\n",
        "  )\n",
        "  print(\"Model Training completed!!! Jay Yogeshwar!!!\")\n",
        "  return model_history"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vvdY4QQSpiKq",
        "colab_type": "code",
        "outputId": "0a5f22d3-aec6-4053-e38a-fd89ba25f8eb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 860
        }
      },
      "source": [
        "from keras import backend as K\n",
        "\n",
        "def recall(y_true, y_pred):\n",
        "  true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "  possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
        "  recall = true_positives / (possible_positives + K.epsilon())\n",
        "  return recall\n",
        "\n",
        "def precision(y_true, y_pred):\n",
        "  true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "  predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
        "  precision = true_positives / (predicted_positives + K.epsilon())\n",
        "  return precision\n",
        "\n",
        "def reduced_mean(y_true,y_pred):\n",
        "  return tf.reduce_mean(tf.abs(y_pred - y_true))\n",
        "\n",
        "epochs=4001\n",
        "params = Params(epochs)\n",
        "data = Data(params)\n",
        "dataloader = DataLoader(params,data)\n",
        "model=build_model()\n",
        "with open('model_summary.txt','w') as fh:\n",
        "    # Pass the file handle in as a lambda function to make it callable\n",
        "    model.summary(print_fn=lambda x: fh.write(x + '\\n'))\n",
        "keras.utils.vis_utils.plot_model(model, to_file='model_plot.png', show_shapes=True, show_layer_names=True)\n",
        "model.compile(optimizer='adam',loss=reduced_mean,metrics=['accuracy'])\n",
        "steps_per_epoch=len(data.train_ids)\n",
        "# save_fname = os.path.join(params.save_dir, '%s-e%s.h5' % (dt.datetime.now().strftime('%d%m%Y-%H%M%S'), str(epochs)))\n",
        "save_fname = os.path.join(params.save_dir, '%s-weights-e-{epoch:02d}-l-{loss:.2f}.hdf5' % (dt.datetime.now().strftime('%d%m%Y-%H%M%S')))\n",
        "\n"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total Train Ids: 161\n",
            "Sample Train Ids: [90, 91, 92, 72, 73, 75, 76, 78, 81, 83]\n",
            "Total Validation Ids: 20\n",
            "Sample Validation Ids: [20079, 20080, 20089, 20107, 20115, 20120, 20147, 20153, 20177, 20005]\n",
            "conv1 shape: (?, ?, ?, 32)\n",
            "conv1 shape: (?, ?, ?, 32)\n",
            "conv1 shape after pool: (?, ?, ?, 32)\n",
            "layer: upsample_concat_1\n",
            "x1 shape: (?, ?, ?, 512)\n",
            "x2 shape: (?, ?, ?, 256)\n",
            "deconvtf shape: (?, ?, ?, 256)\n",
            "deconv_output shape: (?, ?, ?, 512)\n",
            "layer: upsample_concat_1\n",
            "x1 shape: (?, ?, ?, 512)\n",
            "x2 shape: (?, ?, ?, 256)\n",
            "deconvtf shape: (?, ?, ?, 256)\n",
            "deconv_output shape: (?, ?, ?, 512)\n",
            "layer: upsample_concat_2\n",
            "x1 shape: (?, ?, ?, 256)\n",
            "x2 shape: (?, ?, ?, 128)\n",
            "deconvtf shape: (?, ?, ?, 128)\n",
            "deconv_output shape: (?, ?, ?, 256)\n",
            "layer: upsample_concat_2\n",
            "x1 shape: (?, ?, ?, 256)\n",
            "x2 shape: (?, ?, ?, 128)\n",
            "deconvtf shape: (?, ?, ?, 128)\n",
            "deconv_output shape: (?, ?, ?, 256)\n",
            "layer: upsample_concat_3\n",
            "x1 shape: (?, ?, ?, 128)\n",
            "x2 shape: (?, ?, ?, 64)\n",
            "deconvtf shape: (?, ?, ?, 64)\n",
            "deconv_output shape: (?, ?, ?, 128)\n",
            "layer: upsample_concat_3\n",
            "x1 shape: (?, ?, ?, 128)\n",
            "x2 shape: (?, ?, ?, 64)\n",
            "deconvtf shape: (?, ?, ?, 64)\n",
            "deconv_output shape: (?, ?, ?, 128)\n",
            "layer: upsample_concat_4\n",
            "x1 shape: (?, ?, ?, 64)\n",
            "x2 shape: (?, ?, ?, 32)\n",
            "deconvtf shape: (?, ?, ?, 32)\n",
            "deconv_output shape: (?, ?, ?, 64)\n",
            "layer: upsample_concat_4\n",
            "x1 shape: (?, ?, ?, 64)\n",
            "x2 shape: (?, ?, ?, 32)\n",
            "deconvtf shape: (?, ?, ?, 32)\n",
            "deconv_output shape: (?, ?, ?, 64)\n",
            "conv9 shape: (?, ?, ?, 32)\n",
            "conv10 shape: (?, ?, ?, 12)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UoSXvhsPndeE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# dataloader.loadImages()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mJ8IR20yraX8",
        "colab_type": "code",
        "outputId": "5db2b155-61f5-4ec8-d8db-63e4c7386a18",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "\"\"\" ================ TRAIN THE MODEL ================ \"\"\"\n",
        "# steps_per_epoch=10\n",
        "model_history=train_generator(model,dataloader,params.epochs,steps_per_epoch,save_fname,params.epoch_file,params.save_dir)"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model Training started!!! Jay Yogeshwar!!!\n",
            "Epoch to start: 11\n",
            "Epoch 1/4001\n",
            "reading gt image\n",
            "\n",
            "reading gt image\n",
            "time taken for reading image 53:3.8685194320000003 with in_path: /content/drive/My Drive/Data/SeeInDarkDataset/dataset/Sony/short/00053_00_0.04s.ARW & gt_path:/content/drive/My Drive/Data/SeeInDarkDataset/dataset/Sony/long/00053_00_10s.ARW\n",
            "time taken for reading image 142:3.853976796 with in_path: /content/drive/My Drive/Data/SeeInDarkDataset/dataset/Sony/short/00142_00_0.1s.ARW & gt_path:/content/drive/My Drive/Data/SeeInDarkDataset/dataset/Sony/long/00142_00_30s.ARW\n",
            "reading gt image\n",
            "reading gt image\n",
            "time taken for reading image 96:4.526813269 with in_path: /content/drive/My Drive/Data/SeeInDarkDataset/dataset/Sony/short/00096_00_0.1s.ARW & gt_path:/content/drive/My Drive/Data/SeeInDarkDataset/dataset/Sony/long/00096_00_30s.ARW\n",
            "reading gt image\n",
            "  1/161 [..............................] - ETA: 30:16 - loss: 0.2259 - acc: 0.3511time taken for reading image 215:4.661266948000001 with in_path: /content/drive/My Drive/Data/SeeInDarkDataset/dataset/Sony/short/00215_00_0.1s.ARW & gt_path:/content/drive/My Drive/Data/SeeInDarkDataset/dataset/Sony/long/00215_00_10s.ARW\n",
            "  4/161 [..............................] - ETA: 7:53 - loss: 0.1980 - acc: 0.2739 reading gt image\n",
            "time taken for reading image 202:4.545836231999999 with in_path: /content/drive/My Drive/Data/SeeInDarkDataset/dataset/Sony/short/00202_00_0.1s.ARW & gt_path:/content/drive/My Drive/Data/SeeInDarkDataset/dataset/Sony/long/00202_00_10s.ARW\n",
            "  5/161 [..............................] - ETA: 7:47 - loss: 0.2151 - acc: 0.2928time taken for reading image 10:4.422031986 with in_path: /content/drive/My Drive/Data/SeeInDarkDataset/dataset/Sony/short/00010_00_0.04s.ARW & gt_path:/content/drive/My Drive/Data/SeeInDarkDataset/dataset/Sony/long/00010_00_10s.ARW\n",
            "reading gt image\n",
            "  6/161 [>.............................] - ETA: 6:53 - loss: 0.1983 - acc: 0.3059reading gt image\n",
            "time taken for reading image 209:4.553115893000001 with in_path: /content/drive/My Drive/Data/SeeInDarkDataset/dataset/Sony/short/00209_00_0.1s.ARW & gt_path:/content/drive/My Drive/Data/SeeInDarkDataset/dataset/Sony/long/00209_00_10s.ARW\n",
            "time taken for reading image 12:4.349251595 with in_path: /content/drive/My Drive/Data/SeeInDarkDataset/dataset/Sony/short/00012_00_0.04s.ARW & gt_path:/content/drive/My Drive/Data/SeeInDarkDataset/dataset/Sony/long/00012_00_10s.ARW\n",
            "  8/161 [>.............................] - ETA: 6:37 - loss: 0.2013 - acc: 0.3114reading gt image\n",
            "reading gt image\n",
            "time taken for reading image 18:4.569490957999999 with in_path: /content/drive/My Drive/Data/SeeInDarkDataset/dataset/Sony/short/00018_00_0.1s.ARW & gt_path:/content/drive/My Drive/Data/SeeInDarkDataset/dataset/Sony/long/00018_00_10s.ARW\n",
            "time taken for reading image 75:4.350586692 with in_path: /content/drive/My Drive/Data/SeeInDarkDataset/dataset/Sony/short/00075_00_0.1s.ARW & gt_path:/content/drive/My Drive/Data/SeeInDarkDataset/dataset/Sony/long/00075_00_30s.ARW\n",
            " 10/161 [>.............................] - ETA: 6:32 - loss: 0.1949 - acc: 0.3030reading gt image\n",
            "reading gt image\n",
            "time taken for reading image 132:4.362112599 with in_path: /content/drive/My Drive/Data/SeeInDarkDataset/dataset/Sony/short/00132_00_0.1s.ARW & gt_path:/content/drive/My Drive/Data/SeeInDarkDataset/dataset/Sony/long/00132_00_30s.ARW\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-59-f40381a18905>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;34m\"\"\" ================ TRAIN THE MODEL ================ \"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# steps_per_epoch=10\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mmodel_history\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdataloader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msave_fname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepoch_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-56-af007d12ac11>\u001b[0m in \u001b[0;36mtrain_generator\u001b[0;34m(model, data_gen, epochs, steps_per_epoch, save_fname, epoch_num_file)\u001b[0m\n\u001b[1;32m     27\u001b[0m       \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m       \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m       \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m   )\n\u001b[1;32m     31\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Model Training completed!!! Jay Yogeshwar!!!\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name + '` call to the ' +\n\u001b[1;32m     90\u001b[0m                               'Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1416\u001b[0m             \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1417\u001b[0m             \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1418\u001b[0;31m             initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1419\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1420\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m    215\u001b[0m                 outs = model.train_on_batch(x, y,\n\u001b[1;32m    216\u001b[0m                                             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m                                             class_weight=class_weight)\n\u001b[0m\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[1;32m   1215\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1216\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1217\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1218\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0munpack_singleton\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1456\u001b[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[1;32m   1457\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1458\u001b[0;31m                                                run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1459\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1460\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        },
        {
          "output_type": "stream",
          "text": [
            "Process ForkPoolWorker-12:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
            "    self.run()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"/usr/lib/python3.6/multiprocessing/pool.py\", line 119, in worker\n",
            "    result = (True, func(*args, **kwds))\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/keras/utils/data_utils.py\", line 401, in get_index\n",
            "    return _SHARED_SEQUENCES[uid][i]\n",
            "  File \"<ipython-input-31-55d3c2cbbfa1>\", line 74, in __getitem__\n",
            "    raw = rawpy.imread(in_path)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/rawpy/__init__.py\", line 20, in imread\n",
            "    d.open_file(pathOrFile)\n",
            "KeyboardInterrupt\n",
            "Process ForkPoolWorker-11:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
            "    self.run()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"/usr/lib/python3.6/multiprocessing/pool.py\", line 119, in worker\n",
            "    result = (True, func(*args, **kwds))\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/keras/utils/data_utils.py\", line 401, in get_index\n",
            "    return _SHARED_SEQUENCES[uid][i]\n",
            "  File \"<ipython-input-31-55d3c2cbbfa1>\", line 82, in __getitem__\n",
            "    no_auto_bright=True, output_bps=16)\n",
            "KeyboardInterrupt\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K6rt0FGT6jY-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# for epoch in range(lastepoch, 4001):\n",
        "#     #if os.path.isdir('result/%04d' % epoch):\n",
        "#     #    continue\n",
        "#     ###\n",
        "#     # used for skipping the current epoch if the checkpoint directory exists\n",
        "#     ###\n",
        "#     cnt = 0\n",
        "#     if epoch > 2000:\n",
        "#         learning_rate = 1e-5\n",
        "\n",
        "#     for ind in np.random.permutation(len(train_ids)):\n",
        "\n",
        "#         # get the path from image id\n",
        "\n",
        "#         train_id = train_ids[ind]\n",
        "#         in_files = glob.glob(input_dir + '%05d_00*.ARW' % train_id) # get all the images path with pattern 0*train_id\n",
        "#         in_path = in_files[np.random.randint(0, len(in_files))] # get any one image randomly\n",
        "#         in_fn = os.path.basename(in_path) # get only the full image name\n",
        "\n",
        "#         gt_files = glob.glob(gt_dir + '%05d_00*.ARW' % train_id) # get the ground truth image path(s) (only 1 may exist. hence select [0]th below)\n",
        "#         gt_path = gt_files[0] # get the first one (as only 1 gt image should exist)\n",
        "#         gt_fn = os.path.basename(gt_path) # get only the full image name for gt image\n",
        "#         in_exposure = float(in_fn[9:-5]) # get the exposure for input image\n",
        "#         gt_exposure = float(gt_fn[9:-5]) # get the exposure for gt image\n",
        "#         ratio = min(gt_exposure / in_exposure, 300) # get the amplification ratio\n",
        "\n",
        "#         st = time.time()\n",
        "#         cnt += 1\n",
        "\n",
        "#         if input_images[str(ratio)[0:3]][ind] is None: # if image is not loaded (first epoch), load it\n",
        "#             raw = rawpy.imread(in_path)\n",
        "#             input_images[str(ratio)[0:3]][ind] = \\\n",
        "#                 np.expand_dims(pack_raw(raw), axis=0) * ratio # pack the bayer image in 4 channels of RGBG\n",
        "\n",
        "#             gt_raw = rawpy.imread(gt_path)\n",
        "#             im = gt_raw.postprocess(use_camera_wb=True,\n",
        "#                                     half_size=False,\n",
        "#                                     no_auto_bright=True, output_bps=16)\n",
        "#             gt_images[ind] = np.expand_dims(np.float32(im / 65535.0),axis=0) # divide by 65535 to normalise (scale between 0 and 1)\n",
        "\n",
        "#         # crop\n",
        "\n",
        "#         H = input_images[str(ratio)[0:3]][ind].shape[1] # get the image height (number of rows)\n",
        "#         W = input_images[str(ratio)[0:3]][ind].shape[2] # get the image width (number of columns)\n",
        "\n",
        "#         xx = np.random.randint(0, W - ps) # get a random number in W-ps (W-512)\n",
        "#         yy = np.random.randint(0, H - ps) # get a random number in H-ps (H-512)\n",
        "#         input_patch = input_images[str(ratio)[0:3]][ind][:, yy:yy + ps,\n",
        "#                 xx:xx + ps, :]\n",
        "#         gt_patch = gt_images[ind][:, yy * 2:yy * 2 + ps * 2, xx * 2:xx\n",
        "#                                   * 2 + ps * 2, :]\n",
        "\n",
        "#         if np.random.randint(2) == 1:  # random flip for rows\n",
        "#             input_patch = np.flip(input_patch, axis=1)\n",
        "#             gt_patch = np.flip(gt_patch, axis=1)\n",
        "#         if np.random.randint(2) == 1:  # random flip for columns\n",
        "#             input_patch = np.flip(input_patch, axis=2)\n",
        "#             gt_patch = np.flip(gt_patch, axis=2)\n",
        "#         if np.random.randint(2) == 1:  # random transpose\n",
        "#             input_patch = np.transpose(input_patch, (0, 2, 1, 3))\n",
        "#             gt_patch = np.transpose(gt_patch, (0, 2, 1, 3))\\\n",
        "\n",
        "#         input_patch = np.minimum(input_patch, 1.0)\n",
        "\n",
        "#         (_, G_current, output) = sess.run([G_opt, G_loss, out_image],\n",
        "#                 feed_dict={in_image: input_patch, gt_image: gt_patch,\n",
        "#                 lr: learning_rate})\n",
        "#         output = np.minimum(np.maximum(output, 0), 1)\n",
        "#         g_loss[ind] = G_current\n",
        "\n",
        "#         print '%d %d Loss=%.3f Time=%.3f' % (epoch, cnt,\n",
        "#                 np.mean(g_loss[np.where(g_loss)]), time.time() - st)\n",
        "\n",
        "#         if epoch % save_freq == 0:\n",
        "#             if not os.path.isdir(result_dir + '%04d' % epoch):\n",
        "#                 os.makedirs(result_dir + '%04d' % epoch)\n",
        "\n",
        "#             temp = np.concatenate((gt_patch[0, :, :, :], output[0, :, :\n",
        "#                                   , :]), axis=1)\n",
        "#             scipy.misc.toimage(temp * 255, high=255, low=0, cmin=0,\n",
        "#                                cmax=255).save(result_dir\n",
        "#                     + '%04d/%05d_00_train_%d.jpg' % (epoch, train_id,\n",
        "#                     ratio))\n",
        "\n",
        "#     saver.save(sess, checkpoint_dir + 'model.ckpt')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ir4Xcjn_mfhx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import multiprocessing\n",
        "\n",
        "multiprocessing.gpu_count()"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}