{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SeeInDark_Keras_Tensorflow_Sequence_Test.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KashyapCKotak/AI-ML-experiments/blob/master/SeeInDark_Keras_Tensorflow_Sequence_Test.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zuMNVBqt0u-H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# import requests\n",
        "# import os\n",
        "\n",
        "# def download_file_from_google_drive(id, destination):\n",
        "#     URL = \"https://docs.google.com/uc?export=download\"\n",
        "\n",
        "#     session = requests.Session()\n",
        "\n",
        "#     response = session.get(URL, params = { 'id' : id }, stream = True)\n",
        "#     token = get_confirm_token(response)\n",
        "\n",
        "#     if token:\n",
        "#         params = { 'id' : id, 'confirm' : token }\n",
        "#         response = session.get(URL, params = params, stream = True)\n",
        "\n",
        "#     save_response_content(response, destination)    \n",
        "\n",
        "# def get_confirm_token(response):\n",
        "#     for key, value in response.cookies.items():\n",
        "#         if key.startswith('download_warning'):\n",
        "#             return value\n",
        "\n",
        "#     return None\n",
        "\n",
        "# def save_response_content(response, destination):\n",
        "#     CHUNK_SIZE = 32768\n",
        "\n",
        "#     with open(destination, \"wb\") as f:\n",
        "#         for chunk in response.iter_content(CHUNK_SIZE):\n",
        "#             if chunk: # filter out keep-alive new chunks\n",
        "#                 f.write(chunk)\n",
        "\n",
        "\n",
        "\n",
        "# print('Dowloading Sony subset... (25GB)')\n",
        "# download_file_from_google_drive('10kpAcvldtcb9G2ze5hTcF1odzu4V_Zvh', './Sony.zip')\n",
        "\n",
        "# #print('Dowloading Fuji subset... (52GB)')\n",
        "# #download_file_from_google_drive('12hvKCjwuilKTZPe9EZ7ZTb-azOmUA3HT', 'dataset/Fuji.zip')\n",
        "\n",
        "# os.system('unzip ./Sony.zip -d dataset')\n",
        "# #os.system('unzip dataset/Fuji.zip -d dataset')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FsP1lrRN-HPz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# import shutil\n",
        "# shutil.rmtree('result_Sony', ignore_errors=True, onerror=None)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P-_85I6g3eA5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# os.system('unzip ./Sony.zip -d dataset')\n",
        "#os.system('unzip dataset/Fuji.zip -d dataset')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xDQVxKF8nooG",
        "colab_type": "code",
        "outputId": "0d42add6-40e0-4e8f-c22c-8d07c0da8b90",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8yyhvphaOsft",
        "colab_type": "code",
        "outputId": "f9ca9e8c-275f-4787-fac4-6a72b36154db",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import json\n",
        "import glob\n",
        "import os\n",
        "epoch_file='/content/drive/My Drive/Data/SeeInDarkDataset/dataset/Sony/save_Sony/epochNum.txt'\n",
        "save_dir='/content/drive/My Drive/Data/SeeInDarkDataset/dataset/Sony/save_Sony/'\n",
        "last_epoch=0\n",
        "last_line=\"\"\n",
        "with open(epoch_file, mode='r', buffering=1) as epoch_file:\n",
        "  last_line=list(epoch_file)[-1]\n",
        "last_epoch=json.loads(last_line)['epoch']\n",
        "print(\"Epoch to start:\",last_epoch)\n",
        "list_of_files=glob.glob(save_dir+\"*.hdf5\")\n",
        "if not last_epoch<10:\n",
        "  latest_file_path = max(list_of_files, key=os.path.getctime)\n",
        "  print (\"Latest file path:\",latest_file_path)\n",
        "print(int(latest_file_path.split('/')[-1].split('-')[-3])-1)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch to start: 2665\n",
            "Latest file path: /content/drive/My Drive/Data/SeeInDarkDataset/dataset/Sony/save_Sony/10082019-123212-weights-e-2537-l-0.0227426.hdf5\n",
            "2536\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DJhrjFQXhTZv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# with open('/content/drive/My Drive/Data/SeeInDarkDataset/dataset/Sony/save_Sony/epochNum.txt', mode='w', buffering=1) as epoch_file:\n",
        "#   epoch_file.write(json.dumps({'epoch': -1, 'loss': 0, 'datetime': 'dummy start line'}) + '\\n'),"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z9yF04nOlUCP",
        "colab_type": "text"
      },
      "source": [
        "**Main Train Code:**\n",
        "================================="
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m_zLVCtYlfSH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install rawpy"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IogQOu0slZNT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from __future__ import division\n",
        "import os, time, scipy.io\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import rawpy\n",
        "import glob\n",
        "import keras\n",
        "import shutil\n",
        "import datetime as dt\n",
        "from keras.callbacks import EarlyStopping, ModelCheckpoint, TensorBoard, LearningRateScheduler\n",
        "import threading\n",
        "import json"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pv0LQyX43bPE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# !pip install -q tf-nightly-2.0-preview\n",
        "# # Load the TensorBoard notebook extension\n",
        "# %load_ext tensorboard"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-gTj24DXlazJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# import shutil\n",
        "# shutil.rmtree('result_Sony', ignore_errors=True, onerror=None)\n",
        "\n",
        "# input_dir = '/content/drive/My Drive/Data/SeeInDarkDataset/dataset/Sony/short/'\n",
        "# gt_dir = '/content/drive/My Drive/Data/SeeInDarkDataset/dataset/Sony/long/'\n",
        "# os.mkdir('result_Sony')\n",
        "# checkpoint_dir = './result_Sony/'\n",
        "# result_dir = './result_Sony/'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vexCrDNtiIrN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def lrelu(x): # not used. Used Kears LeakyReLU with alpha 0.2 instead\n",
        "    return tf.maximum(x * 0.2, x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6ZkJ3o0ZiKQk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\" This cell contains the custom Lambda layers to be used in building the  keras mdoel \"\"\"\n",
        "def upsample_and_concat(x1,x2,output_channels,in_channels,layer,verbose=False):\n",
        "  if(verbose):\n",
        "    print(\"layer:\",layer)\n",
        "  pool_size = 2\n",
        "  deconv_filter = tf.Variable(tf.truncated_normal([pool_size, pool_size, output_channels, in_channels], stddev=0.02))\n",
        "  if(verbose):\n",
        "    print(\"x1 shape:\",x1.shape)\n",
        "    print(\"x2 shape:\",x2.shape)\n",
        "  deconvtf=tf.nn.conv2d_transpose(x1, deconv_filter, tf.shape(x2), strides=[1, pool_size, pool_size, 1])\n",
        "  if(verbose):\n",
        "    print(\"deconvtf shape:\",deconvtf.shape)\n",
        "  #deconv=keras.layers.Conv2DTranspose(filters=output_channels,kernel_size=(pool_size, pool_size),strides=(pool_size, pool_size))(x1)\n",
        "  #print(\"deconvk shape:\",deconv.shape)\n",
        "\n",
        "  deconv_output = tf.concat([deconvtf, x2], 3)\n",
        "  if(verbose):\n",
        "    print(\"deconv_output shape:\",deconv_output.shape)\n",
        "  deconv_output.set_shape([None, None, None, output_channels * 2])\n",
        "\n",
        "  return deconv_output\n",
        "\n",
        "def Depth_to_space_tf(input):\n",
        "  return tf.depth_to_space(input, 2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2cRpNNVlYXKz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# testing code\n",
        "# t=tf.ones([1,450,350,4])\n",
        "# deconv_filter = tf.Variable(tf.truncated_normal([2, 2, 256, 512], stddev=0.02))\n",
        "# tf.nn.conv2d_transpose(t, deconv_filter, [1,450,350,], strides=[1, pool_size, pool_size, 1])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QUz7pW3dm8Qh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def build_model():\n",
        "  inputs=keras.layers.Input(shape=(None, None, 4))\n",
        "  #definition: slim.conv2d(inputs,number of outputs, kernel shape)\n",
        "  #definition: slim.max_pool2d(inputsconv2d_transpose ,kernel_size, padding)\n",
        "  #conv1 = slim.conv2d(input, 32, [3, 3], rate=1, activation_fn=lrelu, scope='g_conv1_1')\n",
        "  conv1=keras.layers.Conv2D(filters=32,kernel_size=(3,3),strides=(1,1),padding='same',name='conv1_1')(inputs)\n",
        "  conv1=keras.layers.LeakyReLU(alpha=0.2,name='conv1_1_relu')(conv1)\n",
        "  print(\"conv1 shape:\",conv1.shape)\n",
        "  #conv1 = slim.conv2d(conv1, 32, [3, 3], rate=1, activation_fn=lrelu, scope='g_conv1_2')\n",
        "  conv1=keras.layers.Conv2D(filters=32,kernel_size=(3,3),strides=(1,1),padding='same',name='conv1_2')(conv1)\n",
        "  conv1=keras.layers.LeakyReLU(alpha=0.2,name='conv1_2_relu')(conv1)\n",
        "  print(\"conv1 shape:\",conv1.shape)\n",
        "  #pool1 = slim.max_pool2d(conv1, [2, 2], padding='SAME')\n",
        "  pool1=keras.layers.MaxPooling2D(pool_size=(2,2),padding=\"same\",name='pool1')(conv1)\n",
        "  print(\"conv1 shape after pool:\",pool1.shape)\n",
        "\n",
        "  #conv2 = slim.conv2d(pool1, 64, [3, 3], rate=1, activation_fn=lrelu, scope='g_conv2_1')\n",
        "  conv2=keras.layers.Conv2D(filters=64,kernel_size=(3,3),strides=(1,1),padding='same',name='conv2_1')(pool1)\n",
        "  conv2=keras.layers.LeakyReLU(alpha=0.2,name='conv2_1_relu')(conv2)\n",
        "  #conv2 = slim.conv2d(conv2, 64, [3, 3], rate=1, activation_fn=lrelu, scope='g_conv2_2')\n",
        "  conv2=keras.layers.Conv2D(filters=64,kernel_size=(3,3),strides=(1,1),padding='same',name='conv2_2')(conv2)\n",
        "  conv2=keras.layers.LeakyReLU(alpha=0.2,name='conv2_2_relu')(conv2)\n",
        "  #pool2 = slim.max_pool2d(conv2, [2, 2], padding='SAME')\n",
        "  pool2=keras.layers.MaxPooling2D(pool_size=(2,2),padding=\"same\",name='pool2')(conv2)\n",
        "\n",
        "  #conv3 = slim.conv2d(pool2, 128, [3, 3], rate=1, activation_fn=lrelu, scope='g_conv3_1')\n",
        "  conv3=keras.layers.Conv2D(filters=128,kernel_size=(3,3),strides=(1,1),padding='same',name='conv3_1')(pool2)\n",
        "  conv3=keras.layers.LeakyReLU(alpha=0.2,name='conv3_1_relu')(conv3)\n",
        "  #conv3 = slim.conv2d(conv3, 128, [3, 3], rate=1, activation_fn=lrelu, scope='g_conv3_2')\n",
        "  conv3=keras.layers.Conv2D(filters=128,kernel_size=(3,3),strides=(1,1),padding='same',name='conv3_2')(conv3)\n",
        "  conv3=keras.layers.LeakyReLU(alpha=0.2,name='conv3_2_relu')(conv3)\n",
        "  #pool3 = slim.max_pool2d(conv3, [2, 2], padding='SAME')\n",
        "  pool3=keras.layers.MaxPooling2D(pool_size=(2,2),padding=\"same\",name='pool3')(conv3)\n",
        "\n",
        "  #conv4 = slim.conv2d(pool3, 256, [3, 3], rate=1, activation_fn=lrelu, scope='g_conv4_1')\n",
        "  conv4=keras.layers.Conv2D(filters=256,kernel_size=(3,3),strides=(1,1),padding='same',name='conv4_1')(pool3)\n",
        "  conv4=keras.layers.LeakyReLU(alpha=0.2,name='conv4_1_relu')(conv4)\n",
        "  #conv4 = slim.conv2d(conv4, 256, [3, 3], rate=1, activation_fn=lrelu, scope='g_conv4_2')\n",
        "  conv4=keras.layers.Conv2D(filters=256,kernel_size=(3,3),strides=(1,1),padding='same',name='conv4_2')(conv4)\n",
        "  conv4=keras.layers.LeakyReLU(alpha=0.2,name='conv4_2_relu')(conv4)\n",
        "  #pool4 = slim.max_pool2d(conv4, [2, 2], padding='SAME')\n",
        "  pool4=keras.layers.MaxPooling2D(pool_size=(2,2),padding=\"same\",name='pool4')(conv4)\n",
        "\n",
        "  #conv5 = slim.conv2d(pool4, 512, [3, 3], rate=1, activation_fn=lrelu, scope='g_conv5_1')\n",
        "  conv5=keras.layers.Conv2D(filters=512,kernel_size=(3,3),strides=(1,1),padding='same',name='conv5_1')(pool4)\n",
        "  conv5=keras.layers.LeakyReLU(alpha=0.2,name='conv5_1_relu')(conv5)\n",
        "  #conv5 = slim.conv2d(conv5, 512, [3, 3], rate=1, activation_fn=lrelu, scope='g_conv5_2')\n",
        "  conv5=keras.layers.Conv2D(filters=512,kernel_size=(3,3),strides=(1,1),padding='same',name='conv5_2')(conv5)\n",
        "  conv5=keras.layers.LeakyReLU(alpha=0.2,name='conv5_2_relu')(conv5)\n",
        "\n",
        "  #up6 = upsample_and_concat(conv5, conv4, 256, 512)\n",
        "  up6=keras.layers.core.Lambda(upsample_and_concat,arguments={'x2':conv4,'output_channels':256,'in_channels':512,'layer':'upsample_concat_1','verbose':False},name='upsample_concat_1')(conv5)\n",
        "  #conv6 = slim.conv2d(up6, 256, [3, 3], rate=1, activation_fn=lrelu, scope='g_conv6_1')\n",
        "  conv6=keras.layers.Conv2D(filters=256,kernel_size=(3,3),strides=(1,1),padding='same',name='conv6_1')(up6)\n",
        "  conv6=keras.layers.LeakyReLU(alpha=0.2,name='conv6_1_relu')(conv6)\n",
        "  #conv6 = slim.conv2d(conv6, 256, [3, 3], rate=1, activation_fn=lrelu, scope='g_conv6_2')\n",
        "  conv6=keras.layers.Conv2D(filters=256,kernel_size=(3,3),strides=(1,1),padding='same',name='conv6_2')(conv6)\n",
        "  conv6=keras.layers.LeakyReLU(alpha=0.2,name='conv6_2_relu')(conv6)\n",
        "\n",
        "  #up7 = upsample_and_concat(conv6, conv3, 128, 256)\n",
        "  up7=keras.layers.core.Lambda(upsample_and_concat,arguments={'x2':conv3,'output_channels':128,'in_channels':256,'layer':'upsample_concat_2','verbose':False},name='upsample_concat_2')(conv6)\n",
        "  #conv7 = slim.conv2d(up7, 128, [3, 3], rate=1, activation_fn=lrelu, scope='g_conv7_1')\n",
        "  conv7=keras.layers.Conv2D(filters=128,kernel_size=(3,3),strides=(1,1),padding='same',name='conv7_1')(up7)\n",
        "  conv7=keras.layers.LeakyReLU(alpha=0.2,name='conv7_1_relu')(conv7)\n",
        "  #conv7 = slim.conv2d(conv7, 128, [3, 3], rate=1, activation_fn=lrelu, scope='g_conv7_2')\n",
        "  conv7=keras.layers.Conv2D(filters=128,kernel_size=(3,3),strides=(1,1),padding='same',name='conv7_2')(conv7)\n",
        "  conv7=keras.layers.LeakyReLU(alpha=0.2,name='conv7_2_relu')(conv7)\n",
        "\n",
        "  #up8 = upsample_and_concat(conv7, conv2, 64, 128)\n",
        "  up8=keras.layers.core.Lambda(upsample_and_concat,arguments={'x2':conv2,'output_channels':64,'in_channels':128,'layer':'upsample_concat_3','verbose':False},name='upsample_concat_3')(conv7)\n",
        "  #conv8 = slim.conv2d(up8, 64, [3, 3], rate=1, activation_fn=lrelu, scope='g_conv8_1')\n",
        "  conv8=keras.layers.Conv2D(filters=64,kernel_size=(3,3),strides=(1,1),padding='same',name='conv8_1')(up8)\n",
        "  conv8=keras.layers.LeakyReLU(alpha=0.2,name='conv8_1_relu')(conv8)\n",
        "  #conv8 = slim.conv2d(conv8, 64, [3, 3], rate=1, activation_fn=lrelu, scope='g_conv8_2')\n",
        "  conv8=keras.layers.Conv2D(filters=64,kernel_size=(3,3),strides=(1,1),padding='same',name='conv8_2')(conv8)\n",
        "  conv8=keras.layers.LeakyReLU(alpha=0.2,name='conv8_2_relu')(conv8)\n",
        "\n",
        "  #up9 = upsample_and_concat(conv8, conv1, 32, 64)\n",
        "  up9=keras.layers.core.Lambda(upsample_and_concat,arguments={'x2':conv1,'output_channels':32,'in_channels':64,'layer':'upsample_concat_4','verbose':False},name='upsample_concat_4')(conv8)\n",
        "  #conv9 = slim.conv2d(up9, 32, [3, 3], rate=1, activation_fn=lrelu, scope='g_conv9_1')\n",
        "  conv9=keras.layers.Conv2D(filters=32,kernel_size=(3,3),strides=(1,1),padding='same',name='conv9_1')(up9)\n",
        "  conv9=keras.layers.LeakyReLU(alpha=0.2,name='conv9_1_relu')(conv9)\n",
        "  #conv9 = slim.conv2d(conv9, 32, [3, 3], rate=1, activation_fn=lrelu, scope='g_conv9_2')\n",
        "  conv9=keras.layers.Conv2D(filters=32,kernel_size=(3,3),strides=(1,1),padding='same',name='conv9_2')(conv9)\n",
        "  conv9=keras.layers.LeakyReLU(alpha=0.2,name='conv9_2_relu')(conv9)\n",
        "  print(\"conv9 shape:\",conv9.shape)\n",
        "\n",
        "  #conv10 = slim.conv2d(conv9, 12, [1, 1], rate=1, activation_fn=None, scope='g_conv10')\n",
        "  conv10=keras.layers.Conv2D(filters=12,kernel_size=(1,1),strides=(1,1),name='conv10')(conv9)\n",
        "  print(\"conv10 shape:\",conv10.shape)\n",
        "  #no activation function for this last layer\n",
        "\n",
        "  predictions = keras.layers.core.Lambda(Depth_to_space_tf,name='depth_to_space')(conv10)\n",
        "  model = keras.models.Model(inputs=inputs, outputs=predictions)\n",
        "  \n",
        "  return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g8DyCh5GyNEU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def pack_raw(raw):\n",
        "    # pack Bayer image to 4 channels\n",
        "    im = raw.raw_image_visible.astype(np.float32)\n",
        "    #print(\"im shape:\",im.shape)\n",
        "    #print(\"im:\")\n",
        "    #print(im)\n",
        "    im = np.maximum(im - 512, 0) / (16383 - 512)  # subtract the black level and normalise with 16383 \n",
        "    ##\n",
        "    # Seems 16383 is the max value due to a 14 bit depth. can be replaced with \n",
        "    ##\n",
        "    #print(\"after normalisation\")\n",
        "    #print(im)\n",
        "\n",
        "    im = np.expand_dims(im, axis=2)\n",
        "    img_shape = im.shape\n",
        "    H = img_shape[0]\n",
        "    W = img_shape[1]\n",
        "    #print(\"after expand dims:%s ,H:%d ,W:%d \" % (img_shape,H,W))\n",
        "\n",
        "    ###\n",
        "    # single bayer pixel format:\n",
        "    # 0:R 1:G\n",
        "    # 2:G 3:B\n",
        "    ###\n",
        "    out = np.concatenate((im[0:H:2, 0:W:2, :], # get the 0th Red as per above format\n",
        "                          im[0:H:2, 1:W:2, :], # get the 1st Green\n",
        "                          im[1:H:2, 1:W:2, :], # get the 2nd Green\n",
        "                          im[1:H:2, 0:W:2, :]), axis=2) # get the 3rd Green\n",
        "    #print(\"out.shape:\",out.shape)\n",
        "    #print(out)\n",
        "    return out\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XZKmZNa_0H7N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Params():\n",
        "  def __init__(self):\n",
        "    shutil.rmtree('/content/drive/My Drive/Data/SeeInDarkDataset/dataset/Sony/result_Sony', ignore_errors=True, onerror=None)\n",
        "    self.input_dir = '/content/drive/My Drive/Data/SeeInDarkDataset/dataset/Sony/short/'\n",
        "    self.gt_dir = '/content/drive/My Drive/Data/SeeInDarkDataset/dataset/Sony/long/'\n",
        "    os.mkdir('/content/drive/My Drive/Data/SeeInDarkDataset/dataset/Sony/result_Sony')\n",
        "    self.checkpoint_dir = '/content/drive/My Drive/Data/SeeInDarkDataset/dataset/Sony/result_Sony/'\n",
        "    self.result_dir = '/content/drive/My Drive/Data/SeeInDarkDataset/dataset/Sony/result_Sony/'\n",
        "    self.save_dir = '/content/drive/My Drive/Data/SeeInDarkDataset/dataset/Sony/save_Sony/'\n",
        "    self.epoch_file = '/content/drive/My Drive/Data/SeeInDarkDataset/dataset/Sony/save_Sony/epochNum.txt'\n",
        "    self.tensorboard_log_dir = '/content/drive/My Drive/Data/SeeInDarkDataset/dataset/Sony/tensorboard'\n",
        "    if os.path.exists(self.tensorboard_log_dir):\n",
        "      pass\n",
        "    else:\n",
        "      os.mkdir(self.tensorboard_log_dir)\n",
        "    if os.path.exists(self.save_dir):\n",
        "      pass\n",
        "    else:\n",
        "      os.mkdir('/content/drive/My Drive/Data/SeeInDarkDataset/dataset/Sony/save_Sony')\n",
        "    \n",
        "    \n",
        "    self.epochs=4000\n",
        "    self.epoch_counter=0\n",
        "    self.ps=512 # batch/patch size for images for training\n",
        "    self.learning_rate=0.0001 # not used in keras"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HoZq9FqA1Elm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Data():\n",
        "  def __init__(self,params):\n",
        "    self.train_fns = glob.glob(params.gt_dir + '0*.ARW')\n",
        "    \"\"\" 0* is for Training images. 1 for Test and 2 for Validation \"\"\"\n",
        "    self.train_ids = [int(os.path.basename(train_fn)[0:5]) for train_fn in self.train_fns]\n",
        "    print(\"Total Train Ids:\",len(self.train_ids))\n",
        "    print(\"Sample Train Ids:\",self.train_ids[0:10])\n",
        "    self.total_train_ids=len(self.train_ids)\n",
        "    \n",
        "    self.valid_fns = glob.glob(params.gt_dir + '2*.ARW')\n",
        "    \"\"\" 0* is for Training images. 1 for Test and 2 for Validation \"\"\"\n",
        "    self.valid_ids = [int(os.path.basename(valid_fn)[0:5]) for valid_fn in self.valid_fns]\n",
        "    print(\"Total Validation Ids:\",len(self.valid_ids))\n",
        "    print(\"Sample Validation Ids:\",self.valid_ids[0:10])\n",
        "    self.total_valid_ids=len(self.valid_ids)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bVcN0RNdVzAW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class DataLoader(keras.utils.Sequence):\n",
        "  \"\"\"Class to continously load images\"\"\"\n",
        "  \n",
        "  def __init__(self,params,data):\n",
        "    self.epochs=params.epochs\n",
        "    self.input_dir=params.input_dir\n",
        "    self.gt_dir=params.gt_dir\n",
        "    self.train_ids=data.train_ids\n",
        "    self.gt_image_map=[None] * 6000\n",
        "    self.shuffled_ids=np.random.permutation(len(self.train_ids))\n",
        "#     for index, val in np.ndenumerate(self.shuffled_ids):\n",
        "#       print ('index:{}, image:{}'.format(index[0], val))\n",
        "    self.epoch_counter=0\n",
        "    # self.ind=-1\n",
        "    self.ps=params.ps # batch/patch size for images for training\n",
        "    #self.lock = threading.Lock() # used for making thread safe iterators\n",
        "    #self.on_epoch_end()\n",
        "    \n",
        "#     self.gt_images = [None] * 6000\n",
        "#     self.input_images = {}\n",
        "#     self.input_images['300'] = [None] * len(train_ids)\n",
        "#     self.input_images['250'] = [None] * len(train_ids)\n",
        "#     self.input_images['100'] = [None] * len(train_ids)\n",
        "    \n",
        "  def on_epoch_end(self):\n",
        "    self.shuffled_ids=np.random.permutation(len(self.train_ids))\n",
        "#     print(\"in on epoch end\")\n",
        "\n",
        "  def loadImages(self):\n",
        "    i=0\n",
        "    for train_id in self.train_ids:\n",
        "      if(i>=200):\n",
        "        self.gt_image_map[train_id]=None\n",
        "      else:\n",
        "        print('loading gt_image id:',train_id)\n",
        "        gt_files = glob.glob(self.gt_dir + '%05d_00*.ARW' % train_id) # get the ground truth image path(s) (only 1 may exist. hence select [0]th below)\n",
        "        gt_path = gt_files[0] # get the first one (as only 1 gt image should exist)\n",
        "        gt_raw = rawpy.imread(gt_path)\n",
        "        im = gt_raw.postprocess(use_camera_wb=True,\n",
        "                      half_size=False,\n",
        "                      no_auto_bright=True, output_bps=16)\n",
        "        gt_images = np.expand_dims(np.float32(im / 65535.0),axis=0) # divide by 65535 to normalise (scale between 0 and 1)\n",
        "        self.gt_image_map[train_id]=gt_images\n",
        "        i+=1      \n",
        "    print('gt_images loading done')\n",
        "\n",
        "  def __len__(self):\n",
        "    'Denotes the number of batches per epoch'\n",
        "    return len(self.train_ids)\n",
        "\n",
        "  def __getitem__(self, ind):\n",
        "    'Generates data containing batch_size samples' # X : (n_samples, *dim, n_channels)\n",
        "    train_id = self.train_ids[self.shuffled_ids[ind]]\n",
        "    #print(\"retireved the train id: {}, ind: {}\".format(train_id,ind))\n",
        "    in_files = glob.glob(self.input_dir + '%05d_00*.ARW' % train_id) # get all the images path with pattern 0*train_id\n",
        "    try:\n",
        "      in_path = in_files[np.random.randint(0, len(in_files))] # get any one image randomly\n",
        "    except:\n",
        "      print(\"exception- retireved the train id:{}, ind:{}, in_files:{}\".format(train_id,ind,in_files))\n",
        "\n",
        "    in_fn = os.path.basename(in_path) # get only the full image name\n",
        "\n",
        "    gt_files = glob.glob(self.gt_dir + '%05d_00*.ARW' % train_id) # get the ground truth image path(s) (only 1 may exist. hence select [0]th below)\n",
        "    gt_path = gt_files[0] # get the first one (as only 1 gt image should exist)\n",
        "    gt_fn = os.path.basename(gt_path) # get only the full image name for gt image\n",
        "    in_exposure = float(in_fn[9:-5]) # get the exposure for input image\n",
        "    gt_exposure = float(gt_fn[9:-5]) # get the exposure for gt image\n",
        "    ratio = min(gt_exposure / in_exposure, 300) # get the amplification ratio\n",
        "\n",
        "    start = time.process_time()\n",
        "    #cnt += 1\n",
        "\n",
        "    #if input_images[str(ratio)[0:3]][ind] is None: # if image is not loaded (first epoch), load it\n",
        "    raw = rawpy.imread(in_path)\n",
        "    input_images = np.expand_dims(pack_raw(raw), axis=0) * ratio # pack the bayer image in 4 channels of RGBG\n",
        "    if self.gt_image_map[train_id] is None:\n",
        "      print(\"reading gt image\")\n",
        "      # print(\"dict len now:\",len(self.gt_image_map))\n",
        "      gt_raw = rawpy.imread(gt_path)\n",
        "      im = gt_raw.postprocess(use_camera_wb=True,\n",
        "                    half_size=False,\n",
        "                    no_auto_bright=True, output_bps=16)\n",
        "      gt_images = np.expand_dims(np.float32(im / 65535.0),axis=0) # divide by 65535 to normalise (scale between 0 and 1)\n",
        "      self.gt_image_map[train_id]=gt_images\n",
        "    else:\n",
        "      gt_images = self.gt_image_map[train_id]\n",
        "    # crop\n",
        "#     print(\"time taken for reading image %d:%s with in_path: %s & gt_path:%s\" % (train_id,(time.process_time() - start),in_path,gt_path))\n",
        "\n",
        "    H = input_images.shape[1] # get the image height (number of rows)\n",
        "    W = input_images.shape[2] # get the image width (number of columns)\n",
        "\n",
        "    xx = np.random.randint(0, W - self.ps) # get a random number in W-ps (W-512)\n",
        "    yy = np.random.randint(0, H - self.ps) # get a random number in H-ps (H-512)\n",
        "    input_patch = input_images[:, yy:yy + self.ps, xx:xx + self.ps, :]\n",
        "    gt_patch = gt_images[:, yy * 2:yy * 2 + self.ps * 2, xx * 2:xx * 2 + self.ps * 2, :]\n",
        "\n",
        "    if np.random.randint(2) == 1:  # random flip for rows\n",
        "      input_patch = np.flip(input_patch, axis=1)\n",
        "      gt_patch = np.flip(gt_patch, axis=1)\n",
        "    if np.random.randint(2) == 1:  # random flip for columns\n",
        "      input_patch = np.flip(input_patch, axis=2)\n",
        "      gt_patch = np.flip(gt_patch, axis=2)\n",
        "    if np.random.randint(2) == 1:  # random transpose\n",
        "      input_patch = np.transpose(input_patch, (0, 2, 1, 3))\n",
        "      gt_patch = np.transpose(gt_patch, (0, 2, 1, 3))\\\n",
        "\n",
        "    input_patch = np.minimum(input_patch, 1.0)\n",
        "    # print('generated an image with ind:',ind)\n",
        "\n",
        "    return (input_patch,gt_patch)\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YECuKfSCS7JW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# import datetime\n",
        "# class StoreEpochCallBack(tf.keras.callbacks.Callback):\n",
        "#   def __init__(self, params):\n",
        "#     self.f=open(params.epoch_num_file,\"w\")\n",
        "#   def on_train_batch_begin(self, batch, logs=None):\n",
        "#     print('Training: batch {} begins at {}'.format(batch, datetime.datetime.now().time()))\n",
        "\n",
        "#   def on_train_batch_end(self, batch, logs=None):\n",
        "#     print('Training: batch {} ends at {}'.format(batch, datetime.datetime.now().time()))\n",
        "\n",
        "#   def on_test_batch_begin(self, batch, logs=None):\n",
        "#     print('Evaluating: batch {} begins at {}'.format(batch, datetime.datetime.now().time()))\n",
        "\n",
        "#   def on_test_batch_end(self, batch, logs=None):\n",
        "#     print('Evaluating: batch {} ends at {}'.format(batch, datetime.datetime.now().time()))\n",
        "def learningratescheduler(epoch,lr):\n",
        "  if (epoch<50):\n",
        "    return 0.0005\n",
        "  elif (epoch>=50 and epoch<100):\n",
        "    return 0.0003\n",
        "  elif (epoch>=100 and epoch<1000):\n",
        "    return 0.0002\n",
        "  elif (epoch>=1000 and epoch<2000):\n",
        "    return 0.0001\n",
        "  else:\n",
        "    return 0.00001\n",
        "  \n",
        "\n",
        "def train_generator(model,data_gen,epochs,steps_per_epoch,save_fname,epoch_num_file,save_dir,tensorboard_log_dir):\n",
        "  print(\"Model Training started!!! Jay Yogeshwar!!!\")\n",
        "  last_epoch=0\n",
        "  last_line=\"\"\n",
        "  with open(epoch_num_file, mode='r', buffering=1) as begin_epoch_file:\n",
        "    last_line=list(begin_epoch_file)[-1]\n",
        "  last_epoch=json.loads(last_line)['epoch']\n",
        "  print(\"Epoch where last training ended:\",last_epoch)\n",
        "  if(last_epoch>10):\n",
        "    list_of_files=glob.glob(save_dir+\"*.hdf5\")\n",
        "    latest_file_path = max(list_of_files, key=os.path.getctime)\n",
        "    print (\"Latest saved model path:\",latest_file_path)\n",
        "    model.load_weights(latest_file_path)\n",
        "    last_epoch=int(latest_file_path.split('/')[-1].split('-')[-3])-11\n",
        "    print(\"Starting from epoch:\",(last_epoch+1))\n",
        "  else:\n",
        "    last_epoch=0\n",
        "  epoch_file=open(epoch_num_file, mode='a', buffering=1)\n",
        "  epoch_callback = keras.callbacks.LambdaCallback(\n",
        "    on_epoch_end=lambda epoch, logs: epoch_file.write(\n",
        "        json.dumps({'epoch': (epoch+1), 'loss': logs['loss'], 'datetime': (dt.datetime.now().strftime('%d%m%Y-%H%M%S'))}) + '\\n'),\n",
        "    on_train_end=lambda logs: epoch_file.close()\n",
        "  )\n",
        "  callbacks = [\n",
        "    epoch_callback,\n",
        "    ModelCheckpoint(period=1,filepath=save_fname, mode='min', monitor='loss', save_weights_only=True, verbose=1, save_best_only=True),\n",
        "#     TensorBoard(log_dir=tensorboard_log_dir),\n",
        "    LearningRateScheduler(learningratescheduler,verbose=1)\n",
        "  ]\n",
        "  model_history=model.fit_generator(\n",
        "      initial_epoch=last_epoch,\n",
        "      generator=data_gen,\n",
        "      steps_per_epoch=steps_per_epoch,\n",
        "      epochs=epochs,\n",
        "      callbacks=callbacks,\n",
        "      max_queue_size=5,\n",
        "      workers=2,\n",
        "      use_multiprocessing=True\n",
        "  )\n",
        "  print(\"Model Training completed!!! Jay Yogeshwar!!!\")\n",
        "  return model_history"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vvdY4QQSpiKq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras import backend as K\n",
        "\n",
        "def recall(y_true, y_pred):\n",
        "  true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "  possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
        "  recall = true_positives / (possible_positives + K.epsilon())\n",
        "  return recall\n",
        "\n",
        "def precision(y_true, y_pred):\n",
        "  true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "  predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
        "  precision = true_positives / (predicted_positives + K.epsilon())\n",
        "  return precision\n",
        "\n",
        "def reduced_mean(y_true,y_pred):\n",
        "  return tf.reduce_mean(tf.abs(y_pred - y_true))\n",
        "\n",
        "\n",
        "params = Params()\n",
        "data = Data(params)\n",
        "dataloader = DataLoader(params,data)\n",
        "model=build_model()\n",
        "with open('model_summary.txt','w') as fh:\n",
        "    # Pass the file handle in as a lambda function to make it callable\n",
        "    model.summary(print_fn=lambda x: fh.write(x + '\\n'))\n",
        "keras.utils.vis_utils.plot_model(model, to_file='model_plot.png', show_shapes=True, show_layer_names=True)\n",
        "model.compile(optimizer=keras.optimizers.Adam(params.learning_rate),loss=reduced_mean,metrics=['accuracy'])\n",
        "steps_per_epoch=len(data.train_ids)\n",
        "# save_fname = os.path.join(params.save_dir, '%s-e%s.h5' % (dt.datetime.now().strftime('%d%m%Y-%H%M%S'), str(epochs)))\n",
        "save_fname = os.path.join(params.save_dir, '%s-weights-e-{epoch:02d}-l-{loss:.7f}.hdf5' % (dt.datetime.now().strftime('%d%m%Y-%H%M%S')))\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UoSXvhsPndeE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dataloader.loadImages()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mJ8IR20yraX8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "16315b50-f9a6-487a-bae6-04a44510d621"
      },
      "source": [
        "\"\"\" ================ TRAIN THE MODEL ================ \"\"\"\n",
        "# steps_per_epoch=10\n",
        "model_history=train_generator(model,dataloader,params.epochs,steps_per_epoch,save_fname,params.epoch_file,params.save_dir,params.tensorboard_log_dir)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model Training started!!! Jay Yogeshwar!!!\n",
            "Epoch where last training ended: 3552\n",
            "Latest saved model path: /content/drive/My Drive/Data/SeeInDarkDataset/dataset/Sony/save_Sony/12082019-024015-weights-e-3485-l-0.0221652.hdf5\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W0812 08:10:10.988903 140510435506048 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "W0812 08:10:10.990388 140510435506048 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:181: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Starting from epoch: 3425\n",
            "Epoch 3425/4000\n",
            "\n",
            "Epoch 03425: LearningRateScheduler setting learning rate to 1e-05.\n",
            "161/161 [==============================] - 142s 881ms/step - loss: 0.0476 - acc: 0.7520\n",
            "\n",
            "Epoch 03425: loss improved from inf to 0.04765, saving model to /content/drive/My Drive/Data/SeeInDarkDataset/dataset/Sony/save_Sony/12082019-080215-weights-e-3425-l-0.0476487.hdf5\n",
            "Epoch 3426/4000\n",
            "\n",
            "Epoch 03426: LearningRateScheduler setting learning rate to 1e-05.\n",
            "161/161 [==============================] - 73s 456ms/step - loss: 0.0351 - acc: 0.7468\n",
            "\n",
            "Epoch 03426: loss improved from 0.04765 to 0.03506, saving model to /content/drive/My Drive/Data/SeeInDarkDataset/dataset/Sony/save_Sony/12082019-080215-weights-e-3426-l-0.0350645.hdf5\n",
            "Epoch 3427/4000\n",
            "\n",
            "Epoch 03427: LearningRateScheduler setting learning rate to 1e-05.\n",
            "161/161 [==============================] - 56s 349ms/step - loss: 0.0314 - acc: 0.7596\n",
            "\n",
            "Epoch 03427: loss improved from 0.03506 to 0.03141, saving model to /content/drive/My Drive/Data/SeeInDarkDataset/dataset/Sony/save_Sony/12082019-080215-weights-e-3427-l-0.0314115.hdf5\n",
            "Epoch 3428/4000\n",
            "\n",
            "Epoch 03428: LearningRateScheduler setting learning rate to 1e-05.\n",
            "161/161 [==============================] - 53s 327ms/step - loss: 0.0296 - acc: 0.7555\n",
            "\n",
            "Epoch 03428: loss improved from 0.03141 to 0.02962, saving model to /content/drive/My Drive/Data/SeeInDarkDataset/dataset/Sony/save_Sony/12082019-080215-weights-e-3428-l-0.0296213.hdf5\n",
            "Epoch 3429/4000\n",
            "\n",
            "Epoch 03429: LearningRateScheduler setting learning rate to 1e-05.\n",
            "161/161 [==============================] - 48s 295ms/step - loss: 0.0296 - acc: 0.7512\n",
            "\n",
            "Epoch 03429: loss improved from 0.02962 to 0.02962, saving model to /content/drive/My Drive/Data/SeeInDarkDataset/dataset/Sony/save_Sony/12082019-080215-weights-e-3429-l-0.0296213.hdf5\n",
            "Epoch 3430/4000\n",
            "\n",
            "Epoch 03430: LearningRateScheduler setting learning rate to 1e-05.\n",
            "161/161 [==============================] - 47s 295ms/step - loss: 0.0293 - acc: 0.7682\n",
            "\n",
            "Epoch 03430: loss improved from 0.02962 to 0.02932, saving model to /content/drive/My Drive/Data/SeeInDarkDataset/dataset/Sony/save_Sony/12082019-080215-weights-e-3430-l-0.0293243.hdf5\n",
            "Epoch 3431/4000\n",
            "\n",
            "Epoch 03431: LearningRateScheduler setting learning rate to 1e-05.\n",
            "161/161 [==============================] - 45s 278ms/step - loss: 0.0289 - acc: 0.7808\n",
            "\n",
            "Epoch 03431: loss improved from 0.02932 to 0.02886, saving model to /content/drive/My Drive/Data/SeeInDarkDataset/dataset/Sony/save_Sony/12082019-080215-weights-e-3431-l-0.0288612.hdf5\n",
            "Epoch 3432/4000\n",
            "\n",
            "Epoch 03432: LearningRateScheduler setting learning rate to 1e-05.\n",
            "161/161 [==============================] - 44s 275ms/step - loss: 0.0275 - acc: 0.7769\n",
            "\n",
            "Epoch 03432: loss improved from 0.02886 to 0.02755, saving model to /content/drive/My Drive/Data/SeeInDarkDataset/dataset/Sony/save_Sony/12082019-080215-weights-e-3432-l-0.0275460.hdf5\n",
            "Epoch 3433/4000\n",
            "\n",
            "Epoch 03433: LearningRateScheduler setting learning rate to 1e-05.\n",
            "161/161 [==============================] - 45s 278ms/step - loss: 0.0272 - acc: 0.7859\n",
            "\n",
            "Epoch 03433: loss improved from 0.02755 to 0.02725, saving model to /content/drive/My Drive/Data/SeeInDarkDataset/dataset/Sony/save_Sony/12082019-080215-weights-e-3433-l-0.0272482.hdf5\n",
            "Epoch 3434/4000\n",
            "\n",
            "Epoch 03434: LearningRateScheduler setting learning rate to 1e-05.\n",
            "161/161 [==============================] - 43s 267ms/step - loss: 0.0276 - acc: 0.7899\n",
            "\n",
            "Epoch 03434: loss did not improve from 0.02725\n",
            "Epoch 3435/4000\n",
            "\n",
            "Epoch 03435: LearningRateScheduler setting learning rate to 1e-05.\n",
            "161/161 [==============================] - 44s 276ms/step - loss: 0.0270 - acc: 0.7718\n",
            "\n",
            "Epoch 03435: loss improved from 0.02725 to 0.02702, saving model to /content/drive/My Drive/Data/SeeInDarkDataset/dataset/Sony/save_Sony/12082019-080215-weights-e-3435-l-0.0270248.hdf5\n",
            "Epoch 3436/4000\n",
            "\n",
            "Epoch 03436: LearningRateScheduler setting learning rate to 1e-05.\n",
            "161/161 [==============================] - 43s 270ms/step - loss: 0.0263 - acc: 0.7730\n",
            "\n",
            "Epoch 03436: loss improved from 0.02702 to 0.02630, saving model to /content/drive/My Drive/Data/SeeInDarkDataset/dataset/Sony/save_Sony/12082019-080215-weights-e-3436-l-0.0263044.hdf5\n",
            "Epoch 3437/4000\n",
            "\n",
            "Epoch 03437: LearningRateScheduler setting learning rate to 1e-05.\n",
            "161/161 [==============================] - 43s 268ms/step - loss: 0.0263 - acc: 0.7878\n",
            "\n",
            "Epoch 03437: loss did not improve from 0.02630\n",
            "Epoch 3438/4000\n",
            "\n",
            "Epoch 03438: LearningRateScheduler setting learning rate to 1e-05.\n",
            "161/161 [==============================] - 43s 267ms/step - loss: 0.0264 - acc: 0.7885\n",
            "\n",
            "Epoch 03438: loss did not improve from 0.02630\n",
            "Epoch 3439/4000\n",
            "\n",
            "Epoch 03439: LearningRateScheduler setting learning rate to 1e-05.\n",
            "161/161 [==============================] - 44s 276ms/step - loss: 0.0257 - acc: 0.7836\n",
            "\n",
            "Epoch 03439: loss improved from 0.02630 to 0.02571, saving model to /content/drive/My Drive/Data/SeeInDarkDataset/dataset/Sony/save_Sony/12082019-080215-weights-e-3439-l-0.0257147.hdf5\n",
            "Epoch 3440/4000\n",
            "\n",
            "Epoch 03440: LearningRateScheduler setting learning rate to 1e-05.\n",
            "161/161 [==============================] - 44s 271ms/step - loss: 0.0254 - acc: 0.7887\n",
            "\n",
            "Epoch 03440: loss improved from 0.02571 to 0.02543, saving model to /content/drive/My Drive/Data/SeeInDarkDataset/dataset/Sony/save_Sony/12082019-080215-weights-e-3440-l-0.0254269.hdf5\n",
            "Epoch 3441/4000\n",
            "\n",
            "Epoch 03441: LearningRateScheduler setting learning rate to 1e-05.\n",
            "161/161 [==============================] - 44s 272ms/step - loss: 0.0263 - acc: 0.7955\n",
            "\n",
            "Epoch 03441: loss did not improve from 0.02543\n",
            "Epoch 3442/4000\n",
            "\n",
            "Epoch 03442: LearningRateScheduler setting learning rate to 1e-05.\n",
            "161/161 [==============================] - 44s 273ms/step - loss: 0.0254 - acc: 0.7770\n",
            "\n",
            "Epoch 03442: loss improved from 0.02543 to 0.02536, saving model to /content/drive/My Drive/Data/SeeInDarkDataset/dataset/Sony/save_Sony/12082019-080215-weights-e-3442-l-0.0253610.hdf5\n",
            "Epoch 3443/4000\n",
            "\n",
            "Epoch 03443: LearningRateScheduler setting learning rate to 1e-05.\n",
            "161/161 [==============================] - 43s 269ms/step - loss: 0.0258 - acc: 0.7768\n",
            "\n",
            "Epoch 03443: loss did not improve from 0.02536\n",
            "Epoch 3444/4000\n",
            "\n",
            "Epoch 03444: LearningRateScheduler setting learning rate to 1e-05.\n",
            "161/161 [==============================] - 44s 272ms/step - loss: 0.0251 - acc: 0.7877\n",
            "\n",
            "Epoch 03444: loss improved from 0.02536 to 0.02507, saving model to /content/drive/My Drive/Data/SeeInDarkDataset/dataset/Sony/save_Sony/12082019-080215-weights-e-3444-l-0.0250750.hdf5\n",
            "Epoch 3445/4000\n",
            "\n",
            "Epoch 03445: LearningRateScheduler setting learning rate to 1e-05.\n",
            "161/161 [==============================] - 44s 273ms/step - loss: 0.0250 - acc: 0.7739\n",
            "\n",
            "Epoch 03445: loss improved from 0.02507 to 0.02499, saving model to /content/drive/My Drive/Data/SeeInDarkDataset/dataset/Sony/save_Sony/12082019-080215-weights-e-3445-l-0.0249877.hdf5\n",
            "Epoch 3446/4000\n",
            "\n",
            "Epoch 03446: LearningRateScheduler setting learning rate to 1e-05.\n",
            "161/161 [==============================] - 44s 272ms/step - loss: 0.0256 - acc: 0.7819\n",
            "\n",
            "Epoch 03446: loss did not improve from 0.02499\n",
            "Epoch 3447/4000\n",
            "\n",
            "Epoch 03447: LearningRateScheduler setting learning rate to 1e-05.\n",
            "161/161 [==============================] - 43s 264ms/step - loss: 0.0254 - acc: 0.7871\n",
            "\n",
            "Epoch 03447: loss did not improve from 0.02499\n",
            "Epoch 3448/4000\n",
            "\n",
            "Epoch 03448: LearningRateScheduler setting learning rate to 1e-05.\n",
            "161/161 [==============================] - 44s 275ms/step - loss: 0.0247 - acc: 0.7734\n",
            "\n",
            "Epoch 03448: loss improved from 0.02499 to 0.02467, saving model to /content/drive/My Drive/Data/SeeInDarkDataset/dataset/Sony/save_Sony/12082019-080215-weights-e-3448-l-0.0246665.hdf5\n",
            "Epoch 3449/4000\n",
            "\n",
            "Epoch 03449: LearningRateScheduler setting learning rate to 1e-05.\n",
            "161/161 [==============================] - 43s 269ms/step - loss: 0.0251 - acc: 0.7858\n",
            "\n",
            "Epoch 03449: loss did not improve from 0.02467\n",
            "Epoch 3450/4000\n",
            "\n",
            "Epoch 03450: LearningRateScheduler setting learning rate to 1e-05.\n",
            "161/161 [==============================] - 44s 276ms/step - loss: 0.0245 - acc: 0.7891\n",
            "\n",
            "Epoch 03450: loss improved from 0.02467 to 0.02445, saving model to /content/drive/My Drive/Data/SeeInDarkDataset/dataset/Sony/save_Sony/12082019-080215-weights-e-3450-l-0.0244514.hdf5\n",
            "Epoch 3451/4000\n",
            "\n",
            "Epoch 03451: LearningRateScheduler setting learning rate to 1e-05.\n",
            "161/161 [==============================] - 44s 272ms/step - loss: 0.0249 - acc: 0.7894\n",
            "\n",
            "Epoch 03451: loss did not improve from 0.02445\n",
            "Epoch 3452/4000\n",
            "\n",
            "Epoch 03452: LearningRateScheduler setting learning rate to 1e-05.\n",
            "161/161 [==============================] - 43s 267ms/step - loss: 0.0252 - acc: 0.8011\n",
            "\n",
            "Epoch 03452: loss did not improve from 0.02445\n",
            "Epoch 3453/4000\n",
            "\n",
            "Epoch 03453: LearningRateScheduler setting learning rate to 1e-05.\n",
            "161/161 [==============================] - 44s 272ms/step - loss: 0.0251 - acc: 0.7885\n",
            "\n",
            "Epoch 03453: loss did not improve from 0.02445\n",
            "Epoch 3454/4000\n",
            "\n",
            "Epoch 03454: LearningRateScheduler setting learning rate to 1e-05.\n",
            "161/161 [==============================] - 43s 268ms/step - loss: 0.0241 - acc: 0.7816\n",
            "\n",
            "Epoch 03454: loss improved from 0.02445 to 0.02411, saving model to /content/drive/My Drive/Data/SeeInDarkDataset/dataset/Sony/save_Sony/12082019-080215-weights-e-3454-l-0.0241051.hdf5\n",
            "Epoch 3455/4000\n",
            "\n",
            "Epoch 03455: LearningRateScheduler setting learning rate to 1e-05.\n",
            "161/161 [==============================] - 42s 261ms/step - loss: 0.0243 - acc: 0.7959\n",
            "\n",
            "Epoch 03455: loss did not improve from 0.02411\n",
            "Epoch 3456/4000\n",
            "\n",
            "Epoch 03456: LearningRateScheduler setting learning rate to 1e-05.\n",
            "161/161 [==============================] - 42s 262ms/step - loss: 0.0245 - acc: 0.7862\n",
            "\n",
            "Epoch 03456: loss did not improve from 0.02411\n",
            "Epoch 3457/4000\n",
            "\n",
            "Epoch 03457: LearningRateScheduler setting learning rate to 1e-05.\n",
            "161/161 [==============================] - 44s 272ms/step - loss: 0.0247 - acc: 0.7970\n",
            "\n",
            "Epoch 03457: loss did not improve from 0.02411\n",
            "Epoch 3458/4000\n",
            "\n",
            "Epoch 03458: LearningRateScheduler setting learning rate to 1e-05.\n",
            "161/161 [==============================] - 43s 265ms/step - loss: 0.0248 - acc: 0.7906\n",
            "\n",
            "Epoch 03458: loss did not improve from 0.02411\n",
            "Epoch 3459/4000\n",
            "\n",
            "Epoch 03459: LearningRateScheduler setting learning rate to 1e-05.\n",
            "161/161 [==============================] - 43s 269ms/step - loss: 0.0244 - acc: 0.7848\n",
            "\n",
            "Epoch 03459: loss did not improve from 0.02411\n",
            "Epoch 3460/4000\n",
            "\n",
            "Epoch 03460: LearningRateScheduler setting learning rate to 1e-05.\n",
            "161/161 [==============================] - 43s 266ms/step - loss: 0.0249 - acc: 0.7858\n",
            "\n",
            "Epoch 03460: loss did not improve from 0.02411\n",
            "Epoch 3461/4000\n",
            "\n",
            "Epoch 03461: LearningRateScheduler setting learning rate to 1e-05.\n",
            "161/161 [==============================] - 43s 267ms/step - loss: 0.0248 - acc: 0.7865\n",
            "\n",
            "Epoch 03461: loss did not improve from 0.02411\n",
            "Epoch 3462/4000\n",
            "\n",
            "Epoch 03462: LearningRateScheduler setting learning rate to 1e-05.\n",
            "161/161 [==============================] - 44s 272ms/step - loss: 0.0247 - acc: 0.7852\n",
            "\n",
            "Epoch 03462: loss did not improve from 0.02411\n",
            "Epoch 3463/4000\n",
            "\n",
            "Epoch 03463: LearningRateScheduler setting learning rate to 1e-05.\n",
            "161/161 [==============================] - 43s 266ms/step - loss: 0.0249 - acc: 0.7920\n",
            "\n",
            "Epoch 03463: loss did not improve from 0.02411\n",
            "Epoch 3464/4000\n",
            "\n",
            "Epoch 03464: LearningRateScheduler setting learning rate to 1e-05.\n",
            "161/161 [==============================] - 43s 264ms/step - loss: 0.0242 - acc: 0.7928\n",
            "\n",
            "Epoch 03464: loss did not improve from 0.02411\n",
            "Epoch 3465/4000\n",
            "\n",
            "Epoch 03465: LearningRateScheduler setting learning rate to 1e-05.\n",
            "161/161 [==============================] - 43s 266ms/step - loss: 0.0250 - acc: 0.8035\n",
            "\n",
            "Epoch 03465: loss did not improve from 0.02411\n",
            "Epoch 3466/4000\n",
            "\n",
            "Epoch 03466: LearningRateScheduler setting learning rate to 1e-05.\n",
            "161/161 [==============================] - 44s 271ms/step - loss: 0.0245 - acc: 0.7860\n",
            "\n",
            "Epoch 03466: loss did not improve from 0.02411\n",
            "Epoch 3467/4000\n",
            "\n",
            "Epoch 03467: LearningRateScheduler setting learning rate to 1e-05.\n",
            "161/161 [==============================] - 43s 270ms/step - loss: 0.0246 - acc: 0.7996\n",
            "\n",
            "Epoch 03467: loss did not improve from 0.02411\n",
            "Epoch 3468/4000\n",
            "\n",
            "Epoch 03468: LearningRateScheduler setting learning rate to 1e-05.\n",
            "161/161 [==============================] - 44s 275ms/step - loss: 0.0238 - acc: 0.7840\n",
            "\n",
            "Epoch 03468: loss improved from 0.02411 to 0.02377, saving model to /content/drive/My Drive/Data/SeeInDarkDataset/dataset/Sony/save_Sony/12082019-080215-weights-e-3468-l-0.0237743.hdf5\n",
            "Epoch 3469/4000\n",
            "\n",
            "Epoch 03469: LearningRateScheduler setting learning rate to 1e-05.\n",
            "161/161 [==============================] - 43s 266ms/step - loss: 0.0241 - acc: 0.7945\n",
            "\n",
            "Epoch 03469: loss did not improve from 0.02377\n",
            "Epoch 3470/4000\n",
            "\n",
            "Epoch 03470: LearningRateScheduler setting learning rate to 1e-05.\n",
            "161/161 [==============================] - 44s 272ms/step - loss: 0.0243 - acc: 0.7975\n",
            "\n",
            "Epoch 03470: loss did not improve from 0.02377\n",
            "Epoch 3471/4000\n",
            "\n",
            "Epoch 03471: LearningRateScheduler setting learning rate to 1e-05.\n",
            "161/161 [==============================] - 43s 267ms/step - loss: 0.0242 - acc: 0.7856\n",
            "\n",
            "Epoch 03471: loss did not improve from 0.02377\n",
            "Epoch 3472/4000\n",
            "\n",
            "Epoch 03472: LearningRateScheduler setting learning rate to 1e-05.\n",
            "161/161 [==============================] - 43s 269ms/step - loss: 0.0243 - acc: 0.7845\n",
            "\n",
            "Epoch 03472: loss did not improve from 0.02377\n",
            "Epoch 3473/4000\n",
            "\n",
            "Epoch 03473: LearningRateScheduler setting learning rate to 1e-05.\n",
            "161/161 [==============================] - 44s 270ms/step - loss: 0.0245 - acc: 0.7938\n",
            "\n",
            "Epoch 03473: loss did not improve from 0.02377\n",
            "Epoch 3474/4000\n",
            "\n",
            "Epoch 03474: LearningRateScheduler setting learning rate to 1e-05.\n",
            "161/161 [==============================] - 43s 269ms/step - loss: 0.0236 - acc: 0.8012\n",
            "\n",
            "Epoch 03474: loss improved from 0.02377 to 0.02360, saving model to /content/drive/My Drive/Data/SeeInDarkDataset/dataset/Sony/save_Sony/12082019-080215-weights-e-3474-l-0.0236025.hdf5\n",
            "Epoch 3475/4000\n",
            "\n",
            "Epoch 03475: LearningRateScheduler setting learning rate to 1e-05.\n",
            "161/161 [==============================] - 43s 265ms/step - loss: 0.0240 - acc: 0.7966\n",
            "\n",
            "Epoch 03475: loss did not improve from 0.02360\n",
            "Epoch 3476/4000\n",
            "\n",
            "Epoch 03476: LearningRateScheduler setting learning rate to 1e-05.\n",
            "161/161 [==============================] - 44s 275ms/step - loss: 0.0244 - acc: 0.8037\n",
            "\n",
            "Epoch 03476: loss did not improve from 0.02360\n",
            "Epoch 3477/4000\n",
            "\n",
            "Epoch 03477: LearningRateScheduler setting learning rate to 1e-05.\n",
            "161/161 [==============================] - 43s 266ms/step - loss: 0.0244 - acc: 0.8053\n",
            "\n",
            "Epoch 03477: loss did not improve from 0.02360\n",
            "Epoch 3478/4000\n",
            "\n",
            "Epoch 03478: LearningRateScheduler setting learning rate to 1e-05.\n",
            "161/161 [==============================] - 45s 279ms/step - loss: 0.0238 - acc: 0.7965\n",
            "\n",
            "Epoch 03478: loss did not improve from 0.02360\n",
            "Epoch 3479/4000\n",
            "\n",
            "Epoch 03479: LearningRateScheduler setting learning rate to 1e-05.\n",
            "161/161 [==============================] - 43s 268ms/step - loss: 0.0240 - acc: 0.8019\n",
            "\n",
            "Epoch 03479: loss did not improve from 0.02360\n",
            "Epoch 3480/4000\n",
            "\n",
            "Epoch 03480: LearningRateScheduler setting learning rate to 1e-05.\n",
            "161/161 [==============================] - 43s 267ms/step - loss: 0.0238 - acc: 0.8002\n",
            "\n",
            "Epoch 03480: loss did not improve from 0.02360\n",
            "Epoch 3481/4000\n",
            "\n",
            "Epoch 03481: LearningRateScheduler setting learning rate to 1e-05.\n",
            "161/161 [==============================] - 43s 267ms/step - loss: 0.0241 - acc: 0.7876\n",
            "\n",
            "Epoch 03481: loss did not improve from 0.02360\n",
            "Epoch 3482/4000\n",
            "\n",
            "Epoch 03482: LearningRateScheduler setting learning rate to 1e-05.\n",
            "161/161 [==============================] - 43s 269ms/step - loss: 0.0244 - acc: 0.7940\n",
            "\n",
            "Epoch 03482: loss did not improve from 0.02360\n",
            "Epoch 3483/4000\n",
            "\n",
            "Epoch 03483: LearningRateScheduler setting learning rate to 1e-05.\n",
            "161/161 [==============================] - 43s 265ms/step - loss: 0.0239 - acc: 0.8065\n",
            "\n",
            "Epoch 03483: loss did not improve from 0.02360\n",
            "Epoch 3484/4000\n",
            "\n",
            "Epoch 03484: LearningRateScheduler setting learning rate to 1e-05.\n",
            "161/161 [==============================] - 45s 279ms/step - loss: 0.0243 - acc: 0.7801\n",
            "\n",
            "Epoch 03484: loss did not improve from 0.02360\n",
            "Epoch 3485/4000\n",
            "\n",
            "Epoch 03485: LearningRateScheduler setting learning rate to 1e-05.\n",
            "161/161 [==============================] - 42s 261ms/step - loss: 0.0235 - acc: 0.8031\n",
            "\n",
            "Epoch 03485: loss improved from 0.02360 to 0.02348, saving model to /content/drive/My Drive/Data/SeeInDarkDataset/dataset/Sony/save_Sony/12082019-080215-weights-e-3485-l-0.0234838.hdf5\n",
            "Epoch 3486/4000\n",
            "\n",
            "Epoch 03486: LearningRateScheduler setting learning rate to 1e-05.\n",
            "161/161 [==============================] - 43s 268ms/step - loss: 0.0238 - acc: 0.8010\n",
            "\n",
            "Epoch 03486: loss did not improve from 0.02348\n",
            "Epoch 3487/4000\n",
            "\n",
            "Epoch 03487: LearningRateScheduler setting learning rate to 1e-05.\n",
            "161/161 [==============================] - 44s 271ms/step - loss: 0.0252 - acc: 0.7777\n",
            "\n",
            "Epoch 03487: loss did not improve from 0.02348\n",
            "Epoch 3488/4000\n",
            "\n",
            "Epoch 03488: LearningRateScheduler setting learning rate to 1e-05.\n",
            "161/161 [==============================] - 43s 269ms/step - loss: 0.0244 - acc: 0.7818\n",
            "\n",
            "Epoch 03488: loss did not improve from 0.02348\n",
            "Epoch 3489/4000\n",
            "\n",
            "Epoch 03489: LearningRateScheduler setting learning rate to 1e-05.\n",
            "161/161 [==============================] - 43s 270ms/step - loss: 0.0241 - acc: 0.7908\n",
            "\n",
            "Epoch 03489: loss did not improve from 0.02348\n",
            "Epoch 3490/4000\n",
            "\n",
            "Epoch 03490: LearningRateScheduler setting learning rate to 1e-05.\n",
            "161/161 [==============================] - 43s 269ms/step - loss: 0.0248 - acc: 0.7858\n",
            "\n",
            "Epoch 03490: loss did not improve from 0.02348\n",
            "Epoch 3491/4000\n",
            "\n",
            "Epoch 03491: LearningRateScheduler setting learning rate to 1e-05.\n",
            "161/161 [==============================] - 44s 275ms/step - loss: 0.0244 - acc: 0.7811\n",
            "\n",
            "Epoch 03491: loss did not improve from 0.02348\n",
            "Epoch 3492/4000\n",
            "\n",
            "Epoch 03492: LearningRateScheduler setting learning rate to 1e-05.\n",
            "161/161 [==============================] - 44s 272ms/step - loss: 0.0238 - acc: 0.7912\n",
            "\n",
            "Epoch 03492: loss did not improve from 0.02348\n",
            "Epoch 3493/4000\n",
            "\n",
            "Epoch 03493: LearningRateScheduler setting learning rate to 1e-05.\n",
            "161/161 [==============================] - 43s 269ms/step - loss: 0.0235 - acc: 0.7870\n",
            "\n",
            "Epoch 03493: loss improved from 0.02348 to 0.02345, saving model to /content/drive/My Drive/Data/SeeInDarkDataset/dataset/Sony/save_Sony/12082019-080215-weights-e-3493-l-0.0234540.hdf5\n",
            "Epoch 3494/4000\n",
            "\n",
            "Epoch 03494: LearningRateScheduler setting learning rate to 1e-05.\n",
            "161/161 [==============================] - 44s 273ms/step - loss: 0.0236 - acc: 0.7875\n",
            "\n",
            "Epoch 03494: loss did not improve from 0.02345\n",
            "Epoch 3495/4000\n",
            "\n",
            "Epoch 03495: LearningRateScheduler setting learning rate to 1e-05.\n",
            "161/161 [==============================] - 43s 270ms/step - loss: 0.0238 - acc: 0.7790\n",
            "\n",
            "Epoch 03495: loss did not improve from 0.02345\n",
            "Epoch 3496/4000\n",
            "\n",
            "Epoch 03496: LearningRateScheduler setting learning rate to 1e-05.\n",
            "161/161 [==============================] - 44s 273ms/step - loss: 0.0238 - acc: 0.7897\n",
            "\n",
            "Epoch 03496: loss did not improve from 0.02345\n",
            "Epoch 3497/4000\n",
            "\n",
            "Epoch 03497: LearningRateScheduler setting learning rate to 1e-05.\n",
            "161/161 [==============================] - 44s 271ms/step - loss: 0.0236 - acc: 0.7918\n",
            "\n",
            "Epoch 03497: loss did not improve from 0.02345\n",
            "Epoch 3498/4000\n",
            "\n",
            "Epoch 03498: LearningRateScheduler setting learning rate to 1e-05.\n",
            "161/161 [==============================] - 42s 263ms/step - loss: 0.0239 - acc: 0.7986\n",
            "\n",
            "Epoch 03498: loss did not improve from 0.02345\n",
            "Epoch 3499/4000\n",
            "\n",
            "Epoch 03499: LearningRateScheduler setting learning rate to 1e-05.\n",
            "161/161 [==============================] - 43s 264ms/step - loss: 0.0243 - acc: 0.7994\n",
            "\n",
            "Epoch 03499: loss did not improve from 0.02345\n",
            "Epoch 3500/4000\n",
            "\n",
            "Epoch 03500: LearningRateScheduler setting learning rate to 1e-05.\n",
            "161/161 [==============================] - 44s 275ms/step - loss: 0.0241 - acc: 0.8082\n",
            "\n",
            "Epoch 03500: loss did not improve from 0.02345\n",
            "Epoch 3501/4000\n",
            "\n",
            "Epoch 03501: LearningRateScheduler setting learning rate to 1e-05.\n",
            "161/161 [==============================] - 43s 269ms/step - loss: 0.0240 - acc: 0.7830\n",
            "\n",
            "Epoch 03501: loss did not improve from 0.02345\n",
            "Epoch 3502/4000\n",
            "\n",
            "Epoch 03502: LearningRateScheduler setting learning rate to 1e-05.\n",
            "161/161 [==============================] - 43s 266ms/step - loss: 0.0243 - acc: 0.7835\n",
            "\n",
            "Epoch 03502: loss did not improve from 0.02345\n",
            "Epoch 3503/4000\n",
            "\n",
            "Epoch 03503: LearningRateScheduler setting learning rate to 1e-05.\n",
            "161/161 [==============================] - 43s 266ms/step - loss: 0.0237 - acc: 0.8028\n",
            "\n",
            "Epoch 03503: loss did not improve from 0.02345\n",
            "Epoch 3504/4000\n",
            "\n",
            "Epoch 03504: LearningRateScheduler setting learning rate to 1e-05.\n",
            "161/161 [==============================] - 42s 261ms/step - loss: 0.0242 - acc: 0.7868\n",
            "\n",
            "Epoch 03504: loss did not improve from 0.02345\n",
            "Epoch 3505/4000\n",
            "\n",
            "Epoch 03505: LearningRateScheduler setting learning rate to 1e-05.\n",
            "161/161 [==============================] - 44s 275ms/step - loss: 0.0243 - acc: 0.7783\n",
            "\n",
            "Epoch 03505: loss did not improve from 0.02345\n",
            "Epoch 3506/4000\n",
            "\n",
            "Epoch 03506: LearningRateScheduler setting learning rate to 1e-05.\n",
            "161/161 [==============================] - 44s 273ms/step - loss: 0.0242 - acc: 0.7941\n",
            "\n",
            "Epoch 03506: loss did not improve from 0.02345\n",
            "Epoch 3507/4000\n",
            "\n",
            "Epoch 03507: LearningRateScheduler setting learning rate to 1e-05.\n",
            "161/161 [==============================] - 42s 263ms/step - loss: 0.0240 - acc: 0.7883\n",
            "\n",
            "Epoch 03507: loss did not improve from 0.02345\n",
            "Epoch 3508/4000\n",
            "\n",
            "Epoch 03508: LearningRateScheduler setting learning rate to 1e-05.\n",
            "161/161 [==============================] - 43s 267ms/step - loss: 0.0237 - acc: 0.8079\n",
            "\n",
            "Epoch 03508: loss did not improve from 0.02345\n",
            "Epoch 3509/4000\n",
            "\n",
            "Epoch 03509: LearningRateScheduler setting learning rate to 1e-05.\n",
            "161/161 [==============================] - 42s 261ms/step - loss: 0.0237 - acc: 0.8015\n",
            "\n",
            "Epoch 03509: loss did not improve from 0.02345\n",
            "Epoch 3510/4000\n",
            "\n",
            "Epoch 03510: LearningRateScheduler setting learning rate to 1e-05.\n",
            "161/161 [==============================] - 42s 263ms/step - loss: 0.0238 - acc: 0.7906\n",
            "\n",
            "Epoch 03510: loss did not improve from 0.02345\n",
            "Epoch 3511/4000\n",
            "\n",
            "Epoch 03511: LearningRateScheduler setting learning rate to 1e-05.\n",
            "161/161 [==============================] - 42s 264ms/step - loss: 0.0238 - acc: 0.7979\n",
            "\n",
            "Epoch 03511: loss did not improve from 0.02345\n",
            "Epoch 3512/4000\n",
            "\n",
            "Epoch 03512: LearningRateScheduler setting learning rate to 1e-05.\n",
            "161/161 [==============================] - 42s 263ms/step - loss: 0.0238 - acc: 0.7909\n",
            "\n",
            "Epoch 03512: loss did not improve from 0.02345\n",
            "Epoch 3513/4000\n",
            "\n",
            "Epoch 03513: LearningRateScheduler setting learning rate to 1e-05.\n",
            "161/161 [==============================] - 42s 259ms/step - loss: 0.0243 - acc: 0.7761\n",
            "\n",
            "Epoch 03513: loss did not improve from 0.02345\n",
            "Epoch 3514/4000\n",
            "\n",
            "Epoch 03514: LearningRateScheduler setting learning rate to 1e-05.\n",
            "161/161 [==============================] - 43s 266ms/step - loss: 0.0241 - acc: 0.7879\n",
            "\n",
            "Epoch 03514: loss did not improve from 0.02345\n",
            "Epoch 3515/4000\n",
            "\n",
            "Epoch 03515: LearningRateScheduler setting learning rate to 1e-05.\n",
            "161/161 [==============================] - 42s 262ms/step - loss: 0.0241 - acc: 0.8104\n",
            "\n",
            "Epoch 03515: loss did not improve from 0.02345\n",
            "Epoch 3516/4000\n",
            "\n",
            "Epoch 03516: LearningRateScheduler setting learning rate to 1e-05.\n",
            "161/161 [==============================] - 42s 258ms/step - loss: 0.0235 - acc: 0.8012\n",
            "\n",
            "Epoch 03516: loss did not improve from 0.02345\n",
            "Epoch 3517/4000\n",
            "\n",
            "Epoch 03517: LearningRateScheduler setting learning rate to 1e-05.\n",
            "161/161 [==============================] - 43s 265ms/step - loss: 0.0242 - acc: 0.7919\n",
            "\n",
            "Epoch 03517: loss did not improve from 0.02345\n",
            "Epoch 3518/4000\n",
            "\n",
            "Epoch 03518: LearningRateScheduler setting learning rate to 1e-05.\n",
            "161/161 [==============================] - 44s 271ms/step - loss: 0.0241 - acc: 0.8008\n",
            "\n",
            "Epoch 03518: loss did not improve from 0.02345\n",
            "Epoch 3519/4000\n",
            "\n",
            "Epoch 03519: LearningRateScheduler setting learning rate to 1e-05.\n",
            "161/161 [==============================] - 43s 265ms/step - loss: 0.0240 - acc: 0.7971\n",
            "\n",
            "Epoch 03519: loss did not improve from 0.02345\n",
            "Epoch 3520/4000\n",
            "\n",
            "Epoch 03520: LearningRateScheduler setting learning rate to 1e-05.\n",
            "161/161 [==============================] - 44s 274ms/step - loss: 0.0237 - acc: 0.7997\n",
            "\n",
            "Epoch 03520: loss did not improve from 0.02345\n",
            "Epoch 3521/4000\n",
            "\n",
            "Epoch 03521: LearningRateScheduler setting learning rate to 1e-05.\n",
            "161/161 [==============================] - 43s 269ms/step - loss: 0.0238 - acc: 0.8068\n",
            "\n",
            "Epoch 03521: loss did not improve from 0.02345\n",
            "Epoch 3522/4000\n",
            "\n",
            "Epoch 03522: LearningRateScheduler setting learning rate to 1e-05.\n",
            "161/161 [==============================] - 43s 269ms/step - loss: 0.0238 - acc: 0.8004\n",
            "\n",
            "Epoch 03522: loss did not improve from 0.02345\n",
            "Epoch 3523/4000\n",
            "\n",
            "Epoch 03523: LearningRateScheduler setting learning rate to 1e-05.\n",
            "161/161 [==============================] - 43s 269ms/step - loss: 0.0234 - acc: 0.8018\n",
            "\n",
            "Epoch 03523: loss improved from 0.02345 to 0.02342, saving model to /content/drive/My Drive/Data/SeeInDarkDataset/dataset/Sony/save_Sony/12082019-080215-weights-e-3523-l-0.0234192.hdf5\n",
            "Epoch 3524/4000\n",
            "\n",
            "Epoch 03524: LearningRateScheduler setting learning rate to 1e-05.\n",
            "161/161 [==============================] - 44s 271ms/step - loss: 0.0239 - acc: 0.7823\n",
            "\n",
            "Epoch 03524: loss did not improve from 0.02342\n",
            "Epoch 3525/4000\n",
            "\n",
            "Epoch 03525: LearningRateScheduler setting learning rate to 1e-05.\n",
            "161/161 [==============================] - 43s 268ms/step - loss: 0.0238 - acc: 0.7910\n",
            "\n",
            "Epoch 03525: loss did not improve from 0.02342\n",
            "Epoch 3526/4000\n",
            "\n",
            "Epoch 03526: LearningRateScheduler setting learning rate to 1e-05.\n",
            "161/161 [==============================] - 44s 272ms/step - loss: 0.0231 - acc: 0.7920\n",
            "\n",
            "Epoch 03526: loss improved from 0.02342 to 0.02312, saving model to /content/drive/My Drive/Data/SeeInDarkDataset/dataset/Sony/save_Sony/12082019-080215-weights-e-3526-l-0.0231198.hdf5\n",
            "Epoch 3527/4000\n",
            "\n",
            "Epoch 03527: LearningRateScheduler setting learning rate to 1e-05.\n",
            "161/161 [==============================] - 43s 265ms/step - loss: 0.0247 - acc: 0.8016\n",
            "\n",
            "Epoch 03527: loss did not improve from 0.02312\n",
            "Epoch 3528/4000\n",
            "\n",
            "Epoch 03528: LearningRateScheduler setting learning rate to 1e-05.\n",
            "161/161 [==============================] - 42s 263ms/step - loss: 0.0236 - acc: 0.7941\n",
            "\n",
            "Epoch 03528: loss did not improve from 0.02312\n",
            "Epoch 3529/4000\n",
            "\n",
            "Epoch 03529: LearningRateScheduler setting learning rate to 1e-05.\n",
            "161/161 [==============================] - 43s 268ms/step - loss: 0.0231 - acc: 0.7927\n",
            "\n",
            "Epoch 03529: loss improved from 0.02312 to 0.02312, saving model to /content/drive/My Drive/Data/SeeInDarkDataset/dataset/Sony/save_Sony/12082019-080215-weights-e-3529-l-0.0231191.hdf5\n",
            "Epoch 3530/4000\n",
            "\n",
            "Epoch 03530: LearningRateScheduler setting learning rate to 1e-05.\n",
            "161/161 [==============================] - 44s 272ms/step - loss: 0.0241 - acc: 0.7781\n",
            "\n",
            "Epoch 03530: loss did not improve from 0.02312\n",
            "Epoch 3531/4000\n",
            "\n",
            "Epoch 03531: LearningRateScheduler setting learning rate to 1e-05.\n",
            "161/161 [==============================] - 43s 266ms/step - loss: 0.0239 - acc: 0.7834\n",
            "\n",
            "Epoch 03531: loss did not improve from 0.02312\n",
            "Epoch 3532/4000\n",
            "\n",
            "Epoch 03532: LearningRateScheduler setting learning rate to 1e-05.\n",
            "161/161 [==============================] - 44s 272ms/step - loss: 0.0236 - acc: 0.7838\n",
            "\n",
            "Epoch 03532: loss did not improve from 0.02312\n",
            "Epoch 3533/4000\n",
            "\n",
            "Epoch 03533: LearningRateScheduler setting learning rate to 1e-05.\n",
            "161/161 [==============================] - 43s 265ms/step - loss: 0.0243 - acc: 0.7884\n",
            "\n",
            "Epoch 03533: loss did not improve from 0.02312\n",
            "Epoch 3534/4000\n",
            "\n",
            "Epoch 03534: LearningRateScheduler setting learning rate to 1e-05.\n",
            "161/161 [==============================] - 43s 265ms/step - loss: 0.0235 - acc: 0.7908\n",
            "\n",
            "Epoch 03534: loss did not improve from 0.02312\n",
            "Epoch 3535/4000\n",
            "\n",
            "Epoch 03535: LearningRateScheduler setting learning rate to 1e-05.\n",
            "161/161 [==============================] - 44s 273ms/step - loss: 0.0242 - acc: 0.8034\n",
            "\n",
            "Epoch 03535: loss did not improve from 0.02312\n",
            "Epoch 3536/4000\n",
            "\n",
            "Epoch 03536: LearningRateScheduler setting learning rate to 1e-05.\n",
            "161/161 [==============================] - 44s 273ms/step - loss: 0.0234 - acc: 0.8025\n",
            "\n",
            "Epoch 03536: loss did not improve from 0.02312\n",
            "Epoch 3537/4000\n",
            "\n",
            "Epoch 03537: LearningRateScheduler setting learning rate to 1e-05.\n",
            "161/161 [==============================] - 45s 278ms/step - loss: 0.0232 - acc: 0.7899\n",
            "\n",
            "Epoch 03537: loss did not improve from 0.02312\n",
            "Epoch 3538/4000\n",
            "\n",
            "Epoch 03538: LearningRateScheduler setting learning rate to 1e-05.\n",
            "161/161 [==============================] - 44s 276ms/step - loss: 0.0242 - acc: 0.7973\n",
            "\n",
            "Epoch 03538: loss did not improve from 0.02312\n",
            "Epoch 3539/4000\n",
            "\n",
            "Epoch 03539: LearningRateScheduler setting learning rate to 1e-05.\n",
            "161/161 [==============================] - 45s 277ms/step - loss: 0.0249 - acc: 0.7912\n",
            "\n",
            "Epoch 03539: loss did not improve from 0.02312\n",
            "Epoch 3540/4000\n",
            "\n",
            "Epoch 03540: LearningRateScheduler setting learning rate to 1e-05.\n",
            "161/161 [==============================] - 44s 275ms/step - loss: 0.0238 - acc: 0.7831\n",
            "\n",
            "Epoch 03540: loss did not improve from 0.02312\n",
            "Epoch 3541/4000\n",
            "\n",
            "Epoch 03541: LearningRateScheduler setting learning rate to 1e-05.\n",
            "161/161 [==============================] - 44s 275ms/step - loss: 0.0240 - acc: 0.7760\n",
            "\n",
            "Epoch 03541: loss did not improve from 0.02312\n",
            "Epoch 3542/4000\n",
            "\n",
            "Epoch 03542: LearningRateScheduler setting learning rate to 1e-05.\n",
            "161/161 [==============================] - 45s 277ms/step - loss: 0.0238 - acc: 0.8009\n",
            "\n",
            "Epoch 03542: loss did not improve from 0.02312\n",
            "Epoch 3543/4000\n",
            "\n",
            "Epoch 03543: LearningRateScheduler setting learning rate to 1e-05.\n",
            "161/161 [==============================] - 45s 282ms/step - loss: 0.0242 - acc: 0.8058\n",
            "\n",
            "Epoch 03543: loss did not improve from 0.02312\n",
            "Epoch 3544/4000\n",
            "\n",
            "Epoch 03544: LearningRateScheduler setting learning rate to 1e-05.\n",
            "161/161 [==============================] - 46s 284ms/step - loss: 0.0237 - acc: 0.7868\n",
            "\n",
            "Epoch 03544: loss did not improve from 0.02312\n",
            "Epoch 3545/4000\n",
            "\n",
            "Epoch 03545: LearningRateScheduler setting learning rate to 1e-05.\n",
            "161/161 [==============================] - 44s 272ms/step - loss: 0.0238 - acc: 0.7962\n",
            "\n",
            "Epoch 03545: loss did not improve from 0.02312\n",
            "Epoch 3546/4000\n",
            "\n",
            "Epoch 03546: LearningRateScheduler setting learning rate to 1e-05.\n",
            "161/161 [==============================] - 44s 274ms/step - loss: 0.0235 - acc: 0.7978\n",
            "\n",
            "Epoch 03546: loss did not improve from 0.02312\n",
            "Epoch 3547/4000\n",
            "\n",
            "Epoch 03547: LearningRateScheduler setting learning rate to 1e-05.\n",
            "161/161 [==============================] - 43s 268ms/step - loss: 0.0233 - acc: 0.7908\n",
            "\n",
            "Epoch 03547: loss did not improve from 0.02312\n",
            "Epoch 3548/4000\n",
            "\n",
            "Epoch 03548: LearningRateScheduler setting learning rate to 1e-05.\n",
            "161/161 [==============================] - 45s 279ms/step - loss: 0.0239 - acc: 0.8183\n",
            "\n",
            "Epoch 03548: loss did not improve from 0.02312\n",
            "Epoch 3549/4000\n",
            "\n",
            "Epoch 03549: LearningRateScheduler setting learning rate to 1e-05.\n",
            "161/161 [==============================] - 45s 279ms/step - loss: 0.0237 - acc: 0.7889\n",
            "\n",
            "Epoch 03549: loss did not improve from 0.02312\n",
            "Epoch 3550/4000\n",
            "\n",
            "Epoch 03550: LearningRateScheduler setting learning rate to 1e-05.\n",
            "161/161 [==============================] - 45s 279ms/step - loss: 0.0239 - acc: 0.7915\n",
            "\n",
            "Epoch 03550: loss did not improve from 0.02312\n",
            "Epoch 3551/4000\n",
            "\n",
            "Epoch 03551: LearningRateScheduler setting learning rate to 1e-05.\n",
            "161/161 [==============================] - 45s 279ms/step - loss: 0.0231 - acc: 0.7965\n",
            "\n",
            "Epoch 03551: loss did not improve from 0.02312\n",
            "Epoch 3552/4000\n",
            "\n",
            "Epoch 03552: LearningRateScheduler setting learning rate to 1e-05.\n",
            "161/161 [==============================] - 45s 276ms/step - loss: 0.0240 - acc: 0.8026\n",
            "\n",
            "Epoch 03552: loss did not improve from 0.02312\n",
            "Epoch 3553/4000\n",
            "\n",
            "Epoch 03553: LearningRateScheduler setting learning rate to 1e-05.\n",
            "161/161 [==============================] - 44s 271ms/step - loss: 0.0238 - acc: 0.7946\n",
            "\n",
            "Epoch 03553: loss did not improve from 0.02312\n",
            "Epoch 3554/4000\n",
            "\n",
            "Epoch 03554: LearningRateScheduler setting learning rate to 1e-05.\n",
            "161/161 [==============================] - 45s 280ms/step - loss: 0.0231 - acc: 0.7952\n",
            "\n",
            "Epoch 03554: loss did not improve from 0.02312\n",
            "Epoch 3555/4000\n",
            "\n",
            "Epoch 03555: LearningRateScheduler setting learning rate to 1e-05.\n",
            "161/161 [==============================] - 44s 273ms/step - loss: 0.0243 - acc: 0.7981\n",
            "\n",
            "Epoch 03555: loss did not improve from 0.02312\n",
            "Epoch 3556/4000\n",
            "\n",
            "Epoch 03556: LearningRateScheduler setting learning rate to 1e-05.\n",
            "161/161 [==============================] - 46s 285ms/step - loss: 0.0232 - acc: 0.8048\n",
            "\n",
            "Epoch 03556: loss did not improve from 0.02312\n",
            "Epoch 3557/4000\n",
            "\n",
            "Epoch 03557: LearningRateScheduler setting learning rate to 1e-05.\n",
            "161/161 [==============================] - 44s 273ms/step - loss: 0.0230 - acc: 0.8076\n",
            "\n",
            "Epoch 03557: loss improved from 0.02312 to 0.02302, saving model to /content/drive/My Drive/Data/SeeInDarkDataset/dataset/Sony/save_Sony/12082019-080215-weights-e-3557-l-0.0230187.hdf5\n",
            "Epoch 3558/4000\n",
            "\n",
            "Epoch 03558: LearningRateScheduler setting learning rate to 1e-05.\n",
            "161/161 [==============================] - 44s 270ms/step - loss: 0.0242 - acc: 0.7925\n",
            "\n",
            "Epoch 03558: loss did not improve from 0.02302\n",
            "Epoch 3559/4000\n",
            "\n",
            "Epoch 03559: LearningRateScheduler setting learning rate to 1e-05.\n",
            "161/161 [==============================] - 44s 271ms/step - loss: 0.0242 - acc: 0.7875\n",
            "\n",
            "Epoch 03559: loss did not improve from 0.02302\n",
            "Epoch 3560/4000\n",
            "\n",
            "Epoch 03560: LearningRateScheduler setting learning rate to 1e-05.\n",
            "161/161 [==============================] - 43s 270ms/step - loss: 0.0232 - acc: 0.7950\n",
            "\n",
            "Epoch 03560: loss did not improve from 0.02302\n",
            "Epoch 3561/4000\n",
            "\n",
            "Epoch 03561: LearningRateScheduler setting learning rate to 1e-05.\n",
            "161/161 [==============================] - 43s 269ms/step - loss: 0.0235 - acc: 0.7855\n",
            "\n",
            "Epoch 03561: loss did not improve from 0.02302\n",
            "Epoch 3562/4000\n",
            "\n",
            "Epoch 03562: LearningRateScheduler setting learning rate to 1e-05.\n",
            "161/161 [==============================] - 44s 270ms/step - loss: 0.0240 - acc: 0.7929\n",
            "\n",
            "Epoch 03562: loss did not improve from 0.02302\n",
            "Epoch 3563/4000\n",
            "\n",
            "Epoch 03563: LearningRateScheduler setting learning rate to 1e-05.\n",
            "161/161 [==============================] - 44s 272ms/step - loss: 0.0236 - acc: 0.7866\n",
            "\n",
            "Epoch 03563: loss did not improve from 0.02302\n",
            "Epoch 3564/4000\n",
            "\n",
            "Epoch 03564: LearningRateScheduler setting learning rate to 1e-05.\n",
            "161/161 [==============================] - 44s 273ms/step - loss: 0.0232 - acc: 0.8073\n",
            "\n",
            "Epoch 03564: loss did not improve from 0.02302\n",
            "Epoch 3565/4000\n",
            "\n",
            "Epoch 03565: LearningRateScheduler setting learning rate to 1e-05.\n",
            "161/161 [==============================] - 45s 279ms/step - loss: 0.0237 - acc: 0.7787\n",
            "\n",
            "Epoch 03565: loss did not improve from 0.02302\n",
            "Epoch 3566/4000\n",
            "\n",
            "Epoch 03566: LearningRateScheduler setting learning rate to 1e-05.\n",
            "161/161 [==============================] - 43s 270ms/step - loss: 0.0236 - acc: 0.8012\n",
            "\n",
            "Epoch 03566: loss did not improve from 0.02302\n",
            "Epoch 3567/4000\n",
            "\n",
            "Epoch 03567: LearningRateScheduler setting learning rate to 1e-05.\n",
            "161/161 [==============================] - 44s 272ms/step - loss: 0.0230 - acc: 0.8000\n",
            "\n",
            "Epoch 03567: loss improved from 0.02302 to 0.02299, saving model to /content/drive/My Drive/Data/SeeInDarkDataset/dataset/Sony/save_Sony/12082019-080215-weights-e-3567-l-0.0229901.hdf5\n",
            "Epoch 3568/4000\n",
            "\n",
            "Epoch 03568: LearningRateScheduler setting learning rate to 1e-05.\n",
            "161/161 [==============================] - 43s 267ms/step - loss: 0.0234 - acc: 0.8002\n",
            "\n",
            "Epoch 03568: loss did not improve from 0.02299\n",
            "Epoch 3569/4000\n",
            "\n",
            "Epoch 03569: LearningRateScheduler setting learning rate to 1e-05.\n",
            "161/161 [==============================] - 45s 281ms/step - loss: 0.0232 - acc: 0.7978\n",
            "\n",
            "Epoch 03569: loss did not improve from 0.02299\n",
            "Epoch 3570/4000\n",
            "\n",
            "Epoch 03570: LearningRateScheduler setting learning rate to 1e-05.\n",
            "161/161 [==============================] - 44s 273ms/step - loss: 0.0240 - acc: 0.7902\n",
            "\n",
            "Epoch 03570: loss did not improve from 0.02299\n",
            "Epoch 3571/4000\n",
            "\n",
            "Epoch 03571: LearningRateScheduler setting learning rate to 1e-05.\n",
            "161/161 [==============================] - 44s 271ms/step - loss: 0.0241 - acc: 0.7922\n",
            "\n",
            "Epoch 03571: loss did not improve from 0.02299\n",
            "Epoch 3572/4000\n",
            "\n",
            "Epoch 03572: LearningRateScheduler setting learning rate to 1e-05.\n",
            "161/161 [==============================] - 44s 275ms/step - loss: 0.0234 - acc: 0.8035\n",
            "\n",
            "Epoch 03572: loss did not improve from 0.02299\n",
            "Epoch 3573/4000\n",
            "\n",
            "Epoch 03573: LearningRateScheduler setting learning rate to 1e-05.\n",
            "161/161 [==============================] - 45s 280ms/step - loss: 0.0235 - acc: 0.7911\n",
            "\n",
            "Epoch 03573: loss did not improve from 0.02299\n",
            "Epoch 3574/4000\n",
            "\n",
            "Epoch 03574: LearningRateScheduler setting learning rate to 1e-05.\n",
            "161/161 [==============================] - 44s 275ms/step - loss: 0.0235 - acc: 0.8078\n",
            "\n",
            "Epoch 03574: loss did not improve from 0.02299\n",
            "Epoch 3575/4000\n",
            "\n",
            "Epoch 03575: LearningRateScheduler setting learning rate to 1e-05.\n",
            "161/161 [==============================] - 43s 269ms/step - loss: 0.0240 - acc: 0.7873\n",
            "\n",
            "Epoch 03575: loss did not improve from 0.02299\n",
            "Epoch 3576/4000\n",
            "\n",
            "Epoch 03576: LearningRateScheduler setting learning rate to 1e-05.\n",
            "161/161 [==============================] - 45s 277ms/step - loss: 0.0236 - acc: 0.8162\n",
            "\n",
            "Epoch 03576: loss did not improve from 0.02299\n",
            "Epoch 3577/4000\n",
            "\n",
            "Epoch 03577: LearningRateScheduler setting learning rate to 1e-05.\n",
            "161/161 [==============================] - 44s 273ms/step - loss: 0.0237 - acc: 0.8026\n",
            "\n",
            "Epoch 03577: loss did not improve from 0.02299\n",
            "Epoch 3578/4000\n",
            "\n",
            "Epoch 03578: LearningRateScheduler setting learning rate to 1e-05.\n",
            "161/161 [==============================] - 45s 279ms/step - loss: 0.0236 - acc: 0.8035\n",
            "\n",
            "Epoch 03578: loss did not improve from 0.02299\n",
            "Epoch 3579/4000\n",
            "\n",
            "Epoch 03579: LearningRateScheduler setting learning rate to 1e-05.\n",
            "161/161 [==============================] - 43s 268ms/step - loss: 0.0232 - acc: 0.7964\n",
            "\n",
            "Epoch 03579: loss did not improve from 0.02299\n",
            "Epoch 3580/4000\n",
            "\n",
            "Epoch 03580: LearningRateScheduler setting learning rate to 1e-05.\n",
            "161/161 [==============================] - 46s 286ms/step - loss: 0.0233 - acc: 0.7973\n",
            "\n",
            "Epoch 03580: loss did not improve from 0.02299\n",
            "Epoch 3581/4000\n",
            "\n",
            "Epoch 03581: LearningRateScheduler setting learning rate to 1e-05.\n",
            "161/161 [==============================] - 46s 283ms/step - loss: 0.0233 - acc: 0.8005\n",
            "\n",
            "Epoch 03581: loss did not improve from 0.02299\n",
            "Epoch 3582/4000\n",
            "\n",
            "Epoch 03582: LearningRateScheduler setting learning rate to 1e-05.\n",
            "161/161 [==============================] - 45s 280ms/step - loss: 0.0234 - acc: 0.7955\n",
            "\n",
            "Epoch 03582: loss did not improve from 0.02299\n",
            "Epoch 3583/4000\n",
            "\n",
            "Epoch 03583: LearningRateScheduler setting learning rate to 1e-05.\n",
            "161/161 [==============================] - 45s 277ms/step - loss: 0.0233 - acc: 0.8057\n",
            "\n",
            "Epoch 03583: loss did not improve from 0.02299\n",
            "Epoch 3584/4000\n",
            "\n",
            "Epoch 03584: LearningRateScheduler setting learning rate to 1e-05.\n",
            "161/161 [==============================] - 44s 271ms/step - loss: 0.0235 - acc: 0.7989\n",
            "\n",
            "Epoch 03584: loss did not improve from 0.02299\n",
            "Epoch 3585/4000\n",
            "\n",
            "Epoch 03585: LearningRateScheduler setting learning rate to 1e-05.\n",
            "161/161 [==============================] - 44s 272ms/step - loss: 0.0237 - acc: 0.7917\n",
            "\n",
            "Epoch 03585: loss did not improve from 0.02299\n",
            "Epoch 3586/4000\n",
            "\n",
            "Epoch 03586: LearningRateScheduler setting learning rate to 1e-05.\n",
            "160/161 [============================>.] - ETA: 0s - loss: 0.0235 - acc: 0.8035\n",
            "Epoch 03585: loss did not improve from 0.02299\n",
            "161/161 [==============================] - 43s 267ms/step - loss: 0.0235 - acc: 0.8043\n",
            "\n",
            "Epoch 03586: loss did not improve from 0.02299\n",
            "Epoch 3587/4000\n",
            "\n",
            "Epoch 03587: LearningRateScheduler setting learning rate to 1e-05.\n",
            "161/161 [==============================] - 43s 267ms/step - loss: 0.0232 - acc: 0.8036\n",
            "\n",
            "Epoch 03587: loss did not improve from 0.02299\n",
            "Epoch 3588/4000\n",
            "\n",
            "Epoch 03588: LearningRateScheduler setting learning rate to 1e-05.\n",
            "161/161 [==============================] - 43s 269ms/step - loss: 0.0234 - acc: 0.8010\n",
            "\n",
            "Epoch 03588: loss did not improve from 0.02299\n",
            "Epoch 3589/4000\n",
            "\n",
            "Epoch 03589: LearningRateScheduler setting learning rate to 1e-05.\n",
            "161/161 [==============================] - 43s 265ms/step - loss: 0.0238 - acc: 0.8021\n",
            "\n",
            "Epoch 03589: loss did not improve from 0.02299\n",
            "Epoch 3590/4000\n",
            "\n",
            "Epoch 03590: LearningRateScheduler setting learning rate to 1e-05.\n",
            "161/161 [==============================] - 43s 267ms/step - loss: 0.0236 - acc: 0.7883\n",
            "\n",
            "Epoch 03590: loss did not improve from 0.02299\n",
            "Epoch 3591/4000\n",
            "\n",
            "Epoch 03591: LearningRateScheduler setting learning rate to 1e-05.\n",
            "161/161 [==============================] - 43s 269ms/step - loss: 0.0223 - acc: 0.7951\n",
            "\n",
            "Epoch 03591: loss improved from 0.02299 to 0.02232, saving model to /content/drive/My Drive/Data/SeeInDarkDataset/dataset/Sony/save_Sony/12082019-080215-weights-e-3591-l-0.0223216.hdf5\n",
            "Epoch 3592/4000\n",
            "\n",
            "Epoch 03592: LearningRateScheduler setting learning rate to 1e-05.\n",
            "161/161 [==============================] - 42s 261ms/step - loss: 0.0226 - acc: 0.7999\n",
            "\n",
            "Epoch 03592: loss did not improve from 0.02232\n",
            "Epoch 3593/4000\n",
            "\n",
            "Epoch 03593: LearningRateScheduler setting learning rate to 1e-05.\n",
            "161/161 [==============================] - 44s 274ms/step - loss: 0.0238 - acc: 0.8082\n",
            "\n",
            "Epoch 03593: loss did not improve from 0.02232\n",
            "Epoch 3594/4000\n",
            "\n",
            "Epoch 03594: LearningRateScheduler setting learning rate to 1e-05.\n",
            "161/161 [==============================] - 43s 269ms/step - loss: 0.0233 - acc: 0.8037\n",
            "\n",
            "Epoch 03594: loss did not improve from 0.02232\n",
            "Epoch 3595/4000\n",
            "\n",
            "Epoch 03595: LearningRateScheduler setting learning rate to 1e-05.\n",
            "161/161 [==============================] - 43s 269ms/step - loss: 0.0229 - acc: 0.8041\n",
            "\n",
            "Epoch 03595: loss did not improve from 0.02232\n",
            "Epoch 3596/4000\n",
            "\n",
            "Epoch 03596: LearningRateScheduler setting learning rate to 1e-05.\n",
            "161/161 [==============================] - 43s 266ms/step - loss: 0.0228 - acc: 0.7999\n",
            "\n",
            "Epoch 03596: loss did not improve from 0.02232\n",
            "Epoch 3597/4000\n",
            "\n",
            "Epoch 03597: LearningRateScheduler setting learning rate to 1e-05.\n",
            "161/161 [==============================] - 44s 275ms/step - loss: 0.0233 - acc: 0.8090\n",
            "\n",
            "Epoch 03597: loss did not improve from 0.02232\n",
            "Epoch 3598/4000\n",
            "\n",
            "Epoch 03598: LearningRateScheduler setting learning rate to 1e-05.\n",
            "161/161 [==============================] - 43s 265ms/step - loss: 0.0231 - acc: 0.8021\n",
            "\n",
            "Epoch 03598: loss did not improve from 0.02232\n",
            "Epoch 3599/4000\n",
            "\n",
            "Epoch 03599: LearningRateScheduler setting learning rate to 1e-05.\n",
            "161/161 [==============================] - 43s 268ms/step - loss: 0.0240 - acc: 0.8135\n",
            "\n",
            "Epoch 03599: loss did not improve from 0.02232\n",
            "Epoch 3600/4000\n",
            "\n",
            "Epoch 03600: LearningRateScheduler setting learning rate to 1e-05.\n",
            "161/161 [==============================] - 43s 269ms/step - loss: 0.0233 - acc: 0.7966\n",
            "\n",
            "Epoch 03600: loss did not improve from 0.02232\n",
            "Epoch 3601/4000\n",
            "\n",
            "Epoch 03601: LearningRateScheduler setting learning rate to 1e-05.\n",
            "161/161 [==============================] - 43s 265ms/step - loss: 0.0227 - acc: 0.8029\n",
            "\n",
            "Epoch 03601: loss did not improve from 0.02232\n",
            "Epoch 3602/4000\n",
            "\n",
            "Epoch 03602: LearningRateScheduler setting learning rate to 1e-05.\n",
            "161/161 [==============================] - 43s 267ms/step - loss: 0.0234 - acc: 0.8038\n",
            "\n",
            "Epoch 03602: loss did not improve from 0.02232\n",
            "Epoch 3603/4000\n",
            "\n",
            "Epoch 03603: LearningRateScheduler setting learning rate to 1e-05.\n",
            "161/161 [==============================] - 44s 273ms/step - loss: 0.0234 - acc: 0.7901\n",
            "\n",
            "Epoch 03603: loss did not improve from 0.02232\n",
            "Epoch 3604/4000\n",
            "\n",
            "Epoch 03604: LearningRateScheduler setting learning rate to 1e-05.\n",
            "161/161 [==============================] - 43s 265ms/step - loss: 0.0234 - acc: 0.8093\n",
            "\n",
            "Epoch 03604: loss did not improve from 0.02232\n",
            "Epoch 3605/4000\n",
            "\n",
            "Epoch 03605: LearningRateScheduler setting learning rate to 1e-05.\n",
            "161/161 [==============================] - 44s 274ms/step - loss: 0.0234 - acc: 0.7955\n",
            "\n",
            "Epoch 03605: loss did not improve from 0.02232\n",
            "Epoch 3606/4000\n",
            "\n",
            "Epoch 03606: LearningRateScheduler setting learning rate to 1e-05.\n",
            "161/161 [==============================] - 43s 269ms/step - loss: 0.0230 - acc: 0.8056\n",
            "\n",
            "Epoch 03606: loss did not improve from 0.02232\n",
            "Epoch 3607/4000\n",
            "\n",
            "Epoch 03607: LearningRateScheduler setting learning rate to 1e-05.\n",
            "161/161 [==============================] - 42s 263ms/step - loss: 0.0234 - acc: 0.7901\n",
            "\n",
            "Epoch 03607: loss did not improve from 0.02232\n",
            "Epoch 3608/4000\n",
            "\n",
            "Epoch 03608: LearningRateScheduler setting learning rate to 1e-05.\n",
            "161/161 [==============================] - 43s 266ms/step - loss: 0.0236 - acc: 0.8024\n",
            "\n",
            "Epoch 03608: loss did not improve from 0.02232\n",
            "Epoch 3609/4000\n",
            "\n",
            "Epoch 03609: LearningRateScheduler setting learning rate to 1e-05.\n",
            "161/161 [==============================] - 44s 276ms/step - loss: 0.0237 - acc: 0.7878\n",
            "\n",
            "Epoch 03609: loss did not improve from 0.02232\n",
            "Epoch 3610/4000\n",
            "\n",
            "Epoch 03610: LearningRateScheduler setting learning rate to 1e-05.\n",
            "161/161 [==============================] - 44s 272ms/step - loss: 0.0241 - acc: 0.7940\n",
            "\n",
            "Epoch 03610: loss did not improve from 0.02232\n",
            "Epoch 3611/4000\n",
            "\n",
            "Epoch 03611: LearningRateScheduler setting learning rate to 1e-05.\n",
            "161/161 [==============================] - 44s 272ms/step - loss: 0.0237 - acc: 0.8131\n",
            "\n",
            "Epoch 03611: loss did not improve from 0.02232\n",
            "Epoch 3612/4000\n",
            "\n",
            "Epoch 03612: LearningRateScheduler setting learning rate to 1e-05.\n",
            "161/161 [==============================] - 43s 269ms/step - loss: 0.0237 - acc: 0.7935\n",
            "\n",
            "Epoch 03612: loss did not improve from 0.02232\n",
            "Epoch 3613/4000\n",
            "\n",
            "Epoch 03613: LearningRateScheduler setting learning rate to 1e-05.\n",
            "161/161 [==============================] - 44s 275ms/step - loss: 0.0227 - acc: 0.7953\n",
            "\n",
            "Epoch 03613: loss did not improve from 0.02232\n",
            "Epoch 3614/4000\n",
            "\n",
            "Epoch 03614: LearningRateScheduler setting learning rate to 1e-05.\n",
            "161/161 [==============================] - 44s 272ms/step - loss: 0.0235 - acc: 0.7957\n",
            "\n",
            "Epoch 03614: loss did not improve from 0.02232\n",
            "Epoch 3615/4000\n",
            "\n",
            "Epoch 03615: LearningRateScheduler setting learning rate to 1e-05.\n",
            "161/161 [==============================] - 42s 262ms/step - loss: 0.0237 - acc: 0.7963\n",
            "\n",
            "Epoch 03615: loss did not improve from 0.02232\n",
            "Epoch 3616/4000\n",
            "\n",
            "Epoch 03616: LearningRateScheduler setting learning rate to 1e-05.\n",
            "161/161 [==============================] - 43s 270ms/step - loss: 0.0233 - acc: 0.7993\n",
            "\n",
            "Epoch 03616: loss did not improve from 0.02232\n",
            "Epoch 3617/4000\n",
            "\n",
            "Epoch 03617: LearningRateScheduler setting learning rate to 1e-05.\n",
            "161/161 [==============================] - 42s 264ms/step - loss: 0.0233 - acc: 0.7915\n",
            "\n",
            "Epoch 03617: loss did not improve from 0.02232\n",
            "Epoch 3618/4000\n",
            "\n",
            "Epoch 03618: LearningRateScheduler setting learning rate to 1e-05.\n",
            "161/161 [==============================] - 43s 270ms/step - loss: 0.0234 - acc: 0.7937\n",
            "\n",
            "Epoch 03618: loss did not improve from 0.02232\n",
            "Epoch 3619/4000\n",
            "\n",
            "Epoch 03619: LearningRateScheduler setting learning rate to 1e-05.\n",
            "161/161 [==============================] - 43s 267ms/step - loss: 0.0224 - acc: 0.8122\n",
            "\n",
            "Epoch 03619: loss did not improve from 0.02232\n",
            "Epoch 3620/4000\n",
            "\n",
            "Epoch 03620: LearningRateScheduler setting learning rate to 1e-05.\n",
            "161/161 [==============================] - 44s 272ms/step - loss: 0.0235 - acc: 0.7928\n",
            "\n",
            "Epoch 03620: loss did not improve from 0.02232\n",
            "Epoch 3621/4000\n",
            "\n",
            "Epoch 03621: LearningRateScheduler setting learning rate to 1e-05.\n",
            "161/161 [==============================] - 44s 273ms/step - loss: 0.0231 - acc: 0.8099\n",
            "\n",
            "Epoch 03621: loss did not improve from 0.02232\n",
            "Epoch 3622/4000\n",
            "\n",
            "Epoch 03622: LearningRateScheduler setting learning rate to 1e-05.\n",
            "161/161 [==============================] - 43s 267ms/step - loss: 0.0229 - acc: 0.8016\n",
            "\n",
            "Epoch 03622: loss did not improve from 0.02232\n",
            "Epoch 3623/4000\n",
            "\n",
            "Epoch 03623: LearningRateScheduler setting learning rate to 1e-05.\n",
            "161/161 [==============================] - 44s 272ms/step - loss: 0.0241 - acc: 0.8036\n",
            "\n",
            "Epoch 03623: loss did not improve from 0.02232\n",
            "Epoch 3624/4000\n",
            "\n",
            "Epoch 03624: LearningRateScheduler setting learning rate to 1e-05.\n",
            "161/161 [==============================] - 43s 265ms/step - loss: 0.0229 - acc: 0.7986\n",
            "\n",
            "Epoch 03624: loss did not improve from 0.02232\n",
            "Epoch 3625/4000\n",
            "\n",
            "Epoch 03625: LearningRateScheduler setting learning rate to 1e-05.\n",
            "161/161 [==============================] - 44s 271ms/step - loss: 0.0238 - acc: 0.8068\n",
            "\n",
            "Epoch 03625: loss did not improve from 0.02232\n",
            "Epoch 3626/4000\n",
            "\n",
            "Epoch 03626: LearningRateScheduler setting learning rate to 1e-05.\n",
            "161/161 [==============================] - 43s 269ms/step - loss: 0.0232 - acc: 0.8027\n",
            "\n",
            "Epoch 03626: loss did not improve from 0.02232\n",
            "Epoch 3627/4000\n",
            "\n",
            "Epoch 03627: LearningRateScheduler setting learning rate to 1e-05.\n",
            "161/161 [==============================] - 43s 267ms/step - loss: 0.0229 - acc: 0.7958\n",
            "\n",
            "Epoch 03627: loss did not improve from 0.02232\n",
            "Epoch 3628/4000\n",
            "\n",
            "Epoch 03628: LearningRateScheduler setting learning rate to 1e-05.\n",
            "161/161 [==============================] - 43s 266ms/step - loss: 0.0232 - acc: 0.8054\n",
            "\n",
            "Epoch 03628: loss did not improve from 0.02232\n",
            "Epoch 3629/4000\n",
            "\n",
            "Epoch 03629: LearningRateScheduler setting learning rate to 1e-05.\n",
            "161/161 [==============================] - 43s 268ms/step - loss: 0.0233 - acc: 0.7920\n",
            "\n",
            "Epoch 03629: loss did not improve from 0.02232\n",
            "Epoch 3630/4000\n",
            "\n",
            "Epoch 03630: LearningRateScheduler setting learning rate to 1e-05.\n",
            "161/161 [==============================] - 43s 267ms/step - loss: 0.0237 - acc: 0.7911\n",
            "\n",
            "Epoch 03630: loss did not improve from 0.02232\n",
            "Epoch 3631/4000\n",
            "\n",
            "Epoch 03631: LearningRateScheduler setting learning rate to 1e-05.\n",
            "161/161 [==============================] - 44s 271ms/step - loss: 0.0233 - acc: 0.8059\n",
            "\n",
            "Epoch 03631: loss did not improve from 0.02232\n",
            "Epoch 3632/4000\n",
            "\n",
            "Epoch 03632: LearningRateScheduler setting learning rate to 1e-05.\n",
            "161/161 [==============================] - 42s 263ms/step - loss: 0.0231 - acc: 0.8059\n",
            "\n",
            "Epoch 03632: loss did not improve from 0.02232\n",
            "Epoch 3633/4000\n",
            "\n",
            "Epoch 03633: LearningRateScheduler setting learning rate to 1e-05.\n",
            "161/161 [==============================] - 43s 268ms/step - loss: 0.0233 - acc: 0.8042\n",
            "\n",
            "Epoch 03633: loss did not improve from 0.02232\n",
            "Epoch 3634/4000\n",
            "\n",
            "Epoch 03634: LearningRateScheduler setting learning rate to 1e-05.\n",
            "161/161 [==============================] - 44s 271ms/step - loss: 0.0230 - acc: 0.8049\n",
            "\n",
            "Epoch 03634: loss did not improve from 0.02232\n",
            "Epoch 3635/4000\n",
            "\n",
            "Epoch 03635: LearningRateScheduler setting learning rate to 1e-05.\n",
            "161/161 [==============================] - 44s 271ms/step - loss: 0.0221 - acc: 0.8241\n",
            "\n",
            "Epoch 03635: loss improved from 0.02232 to 0.02206, saving model to /content/drive/My Drive/Data/SeeInDarkDataset/dataset/Sony/save_Sony/12082019-080215-weights-e-3635-l-0.0220633.hdf5\n",
            "Epoch 3636/4000\n",
            "\n",
            "Epoch 03636: LearningRateScheduler setting learning rate to 1e-05.\n",
            "161/161 [==============================] - 43s 265ms/step - loss: 0.0232 - acc: 0.8157\n",
            "\n",
            "Epoch 03636: loss did not improve from 0.02206\n",
            "Epoch 3637/4000\n",
            "\n",
            "Epoch 03637: LearningRateScheduler setting learning rate to 1e-05.\n",
            "161/161 [==============================] - 44s 273ms/step - loss: 0.0241 - acc: 0.8179\n",
            "\n",
            "Epoch 03637: loss did not improve from 0.02206\n",
            "Epoch 3638/4000\n",
            "\n",
            "Epoch 03638: LearningRateScheduler setting learning rate to 1e-05.\n",
            "161/161 [==============================] - 43s 268ms/step - loss: 0.0229 - acc: 0.7973\n",
            "\n",
            "Epoch 03638: loss did not improve from 0.02206\n",
            "Epoch 3639/4000\n",
            "\n",
            "Epoch 03639: LearningRateScheduler setting learning rate to 1e-05.\n",
            "161/161 [==============================] - 43s 269ms/step - loss: 0.0237 - acc: 0.7930\n",
            "\n",
            "Epoch 03639: loss did not improve from 0.02206\n",
            "Epoch 3640/4000\n",
            "\n",
            "Epoch 03640: LearningRateScheduler setting learning rate to 1e-05.\n",
            "161/161 [==============================] - 44s 273ms/step - loss: 0.0236 - acc: 0.7967\n",
            "\n",
            "Epoch 03640: loss did not improve from 0.02206\n",
            "Epoch 3641/4000\n",
            "\n",
            "Epoch 03641: LearningRateScheduler setting learning rate to 1e-05.\n",
            "161/161 [==============================] - 43s 267ms/step - loss: 0.0228 - acc: 0.8008\n",
            "\n",
            "Epoch 03641: loss did not improve from 0.02206\n",
            "Epoch 3642/4000\n",
            "\n",
            "Epoch 03642: LearningRateScheduler setting learning rate to 1e-05.\n",
            "161/161 [==============================] - 43s 268ms/step - loss: 0.0227 - acc: 0.8007\n",
            "\n",
            "Epoch 03642: loss did not improve from 0.02206\n",
            "Epoch 3643/4000\n",
            "\n",
            "Epoch 03643: LearningRateScheduler setting learning rate to 1e-05.\n",
            "161/161 [==============================] - 44s 273ms/step - loss: 0.0229 - acc: 0.8033\n",
            "\n",
            "Epoch 03643: loss did not improve from 0.02206\n",
            "Epoch 3644/4000\n",
            "\n",
            "Epoch 03644: LearningRateScheduler setting learning rate to 1e-05.\n",
            "161/161 [==============================] - 43s 265ms/step - loss: 0.0234 - acc: 0.8135\n",
            "\n",
            "Epoch 03644: loss did not improve from 0.02206\n",
            "Epoch 3645/4000\n",
            "\n",
            "Epoch 03645: LearningRateScheduler setting learning rate to 1e-05.\n",
            "161/161 [==============================] - 44s 271ms/step - loss: 0.0232 - acc: 0.8182\n",
            "\n",
            "Epoch 03645: loss did not improve from 0.02206\n",
            "Epoch 3646/4000\n",
            "\n",
            "Epoch 03646: LearningRateScheduler setting learning rate to 1e-05.\n",
            "161/161 [==============================] - 44s 273ms/step - loss: 0.0236 - acc: 0.7949\n",
            "\n",
            "Epoch 03646: loss did not improve from 0.02206\n",
            "Epoch 3647/4000\n",
            "\n",
            "Epoch 03647: LearningRateScheduler setting learning rate to 1e-05.\n",
            "161/161 [==============================] - 43s 269ms/step - loss: 0.0232 - acc: 0.8041\n",
            "\n",
            "Epoch 03647: loss did not improve from 0.02206\n",
            "Epoch 3648/4000\n",
            "\n",
            "Epoch 03648: LearningRateScheduler setting learning rate to 1e-05.\n",
            "161/161 [==============================] - 43s 268ms/step - loss: 0.0230 - acc: 0.7900\n",
            "\n",
            "Epoch 03648: loss did not improve from 0.02206\n",
            "Epoch 3649/4000\n",
            "\n",
            "Epoch 03649: LearningRateScheduler setting learning rate to 1e-05.\n",
            "161/161 [==============================] - 44s 272ms/step - loss: 0.0224 - acc: 0.7996\n",
            "\n",
            "Epoch 03649: loss did not improve from 0.02206\n",
            "Epoch 3650/4000\n",
            "\n",
            "Epoch 03650: LearningRateScheduler setting learning rate to 1e-05.\n",
            "161/161 [==============================] - 43s 269ms/step - loss: 0.0238 - acc: 0.7947\n",
            "\n",
            "Epoch 03650: loss did not improve from 0.02206\n",
            "Epoch 3651/4000\n",
            "\n",
            "Epoch 03651: LearningRateScheduler setting learning rate to 1e-05.\n",
            "161/161 [==============================] - 44s 273ms/step - loss: 0.0231 - acc: 0.7941\n",
            "\n",
            "Epoch 03651: loss did not improve from 0.02206\n",
            "Epoch 3652/4000\n",
            "\n",
            "Epoch 03652: LearningRateScheduler setting learning rate to 1e-05.\n",
            "161/161 [==============================] - 45s 277ms/step - loss: 0.0232 - acc: 0.7817\n",
            "\n",
            "Epoch 03652: loss did not improve from 0.02206\n",
            "Epoch 3653/4000\n",
            "\n",
            "Epoch 03653: LearningRateScheduler setting learning rate to 1e-05.\n",
            "161/161 [==============================] - 44s 274ms/step - loss: 0.0229 - acc: 0.8115\n",
            "\n",
            "Epoch 03653: loss did not improve from 0.02206\n",
            "Epoch 3654/4000\n",
            "\n",
            "Epoch 03654: LearningRateScheduler setting learning rate to 1e-05.\n",
            "161/161 [==============================] - 45s 279ms/step - loss: 0.0231 - acc: 0.8049\n",
            "\n",
            "Epoch 03654: loss did not improve from 0.02206\n",
            "Epoch 3655/4000\n",
            "\n",
            "Epoch 03655: LearningRateScheduler setting learning rate to 1e-05.\n",
            "161/161 [==============================] - 45s 280ms/step - loss: 0.0241 - acc: 0.7939\n",
            "\n",
            "Epoch 03655: loss did not improve from 0.02206\n",
            "Epoch 3656/4000\n",
            "\n",
            "Epoch 03656: LearningRateScheduler setting learning rate to 1e-05.\n",
            "161/161 [==============================] - 44s 273ms/step - loss: 0.0238 - acc: 0.8058\n",
            "\n",
            "Epoch 03656: loss did not improve from 0.02206\n",
            "Epoch 3657/4000\n",
            "\n",
            "Epoch 03657: LearningRateScheduler setting learning rate to 1e-05.\n",
            "161/161 [==============================] - 44s 273ms/step - loss: 0.0240 - acc: 0.8037\n",
            "\n",
            "Epoch 03657: loss did not improve from 0.02206\n",
            "Epoch 3658/4000\n",
            "\n",
            "Epoch 03658: LearningRateScheduler setting learning rate to 1e-05.\n",
            "161/161 [==============================] - 44s 270ms/step - loss: 0.0238 - acc: 0.7920\n",
            "\n",
            "Epoch 03658: loss did not improve from 0.02206\n",
            "Epoch 3659/4000\n",
            "\n",
            "Epoch 03659: LearningRateScheduler setting learning rate to 1e-05.\n",
            "161/161 [==============================] - 43s 267ms/step - loss: 0.0233 - acc: 0.8025\n",
            "\n",
            "Epoch 03659: loss did not improve from 0.02206\n",
            "Epoch 3660/4000\n",
            "\n",
            "Epoch 03660: LearningRateScheduler setting learning rate to 1e-05.\n",
            "161/161 [==============================] - 44s 271ms/step - loss: 0.0234 - acc: 0.8035\n",
            "\n",
            "Epoch 03660: loss did not improve from 0.02206\n",
            "Epoch 3661/4000\n",
            "\n",
            "Epoch 03661: LearningRateScheduler setting learning rate to 1e-05.\n",
            "161/161 [==============================] - 45s 277ms/step - loss: 0.0234 - acc: 0.8065\n",
            "\n",
            "Epoch 03661: loss did not improve from 0.02206\n",
            "Epoch 3662/4000\n",
            "\n",
            "Epoch 03662: LearningRateScheduler setting learning rate to 1e-05.\n",
            "161/161 [==============================] - 45s 277ms/step - loss: 0.0239 - acc: 0.8009\n",
            "\n",
            "Epoch 03662: loss did not improve from 0.02206\n",
            "Epoch 3663/4000\n",
            "\n",
            "Epoch 03663: LearningRateScheduler setting learning rate to 1e-05.\n",
            "161/161 [==============================] - 44s 271ms/step - loss: 0.0234 - acc: 0.8075\n",
            "\n",
            "Epoch 03663: loss did not improve from 0.02206\n",
            "Epoch 3664/4000\n",
            "\n",
            "Epoch 03664: LearningRateScheduler setting learning rate to 1e-05.\n",
            "161/161 [==============================] - 44s 275ms/step - loss: 0.0226 - acc: 0.8008\n",
            "\n",
            "Epoch 03664: loss did not improve from 0.02206\n",
            "Epoch 3665/4000\n",
            "\n",
            "Epoch 03665: LearningRateScheduler setting learning rate to 1e-05.\n",
            "161/161 [==============================] - 44s 274ms/step - loss: 0.0234 - acc: 0.8034\n",
            "\n",
            "Epoch 03665: loss did not improve from 0.02206\n",
            "Epoch 3666/4000\n",
            "\n",
            "Epoch 03666: LearningRateScheduler setting learning rate to 1e-05.\n",
            "161/161 [==============================] - 45s 277ms/step - loss: 0.0230 - acc: 0.7964\n",
            "\n",
            "Epoch 03666: loss did not improve from 0.02206\n",
            "Epoch 3667/4000\n",
            "\n",
            "Epoch 03667: LearningRateScheduler setting learning rate to 1e-05.\n",
            "161/161 [==============================] - 44s 270ms/step - loss: 0.0233 - acc: 0.8096\n",
            "\n",
            "Epoch 03667: loss did not improve from 0.02206\n",
            "Epoch 3668/4000\n",
            "\n",
            "Epoch 03668: LearningRateScheduler setting learning rate to 1e-05.\n",
            "161/161 [==============================] - 44s 270ms/step - loss: 0.0231 - acc: 0.8038\n",
            "\n",
            "Epoch 03668: loss did not improve from 0.02206\n",
            "Epoch 3669/4000\n",
            "\n",
            "Epoch 03669: LearningRateScheduler setting learning rate to 1e-05.\n",
            "161/161 [==============================] - 46s 286ms/step - loss: 0.0231 - acc: 0.8110\n",
            "\n",
            "Epoch 03669: loss did not improve from 0.02206\n",
            "Epoch 3670/4000\n",
            "\n",
            "Epoch 03670: LearningRateScheduler setting learning rate to 1e-05.\n",
            "161/161 [==============================] - 45s 277ms/step - loss: 0.0226 - acc: 0.7967\n",
            "\n",
            "Epoch 03670: loss did not improve from 0.02206\n",
            "Epoch 3671/4000\n",
            "\n",
            "Epoch 03671: LearningRateScheduler setting learning rate to 1e-05.\n",
            "161/161 [==============================] - 46s 285ms/step - loss: 0.0224 - acc: 0.8031\n",
            "\n",
            "Epoch 03671: loss did not improve from 0.02206\n",
            "Epoch 3672/4000\n",
            "\n",
            "Epoch 03672: LearningRateScheduler setting learning rate to 1e-05.\n",
            "161/161 [==============================] - 45s 278ms/step - loss: 0.0235 - acc: 0.8120\n",
            "\n",
            "Epoch 03672: loss did not improve from 0.02206\n",
            "Epoch 3673/4000\n",
            "\n",
            "Epoch 03673: LearningRateScheduler setting learning rate to 1e-05.\n",
            "161/161 [==============================] - 44s 271ms/step - loss: 0.0239 - acc: 0.8028\n",
            "\n",
            "Epoch 03673: loss did not improve from 0.02206\n",
            "Epoch 3674/4000\n",
            "\n",
            "Epoch 03674: LearningRateScheduler setting learning rate to 1e-05.\n",
            "161/161 [==============================] - 43s 269ms/step - loss: 0.0238 - acc: 0.8071\n",
            "\n",
            "Epoch 03674: loss did not improve from 0.02206\n",
            "Epoch 3675/4000\n",
            "\n",
            "Epoch 03675: LearningRateScheduler setting learning rate to 1e-05.\n",
            "161/161 [==============================] - 44s 275ms/step - loss: 0.0229 - acc: 0.8009\n",
            "\n",
            "Epoch 03675: loss did not improve from 0.02206\n",
            "Epoch 3676/4000\n",
            "\n",
            "Epoch 03676: LearningRateScheduler setting learning rate to 1e-05.\n",
            "161/161 [==============================] - 44s 273ms/step - loss: 0.0224 - acc: 0.8115\n",
            "\n",
            "Epoch 03676: loss did not improve from 0.02206\n",
            "Epoch 3677/4000\n",
            "\n",
            "Epoch 03677: LearningRateScheduler setting learning rate to 1e-05.\n",
            "161/161 [==============================] - 45s 278ms/step - loss: 0.0233 - acc: 0.8044\n",
            "\n",
            "Epoch 03677: loss did not improve from 0.02206\n",
            "Epoch 3678/4000\n",
            "\n",
            "Epoch 03678: LearningRateScheduler setting learning rate to 1e-05.\n",
            "161/161 [==============================] - 46s 287ms/step - loss: 0.0231 - acc: 0.7991\n",
            "\n",
            "Epoch 03678: loss did not improve from 0.02206\n",
            "Epoch 3679/4000\n",
            "\n",
            "Epoch 03679: LearningRateScheduler setting learning rate to 1e-05.\n",
            "161/161 [==============================] - 44s 273ms/step - loss: 0.0233 - acc: 0.8063\n",
            "\n",
            "Epoch 03679: loss did not improve from 0.02206\n",
            "Epoch 3680/4000\n",
            "\n",
            "Epoch 03680: LearningRateScheduler setting learning rate to 1e-05.\n",
            "161/161 [==============================] - 44s 276ms/step - loss: 0.0233 - acc: 0.8118\n",
            "\n",
            "Epoch 03680: loss did not improve from 0.02206\n",
            "Epoch 3681/4000\n",
            "\n",
            "Epoch 03681: LearningRateScheduler setting learning rate to 1e-05.\n",
            "161/161 [==============================] - 43s 269ms/step - loss: 0.0231 - acc: 0.8031\n",
            "\n",
            "Epoch 03681: loss did not improve from 0.02206\n",
            "Epoch 3682/4000\n",
            "\n",
            "Epoch 03682: LearningRateScheduler setting learning rate to 1e-05.\n",
            "161/161 [==============================] - 45s 277ms/step - loss: 0.0232 - acc: 0.8075\n",
            "\n",
            "Epoch 03682: loss did not improve from 0.02206\n",
            "Epoch 3683/4000\n",
            "\n",
            "Epoch 03683: LearningRateScheduler setting learning rate to 1e-05.\n",
            "161/161 [==============================] - 45s 278ms/step - loss: 0.0229 - acc: 0.8116\n",
            "\n",
            "Epoch 03683: loss did not improve from 0.02206\n",
            "Epoch 3684/4000\n",
            "\n",
            "Epoch 03684: LearningRateScheduler setting learning rate to 1e-05.\n",
            "161/161 [==============================] - 44s 274ms/step - loss: 0.0235 - acc: 0.8007\n",
            "\n",
            "Epoch 03684: loss did not improve from 0.02206\n",
            "Epoch 3685/4000\n",
            "\n",
            "Epoch 03685: LearningRateScheduler setting learning rate to 1e-05.\n",
            "161/161 [==============================] - 44s 275ms/step - loss: 0.0230 - acc: 0.8028\n",
            "\n",
            "Epoch 03685: loss did not improve from 0.02206\n",
            "Epoch 3686/4000\n",
            "\n",
            "Epoch 03686: LearningRateScheduler setting learning rate to 1e-05.\n",
            "161/161 [==============================] - 44s 275ms/step - loss: 0.0229 - acc: 0.8096\n",
            "\n",
            "Epoch 03686: loss did not improve from 0.02206\n",
            "Epoch 3687/4000\n",
            "\n",
            "Epoch 03687: LearningRateScheduler setting learning rate to 1e-05.\n",
            "161/161 [==============================] - 44s 276ms/step - loss: 0.0230 - acc: 0.7908\n",
            "\n",
            "Epoch 03687: loss did not improve from 0.02206\n",
            "Epoch 3688/4000\n",
            "\n",
            "Epoch 03688: LearningRateScheduler setting learning rate to 1e-05.\n",
            "161/161 [==============================] - 45s 278ms/step - loss: 0.0225 - acc: 0.7999\n",
            "\n",
            "Epoch 03688: loss did not improve from 0.02206\n",
            "Epoch 3689/4000\n",
            "\n",
            "Epoch 03689: LearningRateScheduler setting learning rate to 1e-05.\n",
            "161/161 [==============================] - 45s 280ms/step - loss: 0.0227 - acc: 0.8147\n",
            "\n",
            "Epoch 03689: loss did not improve from 0.02206\n",
            "Epoch 3690/4000\n",
            "\n",
            "Epoch 03690: LearningRateScheduler setting learning rate to 1e-05.\n",
            "161/161 [==============================] - 45s 277ms/step - loss: 0.0235 - acc: 0.7991\n",
            "\n",
            "Epoch 03690: loss did not improve from 0.02206\n",
            "Epoch 3691/4000\n",
            "\n",
            "Epoch 03691: LearningRateScheduler setting learning rate to 1e-05.\n",
            "161/161 [==============================] - 44s 274ms/step - loss: 0.0220 - acc: 0.8032\n",
            "\n",
            "Epoch 03691: loss improved from 0.02206 to 0.02204, saving model to /content/drive/My Drive/Data/SeeInDarkDataset/dataset/Sony/save_Sony/12082019-080215-weights-e-3691-l-0.0220430.hdf5\n",
            "Epoch 3692/4000\n",
            "\n",
            "Epoch 03692: LearningRateScheduler setting learning rate to 1e-05.\n",
            "161/161 [==============================] - 45s 280ms/step - loss: 0.0231 - acc: 0.8020\n",
            "\n",
            "Epoch 03692: loss did not improve from 0.02204\n",
            "Epoch 3693/4000\n",
            "\n",
            "Epoch 03693: LearningRateScheduler setting learning rate to 1e-05.\n",
            "161/161 [==============================] - 45s 281ms/step - loss: 0.0238 - acc: 0.8055\n",
            "\n",
            "Epoch 03693: loss did not improve from 0.02204\n",
            "Epoch 3694/4000\n",
            "\n",
            "Epoch 03694: LearningRateScheduler setting learning rate to 1e-05.\n",
            "161/161 [==============================] - 45s 281ms/step - loss: 0.0230 - acc: 0.8200\n",
            "\n",
            "Epoch 03694: loss did not improve from 0.02204\n",
            "Epoch 3695/4000\n",
            "\n",
            "Epoch 03695: LearningRateScheduler setting learning rate to 1e-05.\n",
            "161/161 [==============================] - 44s 272ms/step - loss: 0.0227 - acc: 0.7944\n",
            "\n",
            "Epoch 03695: loss did not improve from 0.02204\n",
            "Epoch 3696/4000\n",
            "\n",
            "Epoch 03696: LearningRateScheduler setting learning rate to 1e-05.\n",
            "161/161 [==============================] - 45s 279ms/step - loss: 0.0226 - acc: 0.8004\n",
            "\n",
            "Epoch 03696: loss did not improve from 0.02204\n",
            "Epoch 3697/4000\n",
            "\n",
            "Epoch 03697: LearningRateScheduler setting learning rate to 1e-05.\n",
            "161/161 [==============================] - 44s 274ms/step - loss: 0.0232 - acc: 0.8070\n",
            "\n",
            "Epoch 03697: loss did not improve from 0.02204\n",
            "Epoch 3698/4000\n",
            "\n",
            "Epoch 03698: LearningRateScheduler setting learning rate to 1e-05.\n",
            "161/161 [==============================] - 44s 272ms/step - loss: 0.0235 - acc: 0.7971\n",
            "\n",
            "Epoch 03698: loss did not improve from 0.02204\n",
            "Epoch 3699/4000\n",
            "\n",
            "Epoch 03699: LearningRateScheduler setting learning rate to 1e-05.\n",
            "161/161 [==============================] - 44s 273ms/step - loss: 0.0236 - acc: 0.8086\n",
            "\n",
            "Epoch 03699: loss did not improve from 0.02204\n",
            "Epoch 3700/4000\n",
            "\n",
            "Epoch 03700: LearningRateScheduler setting learning rate to 1e-05.\n",
            "161/161 [==============================] - 44s 273ms/step - loss: 0.0231 - acc: 0.7946\n",
            "\n",
            "Epoch 03700: loss did not improve from 0.02204\n",
            "Epoch 3701/4000\n",
            "\n",
            "Epoch 03701: LearningRateScheduler setting learning rate to 1e-05.\n",
            "161/161 [==============================] - 44s 273ms/step - loss: 0.0237 - acc: 0.8208\n",
            "\n",
            "Epoch 03701: loss did not improve from 0.02204\n",
            "Epoch 3702/4000\n",
            "\n",
            "Epoch 03702: LearningRateScheduler setting learning rate to 1e-05.\n",
            "161/161 [==============================] - 45s 279ms/step - loss: 0.0230 - acc: 0.7927\n",
            "\n",
            "Epoch 03702: loss did not improve from 0.02204\n",
            "Epoch 3703/4000\n",
            "\n",
            "Epoch 03703: LearningRateScheduler setting learning rate to 1e-05.\n",
            "161/161 [==============================] - 44s 274ms/step - loss: 0.0231 - acc: 0.8148\n",
            "\n",
            "Epoch 03703: loss did not improve from 0.02204\n",
            "Epoch 3704/4000\n",
            "\n",
            "Epoch 03704: LearningRateScheduler setting learning rate to 1e-05.\n",
            "161/161 [==============================] - 43s 269ms/step - loss: 0.0233 - acc: 0.7966\n",
            "\n",
            "Epoch 03704: loss did not improve from 0.02204\n",
            "Epoch 3705/4000\n",
            "\n",
            "Epoch 03705: LearningRateScheduler setting learning rate to 1e-05.\n",
            "161/161 [==============================] - 44s 272ms/step - loss: 0.0226 - acc: 0.8056\n",
            "\n",
            "Epoch 03705: loss did not improve from 0.02204\n",
            "Epoch 3706/4000\n",
            "\n",
            "Epoch 03706: LearningRateScheduler setting learning rate to 1e-05.\n",
            "161/161 [==============================] - 43s 267ms/step - loss: 0.0232 - acc: 0.7962\n",
            "\n",
            "Epoch 03706: loss did not improve from 0.02204\n",
            "Epoch 3707/4000\n",
            "\n",
            "Epoch 03707: LearningRateScheduler setting learning rate to 1e-05.\n",
            "161/161 [==============================] - 45s 281ms/step - loss: 0.0233 - acc: 0.7942\n",
            "\n",
            "Epoch 03707: loss did not improve from 0.02204\n",
            "Epoch 3708/4000\n",
            "\n",
            "Epoch 03708: LearningRateScheduler setting learning rate to 1e-05.\n",
            "161/161 [==============================] - 44s 276ms/step - loss: 0.0226 - acc: 0.8092\n",
            "\n",
            "Epoch 03708: loss did not improve from 0.02204\n",
            "Epoch 3709/4000\n",
            "\n",
            "Epoch 03709: LearningRateScheduler setting learning rate to 1e-05.\n",
            "161/161 [==============================] - 44s 271ms/step - loss: 0.0233 - acc: 0.8035\n",
            "\n",
            "Epoch 03709: loss did not improve from 0.02204\n",
            "Epoch 3710/4000\n",
            "\n",
            "Epoch 03710: LearningRateScheduler setting learning rate to 1e-05.\n",
            "161/161 [==============================] - 44s 272ms/step - loss: 0.0233 - acc: 0.7971\n",
            "\n",
            "Epoch 03710: loss did not improve from 0.02204\n",
            "Epoch 3711/4000\n",
            "\n",
            "Epoch 03711: LearningRateScheduler setting learning rate to 1e-05.\n",
            "161/161 [==============================] - 46s 286ms/step - loss: 0.0236 - acc: 0.8113\n",
            "\n",
            "Epoch 03711: loss did not improve from 0.02204\n",
            "Epoch 3712/4000\n",
            "\n",
            "Epoch 03712: LearningRateScheduler setting learning rate to 1e-05.\n",
            "161/161 [==============================] - 45s 279ms/step - loss: 0.0233 - acc: 0.7832\n",
            "\n",
            "Epoch 03712: loss did not improve from 0.02204\n",
            "Epoch 3713/4000\n",
            "\n",
            "Epoch 03713: LearningRateScheduler setting learning rate to 1e-05.\n",
            "161/161 [==============================] - 44s 272ms/step - loss: 0.0238 - acc: 0.8053\n",
            "\n",
            "Epoch 03713: loss did not improve from 0.02204\n",
            "Epoch 3714/4000\n",
            "\n",
            "Epoch 03714: LearningRateScheduler setting learning rate to 1e-05.\n",
            "161/161 [==============================] - 45s 277ms/step - loss: 0.0227 - acc: 0.8070\n",
            "\n",
            "Epoch 03714: loss did not improve from 0.02204\n",
            "Epoch 3715/4000\n",
            "\n",
            "Epoch 03715: LearningRateScheduler setting learning rate to 1e-05.\n",
            "161/161 [==============================] - 45s 277ms/step - loss: 0.0231 - acc: 0.8005\n",
            "\n",
            "Epoch 03715: loss did not improve from 0.02204\n",
            "Epoch 3716/4000\n",
            "\n",
            "Epoch 03716: LearningRateScheduler setting learning rate to 1e-05.\n",
            "161/161 [==============================] - 44s 272ms/step - loss: 0.0233 - acc: 0.7852\n",
            "\n",
            "Epoch 03716: loss did not improve from 0.02204\n",
            "Epoch 3717/4000\n",
            "\n",
            "Epoch 03717: LearningRateScheduler setting learning rate to 1e-05.\n",
            "161/161 [==============================] - 44s 274ms/step - loss: 0.0240 - acc: 0.8175\n",
            "\n",
            "Epoch 03717: loss did not improve from 0.02204\n",
            "Epoch 3718/4000\n",
            "\n",
            "Epoch 03718: LearningRateScheduler setting learning rate to 1e-05.\n",
            "161/161 [==============================] - 43s 269ms/step - loss: 0.0232 - acc: 0.7972\n",
            "\n",
            "Epoch 03718: loss did not improve from 0.02204\n",
            "Epoch 3719/4000\n",
            "\n",
            "Epoch 03719: LearningRateScheduler setting learning rate to 1e-05.\n",
            "161/161 [==============================] - 43s 270ms/step - loss: 0.0232 - acc: 0.7977\n",
            "\n",
            "Epoch 03719: loss did not improve from 0.02204\n",
            "Epoch 3720/4000\n",
            "\n",
            "Epoch 03720: LearningRateScheduler setting learning rate to 1e-05.\n",
            "161/161 [==============================] - 44s 272ms/step - loss: 0.0226 - acc: 0.8205\n",
            "\n",
            "Epoch 03720: loss did not improve from 0.02204\n",
            "Epoch 3721/4000\n",
            "\n",
            "Epoch 03721: LearningRateScheduler setting learning rate to 1e-05.\n",
            "161/161 [==============================] - 44s 273ms/step - loss: 0.0232 - acc: 0.8126\n",
            "\n",
            "Epoch 03721: loss did not improve from 0.02204\n",
            "Epoch 3722/4000\n",
            "\n",
            "Epoch 03722: LearningRateScheduler setting learning rate to 1e-05.\n",
            "161/161 [==============================] - 45s 280ms/step - loss: 0.0231 - acc: 0.7941\n",
            "\n",
            "Epoch 03722: loss did not improve from 0.02204\n",
            "Epoch 3723/4000\n",
            "\n",
            "Epoch 03723: LearningRateScheduler setting learning rate to 1e-05.\n",
            "161/161 [==============================] - 46s 283ms/step - loss: 0.0226 - acc: 0.8200\n",
            "\n",
            "Epoch 03723: loss did not improve from 0.02204\n",
            "Epoch 3724/4000\n",
            "\n",
            "Epoch 03724: LearningRateScheduler setting learning rate to 1e-05.\n",
            "161/161 [==============================] - 46s 283ms/step - loss: 0.0230 - acc: 0.7996\n",
            "\n",
            "Epoch 03724: loss did not improve from 0.02204\n",
            "Epoch 3725/4000\n",
            "\n",
            "Epoch 03725: LearningRateScheduler setting learning rate to 1e-05.\n",
            "161/161 [==============================] - 45s 279ms/step - loss: 0.0229 - acc: 0.8012\n",
            "\n",
            "Epoch 03725: loss did not improve from 0.02204\n",
            "Epoch 3726/4000\n",
            "\n",
            "Epoch 03726: LearningRateScheduler setting learning rate to 1e-05.\n",
            "161/161 [==============================] - 45s 278ms/step - loss: 0.0230 - acc: 0.8143\n",
            "\n",
            "Epoch 03726: loss did not improve from 0.02204\n",
            "Epoch 3727/4000\n",
            "\n",
            "Epoch 03727: LearningRateScheduler setting learning rate to 1e-05.\n",
            "161/161 [==============================] - 44s 274ms/step - loss: 0.0226 - acc: 0.8279\n",
            "\n",
            "Epoch 03727: loss did not improve from 0.02204\n",
            "Epoch 3728/4000\n",
            "\n",
            "Epoch 03728: LearningRateScheduler setting learning rate to 1e-05.\n",
            "161/161 [==============================] - 44s 274ms/step - loss: 0.0231 - acc: 0.8009\n",
            "\n",
            "Epoch 03728: loss did not improve from 0.02204\n",
            "Epoch 3729/4000\n",
            "\n",
            "Epoch 03729: LearningRateScheduler setting learning rate to 1e-05.\n",
            "161/161 [==============================] - 44s 275ms/step - loss: 0.0226 - acc: 0.8050\n",
            "\n",
            "Epoch 03729: loss did not improve from 0.02204\n",
            "Epoch 3730/4000\n",
            "\n",
            "Epoch 03730: LearningRateScheduler setting learning rate to 1e-05.\n",
            "161/161 [==============================] - 44s 273ms/step - loss: 0.0231 - acc: 0.7963\n",
            "\n",
            "Epoch 03730: loss did not improve from 0.02204\n",
            "Epoch 3731/4000\n",
            "\n",
            "Epoch 03731: LearningRateScheduler setting learning rate to 1e-05.\n",
            "161/161 [==============================] - 45s 280ms/step - loss: 0.0232 - acc: 0.7999\n",
            "\n",
            "Epoch 03731: loss did not improve from 0.02204\n",
            "Epoch 3732/4000\n",
            "\n",
            "Epoch 03732: LearningRateScheduler setting learning rate to 1e-05.\n",
            "161/161 [==============================] - 45s 278ms/step - loss: 0.0229 - acc: 0.8084\n",
            "\n",
            "Epoch 03732: loss did not improve from 0.02204\n",
            "Epoch 3733/4000\n",
            "\n",
            "Epoch 03733: LearningRateScheduler setting learning rate to 1e-05.\n",
            "161/161 [==============================] - 44s 271ms/step - loss: 0.0232 - acc: 0.8086\n",
            "\n",
            "Epoch 03733: loss did not improve from 0.02204\n",
            "Epoch 3734/4000\n",
            "\n",
            "Epoch 03734: LearningRateScheduler setting learning rate to 1e-05.\n",
            "161/161 [==============================] - 44s 272ms/step - loss: 0.0235 - acc: 0.7970\n",
            "\n",
            "Epoch 03734: loss did not improve from 0.02204\n",
            "Epoch 3735/4000\n",
            "\n",
            "Epoch 03735: LearningRateScheduler setting learning rate to 1e-05.\n",
            "161/161 [==============================] - 43s 270ms/step - loss: 0.0233 - acc: 0.8060\n",
            "\n",
            "Epoch 03735: loss did not improve from 0.02204\n",
            "Epoch 3736/4000\n",
            "\n",
            "Epoch 03736: LearningRateScheduler setting learning rate to 1e-05.\n",
            "161/161 [==============================] - 43s 268ms/step - loss: 0.0225 - acc: 0.8096\n",
            "\n",
            "Epoch 03736: loss did not improve from 0.02204\n",
            "Epoch 3737/4000\n",
            "\n",
            "Epoch 03737: LearningRateScheduler setting learning rate to 1e-05.\n",
            "161/161 [==============================] - 44s 275ms/step - loss: 0.0226 - acc: 0.8005\n",
            "\n",
            "Epoch 03737: loss did not improve from 0.02204\n",
            "Epoch 3738/4000\n",
            "\n",
            "Epoch 03738: LearningRateScheduler setting learning rate to 1e-05.\n",
            "161/161 [==============================] - 45s 277ms/step - loss: 0.0233 - acc: 0.7915\n",
            "\n",
            "Epoch 03738: loss did not improve from 0.02204\n",
            "Epoch 3739/4000\n",
            "\n",
            "Epoch 03739: LearningRateScheduler setting learning rate to 1e-05.\n",
            "161/161 [==============================] - 44s 274ms/step - loss: 0.0227 - acc: 0.7827\n",
            "\n",
            "Epoch 03739: loss did not improve from 0.02204\n",
            "Epoch 3740/4000\n",
            "\n",
            "Epoch 03740: LearningRateScheduler setting learning rate to 1e-05.\n",
            "161/161 [==============================] - 44s 276ms/step - loss: 0.0228 - acc: 0.7946\n",
            "\n",
            "Epoch 03740: loss did not improve from 0.02204\n",
            "Epoch 3741/4000\n",
            "\n",
            "Epoch 03741: LearningRateScheduler setting learning rate to 1e-05.\n",
            "161/161 [==============================] - 45s 281ms/step - loss: 0.0220 - acc: 0.7889\n",
            "\n",
            "Epoch 03741: loss improved from 0.02204 to 0.02197, saving model to /content/drive/My Drive/Data/SeeInDarkDataset/dataset/Sony/save_Sony/12082019-080215-weights-e-3741-l-0.0219692.hdf5\n",
            "Epoch 3742/4000\n",
            "\n",
            "Epoch 03742: LearningRateScheduler setting learning rate to 1e-05.\n",
            "161/161 [==============================] - 45s 278ms/step - loss: 0.0237 - acc: 0.8101\n",
            "\n",
            "Epoch 03742: loss did not improve from 0.02197\n",
            "Epoch 3743/4000\n",
            "\n",
            "Epoch 03743: LearningRateScheduler setting learning rate to 1e-05.\n",
            "161/161 [==============================] - 45s 282ms/step - loss: 0.0222 - acc: 0.8162\n",
            "\n",
            "Epoch 03743: loss did not improve from 0.02197\n",
            "Epoch 3744/4000\n",
            "\n",
            "Epoch 03744: LearningRateScheduler setting learning rate to 1e-05.\n",
            "161/161 [==============================] - 45s 280ms/step - loss: 0.0231 - acc: 0.8176\n",
            "\n",
            "Epoch 03744: loss did not improve from 0.02197\n",
            "Epoch 3745/4000\n",
            "\n",
            "Epoch 03745: LearningRateScheduler setting learning rate to 1e-05.\n",
            "161/161 [==============================] - 44s 275ms/step - loss: 0.0226 - acc: 0.8075\n",
            "\n",
            "Epoch 03745: loss did not improve from 0.02197\n",
            "Epoch 3746/4000\n",
            "\n",
            "Epoch 03746: LearningRateScheduler setting learning rate to 1e-05.\n",
            "161/161 [==============================] - 44s 272ms/step - loss: 0.0231 - acc: 0.7965\n",
            "\n",
            "Epoch 03746: loss did not improve from 0.02197\n",
            "Epoch 3747/4000\n",
            "\n",
            "Epoch 03747: LearningRateScheduler setting learning rate to 1e-05.\n",
            "161/161 [==============================] - 44s 271ms/step - loss: 0.0223 - acc: 0.8028\n",
            "\n",
            "Epoch 03747: loss did not improve from 0.02197\n",
            "Epoch 3748/4000\n",
            "\n",
            "Epoch 03748: LearningRateScheduler setting learning rate to 1e-05.\n",
            "161/161 [==============================] - 44s 276ms/step - loss: 0.0224 - acc: 0.8011\n",
            "\n",
            "Epoch 03748: loss did not improve from 0.02197\n",
            "Epoch 3749/4000\n",
            "\n",
            "Epoch 03749: LearningRateScheduler setting learning rate to 1e-05.\n",
            "161/161 [==============================] - 44s 273ms/step - loss: 0.0224 - acc: 0.8044\n",
            "\n",
            "Epoch 03749: loss did not improve from 0.02197\n",
            "Epoch 3750/4000\n",
            "\n",
            "Epoch 03750: LearningRateScheduler setting learning rate to 1e-05.\n",
            "161/161 [==============================] - 44s 275ms/step - loss: 0.0235 - acc: 0.7799\n",
            "\n",
            "Epoch 03750: loss did not improve from 0.02197\n",
            "Epoch 3751/4000\n",
            "\n",
            "Epoch 03751: LearningRateScheduler setting learning rate to 1e-05.\n",
            "161/161 [==============================] - 43s 268ms/step - loss: 0.0228 - acc: 0.8190\n",
            "\n",
            "Epoch 03751: loss did not improve from 0.02197\n",
            "Epoch 3752/4000\n",
            "\n",
            "Epoch 03752: LearningRateScheduler setting learning rate to 1e-05.\n",
            "161/161 [==============================] - 44s 272ms/step - loss: 0.0235 - acc: 0.7936\n",
            "\n",
            "Epoch 03752: loss did not improve from 0.02197\n",
            "Epoch 3753/4000\n",
            "\n",
            "Epoch 03753: LearningRateScheduler setting learning rate to 1e-05.\n",
            "161/161 [==============================] - 45s 279ms/step - loss: 0.0222 - acc: 0.8065\n",
            "\n",
            "Epoch 03753: loss did not improve from 0.02197\n",
            "Epoch 3754/4000\n",
            "\n",
            "Epoch 03754: LearningRateScheduler setting learning rate to 1e-05.\n",
            "161/161 [==============================] - 45s 281ms/step - loss: 0.0227 - acc: 0.8167\n",
            "\n",
            "Epoch 03754: loss did not improve from 0.02197\n",
            "Epoch 3755/4000\n",
            "\n",
            "Epoch 03755: LearningRateScheduler setting learning rate to 1e-05.\n",
            "161/161 [==============================] - 44s 271ms/step - loss: 0.0228 - acc: 0.8110\n",
            "\n",
            "Epoch 03755: loss did not improve from 0.02197\n",
            "Epoch 3756/4000\n",
            "\n",
            "Epoch 03756: LearningRateScheduler setting learning rate to 1e-05.\n",
            "161/161 [==============================] - 44s 274ms/step - loss: 0.0230 - acc: 0.8094\n",
            "\n",
            "Epoch 03756: loss did not improve from 0.02197\n",
            "Epoch 3757/4000\n",
            "\n",
            "Epoch 03757: LearningRateScheduler setting learning rate to 1e-05.\n",
            "161/161 [==============================] - 45s 281ms/step - loss: 0.0229 - acc: 0.8068\n",
            "\n",
            "Epoch 03757: loss did not improve from 0.02197\n",
            "Epoch 3758/4000\n",
            "\n",
            "Epoch 03758: LearningRateScheduler setting learning rate to 1e-05.\n",
            "161/161 [==============================] - 45s 278ms/step - loss: 0.0223 - acc: 0.8126\n",
            "\n",
            "Epoch 03758: loss did not improve from 0.02197\n",
            "Epoch 3759/4000\n",
            "\n",
            "Epoch 03759: LearningRateScheduler setting learning rate to 1e-05.\n",
            "161/161 [==============================] - 45s 278ms/step - loss: 0.0226 - acc: 0.7982\n",
            "\n",
            "Epoch 03759: loss did not improve from 0.02197\n",
            "Epoch 3760/4000\n",
            "\n",
            "Epoch 03760: LearningRateScheduler setting learning rate to 1e-05.\n",
            "161/161 [==============================] - 43s 268ms/step - loss: 0.0228 - acc: 0.8115\n",
            "\n",
            "Epoch 03760: loss did not improve from 0.02197\n",
            "Epoch 3761/4000\n",
            "\n",
            "Epoch 03761: LearningRateScheduler setting learning rate to 1e-05.\n",
            "161/161 [==============================] - 44s 276ms/step - loss: 0.0234 - acc: 0.8044\n",
            "\n",
            "Epoch 03761: loss did not improve from 0.02197\n",
            "Epoch 3762/4000\n",
            "\n",
            "Epoch 03762: LearningRateScheduler setting learning rate to 1e-05.\n",
            "161/161 [==============================] - 44s 273ms/step - loss: 0.0233 - acc: 0.7931\n",
            "\n",
            "Epoch 03762: loss did not improve from 0.02197\n",
            "Epoch 3763/4000\n",
            "\n",
            "Epoch 03763: LearningRateScheduler setting learning rate to 1e-05.\n",
            "161/161 [==============================] - 45s 281ms/step - loss: 0.0232 - acc: 0.8060\n",
            "\n",
            "Epoch 03763: loss did not improve from 0.02197\n",
            "Epoch 3764/4000\n",
            "\n",
            "Epoch 03764: LearningRateScheduler setting learning rate to 1e-05.\n",
            "161/161 [==============================] - 43s 270ms/step - loss: 0.0221 - acc: 0.7846\n",
            "\n",
            "Epoch 03764: loss did not improve from 0.02197\n",
            "Epoch 3765/4000\n",
            "\n",
            "Epoch 03765: LearningRateScheduler setting learning rate to 1e-05.\n",
            "161/161 [==============================] - 45s 279ms/step - loss: 0.0227 - acc: 0.7850\n",
            "\n",
            "Epoch 03765: loss did not improve from 0.02197\n",
            "Epoch 3766/4000\n",
            "\n",
            "Epoch 03766: LearningRateScheduler setting learning rate to 1e-05.\n",
            "161/161 [==============================] - 43s 267ms/step - loss: 0.0226 - acc: 0.7767\n",
            "\n",
            "Epoch 03766: loss did not improve from 0.02197\n",
            "Epoch 3767/4000\n",
            "\n",
            "Epoch 03767: LearningRateScheduler setting learning rate to 1e-05.\n",
            "161/161 [==============================] - 45s 282ms/step - loss: 0.0233 - acc: 0.7972\n",
            "\n",
            "Epoch 03767: loss did not improve from 0.02197\n",
            "Epoch 3768/4000\n",
            "\n",
            "Epoch 03768: LearningRateScheduler setting learning rate to 1e-05.\n",
            "161/161 [==============================] - 44s 275ms/step - loss: 0.0227 - acc: 0.8120\n",
            "\n",
            "Epoch 03768: loss did not improve from 0.02197\n",
            "Epoch 3769/4000\n",
            "\n",
            "Epoch 03769: LearningRateScheduler setting learning rate to 1e-05.\n",
            "161/161 [==============================] - 45s 277ms/step - loss: 0.0231 - acc: 0.8047\n",
            "\n",
            "Epoch 03769: loss did not improve from 0.02197\n",
            "Epoch 3770/4000\n",
            "\n",
            "Epoch 03770: LearningRateScheduler setting learning rate to 1e-05.\n",
            "161/161 [==============================] - 45s 279ms/step - loss: 0.0236 - acc: 0.8033\n",
            "\n",
            "Epoch 03770: loss did not improve from 0.02197\n",
            "Epoch 3771/4000\n",
            "\n",
            "Epoch 03771: LearningRateScheduler setting learning rate to 1e-05.\n",
            "161/161 [==============================] - 45s 281ms/step - loss: 0.0228 - acc: 0.8006\n",
            "\n",
            "Epoch 03771: loss did not improve from 0.02197\n",
            "Epoch 3772/4000\n",
            "\n",
            "Epoch 03772: LearningRateScheduler setting learning rate to 1e-05.\n",
            "161/161 [==============================] - 44s 270ms/step - loss: 0.0233 - acc: 0.7942\n",
            "\n",
            "Epoch 03772: loss did not improve from 0.02197\n",
            "Epoch 3773/4000\n",
            "\n",
            "Epoch 03773: LearningRateScheduler setting learning rate to 1e-05.\n",
            "161/161 [==============================] - 44s 271ms/step - loss: 0.0229 - acc: 0.7938\n",
            "\n",
            "Epoch 03773: loss did not improve from 0.02197\n",
            "Epoch 3774/4000\n",
            "\n",
            "Epoch 03774: LearningRateScheduler setting learning rate to 1e-05.\n",
            "161/161 [==============================] - 44s 272ms/step - loss: 0.0221 - acc: 0.8048\n",
            "\n",
            "Epoch 03774: loss did not improve from 0.02197\n",
            "Epoch 3775/4000\n",
            "\n",
            "Epoch 03775: LearningRateScheduler setting learning rate to 1e-05.\n",
            "161/161 [==============================] - 44s 272ms/step - loss: 0.0235 - acc: 0.8082\n",
            "\n",
            "Epoch 03775: loss did not improve from 0.02197\n",
            "Epoch 3776/4000\n",
            "\n",
            "Epoch 03776: LearningRateScheduler setting learning rate to 1e-05.\n",
            "161/161 [==============================] - 45s 279ms/step - loss: 0.0228 - acc: 0.8052\n",
            "\n",
            "Epoch 03776: loss did not improve from 0.02197\n",
            "Epoch 3777/4000\n",
            "\n",
            "Epoch 03777: LearningRateScheduler setting learning rate to 1e-05.\n",
            "161/161 [==============================] - 44s 273ms/step - loss: 0.0231 - acc: 0.7966\n",
            "\n",
            "Epoch 03777: loss did not improve from 0.02197\n",
            "Epoch 3778/4000\n",
            "\n",
            "Epoch 03778: LearningRateScheduler setting learning rate to 1e-05.\n",
            "161/161 [==============================] - 46s 285ms/step - loss: 0.0233 - acc: 0.8152\n",
            "\n",
            "Epoch 03778: loss did not improve from 0.02197\n",
            "Epoch 3779/4000\n",
            "\n",
            "Epoch 03779: LearningRateScheduler setting learning rate to 1e-05.\n",
            "161/161 [==============================] - 44s 271ms/step - loss: 0.0222 - acc: 0.8077\n",
            "\n",
            "Epoch 03779: loss did not improve from 0.02197\n",
            "Epoch 3780/4000\n",
            "\n",
            "Epoch 03780: LearningRateScheduler setting learning rate to 1e-05.\n",
            "161/161 [==============================] - 45s 276ms/step - loss: 0.0233 - acc: 0.8240\n",
            "\n",
            "Epoch 03780: loss did not improve from 0.02197\n",
            "Epoch 3781/4000\n",
            "\n",
            "Epoch 03781: LearningRateScheduler setting learning rate to 1e-05.\n",
            "161/161 [==============================] - 44s 274ms/step - loss: 0.0223 - acc: 0.8106\n",
            "\n",
            "Epoch 03781: loss did not improve from 0.02197\n",
            "Epoch 3782/4000\n",
            "\n",
            "Epoch 03782: LearningRateScheduler setting learning rate to 1e-05.\n",
            "161/161 [==============================] - 44s 276ms/step - loss: 0.0233 - acc: 0.8021\n",
            "\n",
            "Epoch 03782: loss did not improve from 0.02197\n",
            "Epoch 3783/4000\n",
            "\n",
            "Epoch 03783: LearningRateScheduler setting learning rate to 1e-05.\n",
            "161/161 [==============================] - 45s 278ms/step - loss: 0.0223 - acc: 0.8014\n",
            "\n",
            "Epoch 03783: loss did not improve from 0.02197\n",
            "Epoch 3784/4000\n",
            "\n",
            "Epoch 03784: LearningRateScheduler setting learning rate to 1e-05.\n",
            "161/161 [==============================] - 44s 270ms/step - loss: 0.0236 - acc: 0.8008\n",
            "\n",
            "Epoch 03784: loss did not improve from 0.02197\n",
            "Epoch 3785/4000\n",
            "\n",
            "Epoch 03785: LearningRateScheduler setting learning rate to 1e-05.\n",
            "161/161 [==============================] - 43s 270ms/step - loss: 0.0230 - acc: 0.7922\n",
            "\n",
            "Epoch 03785: loss did not improve from 0.02197\n",
            "Epoch 3786/4000\n",
            "\n",
            "Epoch 03786: LearningRateScheduler setting learning rate to 1e-05.\n",
            "161/161 [==============================] - 44s 272ms/step - loss: 0.0229 - acc: 0.8141\n",
            "\n",
            "Epoch 03786: loss did not improve from 0.02197\n",
            "Epoch 3787/4000\n",
            "\n",
            "Epoch 03787: LearningRateScheduler setting learning rate to 1e-05.\n",
            "161/161 [==============================] - 44s 276ms/step - loss: 0.0236 - acc: 0.8035\n",
            "\n",
            "Epoch 03787: loss did not improve from 0.02197\n",
            "Epoch 3788/4000\n",
            "\n",
            "Epoch 03788: LearningRateScheduler setting learning rate to 1e-05.\n",
            "161/161 [==============================] - 44s 270ms/step - loss: 0.0228 - acc: 0.7970\n",
            "\n",
            "Epoch 03788: loss did not improve from 0.02197\n",
            "Epoch 3789/4000\n",
            "\n",
            "Epoch 03789: LearningRateScheduler setting learning rate to 1e-05.\n",
            "161/161 [==============================] - 44s 272ms/step - loss: 0.0231 - acc: 0.7986\n",
            "\n",
            "Epoch 03789: loss did not improve from 0.02197\n",
            "Epoch 3790/4000\n",
            "\n",
            "Epoch 03790: LearningRateScheduler setting learning rate to 1e-05.\n",
            "161/161 [==============================] - 44s 275ms/step - loss: 0.0234 - acc: 0.8003\n",
            "\n",
            "Epoch 03790: loss did not improve from 0.02197\n",
            "Epoch 3791/4000\n",
            "\n",
            "Epoch 03791: LearningRateScheduler setting learning rate to 1e-05.\n",
            "161/161 [==============================] - 45s 278ms/step - loss: 0.0231 - acc: 0.8045\n",
            "\n",
            "Epoch 03791: loss did not improve from 0.02197\n",
            "Epoch 3792/4000\n",
            "\n",
            "Epoch 03792: LearningRateScheduler setting learning rate to 1e-05.\n",
            "161/161 [==============================] - 44s 276ms/step - loss: 0.0224 - acc: 0.8084\n",
            "\n",
            "Epoch 03792: loss did not improve from 0.02197\n",
            "Epoch 3793/4000\n",
            "\n",
            "Epoch 03793: LearningRateScheduler setting learning rate to 1e-05.\n",
            " 95/161 [================>.............] - ETA: 17s - loss: 0.0251 - acc: 0.8163Buffered data was truncated after reaching the output size limit."
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K6rt0FGT6jY-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# for epoch in range(lastepoch, 4001):\n",
        "#     #if os.path.isdir('result/%04d' % epoch):\n",
        "#     #    continue\n",
        "#     ###\n",
        "#     # used for skipping the current epoch if the checkpoint directory exists\n",
        "#     ###\n",
        "#     cnt = 0\n",
        "#     if epoch > 2000:\n",
        "#         learning_rate = 1e-5\n",
        "\n",
        "#     for ind in np.random.permutation(len(train_ids)):\n",
        "\n",
        "#         # get the path from image id\n",
        "\n",
        "#         train_id = train_ids[ind]\n",
        "#         in_files = glob.glob(input_dir + '%05d_00*.ARW' % train_id) # get all the images path with pattern 0*train_id\n",
        "#         in_path = in_files[np.random.randint(0, len(in_files))] # get any one image randomly\n",
        "#         in_fn = os.path.basename(in_path) # get only the full image name\n",
        "\n",
        "#         gt_files = glob.glob(gt_dir + '%05d_00*.ARW' % train_id) # get the ground truth image path(s) (only 1 may exist. hence select [0]th below)\n",
        "#         gt_path = gt_files[0] # get the first one (as only 1 gt image should exist)\n",
        "#         gt_fn = os.path.basename(gt_path) # get only the full image name for gt image\n",
        "#         in_exposure = float(in_fn[9:-5]) # get the exposure for input image\n",
        "#         gt_exposure = float(gt_fn[9:-5]) # get the exposure for gt image\n",
        "#         ratio = min(gt_exposure / in_exposure, 300) # get the amplification ratio\n",
        "\n",
        "#         st = time.time()\n",
        "#         cnt += 1\n",
        "\n",
        "#         if input_images[str(ratio)[0:3]][ind] is None: # if image is not loaded (first epoch), load it\n",
        "#             raw = rawpy.imread(in_path)\n",
        "#             input_images[str(ratio)[0:3]][ind] = \\\n",
        "#                 np.expand_dims(pack_raw(raw), axis=0) * ratio # pack the bayer image in 4 channels of RGBG\n",
        "\n",
        "#             gt_raw = rawpy.imread(gt_path)\n",
        "#             im = gt_raw.postprocess(use_camera_wb=True,\n",
        "#                                     half_size=False,\n",
        "#                                     no_auto_bright=True, output_bps=16)\n",
        "#             gt_images[ind] = np.expand_dims(np.float32(im / 65535.0),axis=0) # divide by 65535 to normalise (scale between 0 and 1)\n",
        "\n",
        "#         # crop\n",
        "\n",
        "#         H = input_images[str(ratio)[0:3]][ind].shape[1] # get the image height (number of rows)\n",
        "#         W = input_images[str(ratio)[0:3]][ind].shape[2] # get the image width (number of columns)\n",
        "\n",
        "#         xx = np.random.randint(0, W - ps) # get a random number in W-ps (W-512)\n",
        "#         yy = np.random.randint(0, H - ps) # get a random number in H-ps (H-512)\n",
        "#         input_patch = input_images[str(ratio)[0:3]][ind][:, yy:yy + ps,\n",
        "#                 xx:xx + ps, :]\n",
        "#         gt_patch = gt_images[ind][:, yy * 2:yy * 2 + ps * 2, xx * 2:xx\n",
        "#                                   * 2 + ps * 2, :]\n",
        "\n",
        "#         if np.random.randint(2) == 1:  # random flip for rows\n",
        "#             input_patch = np.flip(input_patch, axis=1)\n",
        "#             gt_patch = np.flip(gt_patch, axis=1)\n",
        "#         if np.random.randint(2) == 1:  # random flip for columns\n",
        "#             input_patch = np.flip(input_patch, axis=2)\n",
        "#             gt_patch = np.flip(gt_patch, axis=2)\n",
        "#         if np.random.randint(2) == 1:  # random transpose\n",
        "#             input_patch = np.transpose(input_patch, (0, 2, 1, 3))\n",
        "#             gt_patch = np.transpose(gt_patch, (0, 2, 1, 3))\\\n",
        "\n",
        "#         input_patch = np.minimum(input_patch, 1.0)\n",
        "\n",
        "#         (_, G_current, output) = sess.run([G_opt, G_loss, out_image],\n",
        "#                 feed_dict={in_image: input_patch, gt_image: gt_patch,\n",
        "#                 lr: learning_rate})\n",
        "#         output = np.minimum(np.maximum(output, 0), 1)\n",
        "#         g_loss[ind] = G_current\n",
        "\n",
        "#         print '%d %d Loss=%.3f Time=%.3f' % (epoch, cnt,\n",
        "#                 np.mean(g_loss[np.where(g_loss)]), time.time() - st)\n",
        "\n",
        "#         if epoch % save_freq == 0:\n",
        "#             if not os.path.isdir(result_dir + '%04d' % epoch):\n",
        "#                 os.makedirs(result_dir + '%04d' % epoch)\n",
        "\n",
        "#             temp = np.concatenate((gt_patch[0, :, :, :], output[0, :, :\n",
        "#                                   , :]), axis=1)\n",
        "#             scipy.misc.toimage(temp * 255, high=255, low=0, cmin=0,\n",
        "#                                cmax=255).save(result_dir\n",
        "#                     + '%04d/%05d_00_train_%d.jpg' % (epoch, train_id,\n",
        "#                     ratio))\n",
        "\n",
        "#     saver.save(sess, checkpoint_dir + 'model.ckpt')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JBj7W1fYVIrC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.save_weights('final_'+save_fname)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N6sFfJ4oV7X-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.save('final_full_'+save_fname)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N63_LgRdETgf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# %tensorboard --logdir '/content/drive/My Drive/Data/SeeInDarkDataset/dataset/Sony/tensorboard'\n",
        "# from tensorboard import notebook\n",
        "# notebook.list() # View open TensorBoard instances\n",
        "# notebook.display(port=6006, height=1000) "
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}
