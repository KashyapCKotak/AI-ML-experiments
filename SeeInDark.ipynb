{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "StockTF1_4Sequential.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KashyapCKotak/AI-ML-experiments/blob/master/SeeInDark.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fLKIO7Kqy1M2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import math\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "class DataLoader():\n",
        "    \"\"\"A class for loading and transforming data for the lstm model\"\"\"\n",
        "\n",
        "    def __init__(self, filename, split, cols):\n",
        "        dataframe = pd.read_csv(filename)\n",
        "        print(\"data shape:\",dataframe.shape)\n",
        "        i_split = int(len(dataframe) * split)\n",
        "        self.data_train = dataframe.get(cols).values[:i_split]\n",
        "        self.data_test  = dataframe.get(cols).values[i_split:]\n",
        "        self.len_train  = len(self.data_train)\n",
        "        self.len_test   = len(self.data_test)\n",
        "        self.len_train_windows = None\n",
        "\n",
        "    def get_test_data(self, seq_len, normalise):\n",
        "        '''\n",
        "        Create x, y test data windows\n",
        "        Warning: batch method, not generative, make sure you have enough memory to\n",
        "        load data, otherwise reduce size of the training split.\n",
        "        '''\n",
        "        data_windows = []\n",
        "        #data_x=[]\n",
        "        #data_y=[]\n",
        "        for i in range(self.len_test - seq_len):\n",
        "            data_windows.append(self.data_test[i:i+seq_len])\n",
        "\n",
        "        data_windows = np.array(data_windows).astype(float)\n",
        "        data_windows = self.normalise_windows(data_windows, single_window=False) if normalise else data_windows\n",
        "\n",
        "        x = data_windows[:, :-1]\n",
        "        y = data_windows[:, -1, [0]]\n",
        "            #x,y=self._next_window(i,seq_len,normalise,train=False)\n",
        "            #data_x.append(x)\n",
        "            #data_y.append(y)\n",
        "        #return np.array(data_x),np.array(data_y)\n",
        "        return x,y\n",
        "\n",
        "    def get_train_data(self, seq_len, normalise):\n",
        "        '''\n",
        "        Create x, y train data windows\n",
        "        Warning: batch method, not generative, make sure you have enough memory to\n",
        "        load data, otherwise use generate_training_window() method.\n",
        "        '''\n",
        "        data_x = []\n",
        "        data_y = []\n",
        "        for i in range(self.len_train - seq_len):\n",
        "            x, y = self._next_window(i, seq_len, normalise)\n",
        "            data_x.append(x)\n",
        "            data_y.append(y)\n",
        "        return np.array(data_x), np.array(data_y)\n",
        "\n",
        "    def generate_train_batch(self, seq_len, batch_size, normalise, epochs):\n",
        "        '''Yield a generator of training data from filename on given list of cols split for train/test'''\n",
        "        i = 0\n",
        "        print(\"train length:\",self.len_train)\n",
        "        #while epoch < epochs:\n",
        "        while i < ((self.len_train - seq_len)*(epochs+1)):\n",
        "            #print(\"i:\",i)\n",
        "            x_batch = []\n",
        "            y_batch = []\n",
        "            for b in range(batch_size):\n",
        "                if i >= (self.len_train - seq_len):\n",
        "                    # stop-condition for a smaller final batch if data doesn't divide evenly\n",
        "                    yield np.array(x_batch), np.array(y_batch)\n",
        "                    i = 0\n",
        "                    print(\"i set to 0\")\n",
        "                x, y = self._next_window(i, seq_len, normalise)\n",
        "                x_batch.append(x)\n",
        "                y_batch.append(y)\n",
        "                i += 1\n",
        "            \n",
        "            #print (\"x:\",np.array(x_batch).shape)\n",
        "            #print (\"y:\",np.array(y_batch).shape)\n",
        "            yield np.array(x_batch), np.array(y_batch)\n",
        "        #epoch += 1\n",
        "\n",
        "    def _next_window(self, i, seq_len, normalise):\n",
        "        '''Generates the next data window from the given index location i'''\n",
        "        window = self.data_train[i:i+seq_len]\n",
        "        #if train:\n",
        "        #    window = self.data_train[i:i+seq_len]\n",
        "        #else:\n",
        "        #    window = self.data_test[i:i+seq_len]\n",
        "        window = self.normalise_windows(window, single_window=True)[0] if normalise else window\n",
        "        x = window[:-1]\n",
        "        y = window[-1, [0]]\n",
        "        return x, y\n",
        "\n",
        "    def normalise_windows(self, window_data, single_window=False):\n",
        "        '''Normalise window with a base value of zero'''\n",
        "        eps=0.00001\n",
        "        normalised_data = []\n",
        "        window_data = [window_data] if single_window else window_data\n",
        "        for window in window_data:\n",
        "            normalised_window = []\n",
        "            for col_i in range(window.shape[1]):\n",
        "                normalised_col = [((float(p) / (float(window[0, col_i])+eps)) - 1) for p in window[:, col_i]]\n",
        "                normalised_window.append(normalised_col)\n",
        "            normalised_window = np.array(normalised_window).T # reshape and transpose array back into original multidimensional format\n",
        "            normalised_data.append(normalised_window)\n",
        "        return np.array(normalised_data)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WDtY8dAay2sM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import datetime as dt\n",
        "\n",
        "class Timer():\n",
        "\n",
        "\tdef __init__(self):\n",
        "\t\tself.start_dt = None\n",
        "\n",
        "\tdef start(self):\n",
        "\t\tself.start_dt = dt.datetime.now()\n",
        "\n",
        "\tdef stop(self):\n",
        "\t\tend_dt = dt.datetime.now()\n",
        "\t\tprint('Time taken: %s' % (end_dt - self.start_dt))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Qppgq8_y6ar",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import math\n",
        "import numpy as np\n",
        "import datetime as dt\n",
        "from numpy import newaxis\n",
        "from keras.layers import Dense, Activation, Dropout, LSTM\n",
        "from keras.models import Sequential, load_model\n",
        "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "from keras.utils import plot_model\n",
        "import pydot\n",
        "\n",
        "class Model():\n",
        "\t\"\"\"A class for an building and inferencing an lstm model\"\"\"\n",
        "\n",
        "\tdef __init__(self):\n",
        "\t\tself.model = Sequential()\n",
        "\n",
        "\tdef load_model(self, filepath):\n",
        "\t\tprint('[Model] Loading model from file %s' % filepath)\n",
        "\t\tself.model = load_model(filepath)\n",
        "\n",
        "\tdef build_model(self, configs):\n",
        "\t\ttimer = Timer()\n",
        "\t\ttimer.start()\n",
        "\n",
        "\t\tfor layer in configs['model']['layers']:\n",
        "\t\t\tneurons = layer['neurons'] if 'neurons' in layer else None\n",
        "\t\t\tdropout_rate = layer['rate'] if 'rate' in layer else None\n",
        "\t\t\tactivation = layer['activation'] if 'activation' in layer else None\n",
        "\t\t\treturn_seq = layer['return_seq'] if 'return_seq' in layer else None\n",
        "\t\t\tinput_timesteps = layer['input_timesteps'] if 'input_timesteps' in layer else None\n",
        "\t\t\tinput_dim = layer['input_dim'] if 'input_dim' in layer else None\n",
        "\n",
        "\t\t\tif layer['type'] == 'dense':\n",
        "\t\t\t\tself.model.add(Dense(neurons, activation=activation))\n",
        "\t\t\tif layer['type'] == 'lstm':\n",
        "\t\t\t\tself.model.add(LSTM(neurons, input_shape=(input_timesteps, input_dim), return_sequences=return_seq))\n",
        "\t\t\tif layer['type'] == 'dropout':\n",
        "\t\t\t\tself.model.add(Dropout(dropout_rate))\n",
        "\n",
        "\t\tself.model.compile(loss=configs['model']['loss'], optimizer=configs['model']['optimizer'],metrics=['mean_squared_error'])\n",
        "\t\tprint(self.model.summary())\n",
        "\t\tplot_model(self.model, to_file='model.png')\n",
        "      \n",
        "\t\tprint('[Model] Model Compiled')\n",
        "\t\ttimer.stop()\n",
        "\t\treturn self.model\n",
        "\n",
        "\tdef train(self, x, y, epochs, batch_size, save_dir=\"\"):\n",
        "\t\ttimer = Timer()\n",
        "\t\ttimer.start()\n",
        "\t\tprint('X shape:', (x.shape))\n",
        "\t\tprint('[Model] Training Started')\n",
        "\t\tprint('[Model] %s epochs, %s batch size' % (epochs, batch_size))\n",
        "\t\t\n",
        "\t\tsave_fname = os.path.join(save_dir, '%s-e%s.h5' % (dt.datetime.now().strftime('%d%m%Y-%H%M%S'), str(epochs)))\n",
        "\t\tcallbacks = [\n",
        "\t\t\tEarlyStopping(monitor='val_loss', patience=2),\n",
        "\t\t\tModelCheckpoint(filepath=save_fname, monitor='val_loss', save_best_only=True)\n",
        "\t\t]\n",
        "\t\tmodelhistory=self.model.fit(\n",
        "\t\t\tx,\n",
        "\t\t\ty,\n",
        "\t\t\tepochs=epochs,\n",
        "\t\t\tbatch_size=batch_size,\n",
        "\t\t\tcallbacks=callbacks\n",
        "\t\t)\n",
        "\t\tself.model.save(save_fname)\n",
        "\t\tprint('[Model] Training Completed. Model saved as %s' % save_fname)\n",
        "\t\ttimer.stop()\n",
        "\t\treturn modelhistory\n",
        "\n",
        "\tdef train_generator(self, data_gen, epochs, batch_size, steps_per_epoch, save_dir=\"\"):\n",
        "\t\ttimer = Timer()\n",
        "\t\ttimer.start()\n",
        "\t\t#print('X shape:', (x.shape))\n",
        "\t\tprint('[Model] Training Started')\n",
        "\t\tprint('[Model] %s epochs, %s batch size, %s batches per epoch' % (epochs, batch_size, steps_per_epoch))\n",
        "\t\t\n",
        "\t\tsave_fname = os.path.join(save_dir, '%s-e%s.h5' % (dt.datetime.now().strftime('%d%m%Y-%H%M%S'), str(epochs)))\n",
        "\t\tcallbacks = [\n",
        "\t\t\tModelCheckpoint(filepath=save_fname, monitor='loss', save_best_only=True)\n",
        "\t\t]\n",
        "\t\tmodelhistory=self.model.fit_generator(\n",
        "\t\t\tdata_gen,\n",
        "\t\t\tsteps_per_epoch=steps_per_epoch,\n",
        "\t\t\tepochs=epochs,\n",
        "\t\t\tcallbacks=callbacks,\n",
        "\t\t\tworkers=0\n",
        "\t\t)\n",
        "\t\t\n",
        "\t\tprint('[Model] Training Completed. Model saved as %s' % save_fname)\n",
        "\t\ttimer.stop()\n",
        "\t\treturn modelhistory\n",
        "\n",
        "\tdef predict_point_by_point(self, data):\n",
        "\t\t#Predict each timestep given the last sequence of true data, in effect only predicting 1 step ahead each time\n",
        "\t\tprint('[Model] Predicting Point-by-Point...')\n",
        "\t\tpredicted = self.model.predict(data)\n",
        "\t\tpredicted = np.reshape(predicted, (predicted.size,))\n",
        "\t\treturn predicted\n",
        "\n",
        "\tdef predict_sequences_multiple(self, data, window_size, prediction_len):\n",
        "\t\t#Predict sequence of 50 steps before shifting prediction run forward by 50 steps\n",
        "\t\tprint('[Model] Predicting Sequences Multiple...')\n",
        "\t\tprediction_seqs = []\n",
        "\t\tfor i in range(int(len(data)/prediction_len)):\n",
        "\t\t\tcurr_frame = data[i*prediction_len]\n",
        "\t\t\tpredicted = []\n",
        "\t\t\tfor j in range(prediction_len):\n",
        "\t\t\t\tpredicted.append(self.model.predict(curr_frame[newaxis,:,:])[0,0])\n",
        "\t\t\t\tcurr_frame = curr_frame[1:]\n",
        "\t\t\t\tcurr_frame = np.insert(curr_frame, [window_size-2], predicted[-1], axis=0)\n",
        "\t\t\tprediction_seqs.append(predicted)\n",
        "\t\treturn prediction_seqs\n",
        "\n",
        "\tdef predict_sequence_full(self, data, window_size):\n",
        "\t\t#Shift the window by 1 new prediction each time, re-run predictions on new window\n",
        "\t\tprint('[Model] Predicting Sequences Full...')\n",
        "\t\tcurr_frame = data[0]\n",
        "\t\tpredicted = []\n",
        "\t\tfor i in range(len(data)):\n",
        "\t\t\tpredicted.append(self.model.predict(curr_frame[newaxis,:,:])[0,0])\n",
        "\t\t\tcurr_frame = curr_frame[1:]\n",
        "\t\t\tcurr_frame = np.insert(curr_frame, [window_size-2], predicted[-1], axis=0)\n",
        "\t\treturn predicted\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GodPk8nqzBjO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#/content/drive/My Drive/Data/2015-02-02-To-2019-5-19-5-Min.csv\n",
        "configJson={\n",
        "\t\"data\": {\n",
        "\t\t\"filename\": \"2015-02-02-To-2019-5-19-5-Min.csv\",\n",
        "\t\t\"columns\": [\n",
        "\t\t\t\"Close\",\"Volume\"\n",
        "\t\t],\n",
        "\t\t\"sequence_length\": 50,\n",
        "\t\t\"train_test_split\": 0.80,\n",
        "\t\t\"normalise\": True\n",
        "\t},\n",
        "\t\"training\": {\n",
        "\t\t\"epochs\": 2,\n",
        "\t\t\"batch_size\": 32\n",
        "\t},\n",
        "\t\"model\": {\n",
        "\t\t\"loss\": \"mse\",\n",
        "\t\t\"optimizer\": \"adam\",\n",
        "\t\t\"layers\": [\n",
        "\t\t\t{\n",
        "\t\t\t\t\"type\": \"lstm\",\n",
        "\t\t\t\t\"neurons\": 100,\n",
        "\t\t\t\t\"input_timesteps\": 49,\n",
        "\t\t\t\t\"input_dim\": 2,\n",
        "\t\t\t\t\"return_seq\": True\n",
        "\t\t\t},\n",
        "\t\t\t{\n",
        "\t\t\t\t\"type\": \"dropout\",\n",
        "\t\t\t\t\"rate\": 0.2\n",
        "\t\t\t},\n",
        "\t\t\t{\n",
        "\t\t\t\t\"type\": \"lstm\",\n",
        "\t\t\t\t\"neurons\": 100,\n",
        "\t\t\t\t\"return_seq\": False\n",
        "\t\t\t},\n",
        "\t\t\t{\n",
        "\t\t\t\t\"type\": \"dropout\",\n",
        "\t\t\t\t\"rate\": 0.2\n",
        "\t\t\t},\n",
        "\t\t\t{\n",
        "\t\t\t\t\"type\": \"dense\",\n",
        "\t\t\t\t\"neurons\": 1,\n",
        "\t\t\t\t\"activation\": \"linear\"\n",
        "\t\t\t}\n",
        "\t\t]\n",
        "\t}\n",
        "}\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A7XZL9ayzEuR",
        "colab_type": "code",
        "outputId": "43f9da91-d78d-4cd7-b9a8-2cc6fa829d28",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 538
        }
      },
      "source": [
        "configs = configJson\n",
        "\n",
        "data = DataLoader(\n",
        "\tos.path.join(configs['data']['filename']),\n",
        "\tconfigs['data']['train_test_split'],\n",
        "\tconfigs['data']['columns']\n",
        ")\n",
        "\n",
        "model = Model()\n",
        "model.build_model(configs)\n",
        "\n",
        "#x, y = data.get_train_data(\n",
        "#\tseq_len = configs['data']['sequence_length'],\n",
        "#\tnormalise = configs['data']['normalise']\n",
        "#)\n",
        "\n",
        "\n",
        "# out-of memory generative training\n",
        "steps_per_epoch = math.ceil((data.len_train - configs['data']['sequence_length']) / configs['training']['batch_size'])\n",
        "modelhistory=model.train_generator(\n",
        "\tdata_gen = data.generate_train_batch(\n",
        "\t\tseq_len = configs['data']['sequence_length'],\n",
        "\t\tbatch_size = configs['training']['batch_size'],\n",
        "\t\tnormalise = configs['data']['normalise'],\n",
        "      epochs = configs['training']['epochs']\n",
        "\t),\n",
        "\tepochs = configs['training']['epochs'],\n",
        "\tbatch_size = configs['training']['batch_size'],\n",
        "\tsteps_per_epoch = steps_per_epoch\n",
        ")\n",
        "#model.train(\n",
        "#\tx,\n",
        "#\ty,\n",
        "#\tepochs = configs['training']['epochs'],\n",
        "#\tbatch_size = configs['training']['batch_size']\n",
        "#)\n",
        "\n",
        "\n",
        "x_test, y_test = data.get_test_data(\n",
        "\tseq_len = configs['data']['sequence_length'],\n",
        "\tnormalise = configs['data']['normalise']\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "data shape: (79005, 12)\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "lstm_13 (LSTM)               (None, 49, 100)           41200     \n",
            "_________________________________________________________________\n",
            "dropout_13 (Dropout)         (None, 49, 100)           0         \n",
            "_________________________________________________________________\n",
            "lstm_14 (LSTM)               (None, 100)               80400     \n",
            "_________________________________________________________________\n",
            "dropout_14 (Dropout)         (None, 100)               0         \n",
            "_________________________________________________________________\n",
            "dense_7 (Dense)              (None, 1)                 101       \n",
            "=================================================================\n",
            "Total params: 121,701\n",
            "Trainable params: 121,701\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "[Model] Model Compiled\n",
            "Time taken: 0:00:00.562177\n",
            "[Model] Training Started\n",
            "[Model] 2 epochs, 32 batch size, 1974 batches per epoch\n",
            "Epoch 1/2\n",
            "train length: 63204\n",
            "1974/1974 [==============================] - 283s 143ms/step - loss: 0.0013 - mean_squared_error: 0.0013\n",
            "Epoch 2/2\n",
            "i set to 0\n",
            "1974/1974 [==============================] - 281s 142ms/step - loss: 5.3661e-04 - mean_squared_error: 5.3661e-04\n",
            "[Model] Training Completed. Model saved as 17072019-173503-e2.h5\n",
            "Time taken: 0:09:26.879120\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pDKPXn1_zKeh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "predictions_multiseq = model.predict_sequences_multiple(x_test, configs['data']['sequence_length'], configs['data']['sequence_length'])\n",
        "#predictions_fullseq = model.predict_sequence_full(x_test, configs['data']['sequence_length'])\n",
        "predictions_pointbypoint = model.predict_point_by_point(x_test) \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bZSaAVLJzMpj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "def plot_results_multiple(predicted_data, true_data, prediction_len):\n",
        "    fig = plt.figure(facecolor='white',figsize=(200, 6))\n",
        "    ax = fig.add_subplot(111)\n",
        "    ax.plot(true_data, label='True Data')\n",
        "\t# Pad the list of predictions to shift it in the graph to it's correct start\n",
        "    for i, data in enumerate(predicted_data):\n",
        "        padding = [None for p in range(i * prediction_len)]\n",
        "        plt.plot(padding + data, label='Prediction')\n",
        "#        plt.legend()\n",
        "    plt.shosniw()\n",
        "    plt.savefig('multiple.png')\n",
        "    \n",
        "def plot_results(predicted_data, true_data):\n",
        "    fig = plt.figure(facecolor='white',figsize=(200, 6))\n",
        "    ax = fig.add_subplot(111)\n",
        "    ax.plot(true_data, label='True Data')\n",
        "    plt.plot(predicted_data, label='Prediction')\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "    plt.savefig('point.png')\n",
        "\n",
        "    \n",
        "#plot_results(predictions_fullseq, y_test)\n",
        "plot_results(predictions_pointbypoint, y_test)\n",
        "plot_results_multiple(predictions_multiseq, y_test, configs['data']['sequence_length'])\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MzyzJsjmzRHF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "# Visualize training history\n",
        "print(modelhistory.history.keys())\n",
        "# summarize history for accuracy\n",
        "plt.plot(modelhistory.history['mean_squared_error'])\n",
        "plt.title('MSE')\n",
        "plt.ylabel('mse')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'test'], loc='upper left')\n",
        "plt.show()\n",
        "# summarize history for loss\n",
        "plt.plot(modelhistory.history['loss'])\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'test'], loc='upper left')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}